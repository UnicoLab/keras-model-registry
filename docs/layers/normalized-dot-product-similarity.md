---
title: NormalizedDotProductSimilarity - KMR
description: Compute normalized dot product (cosine) similarity between user and item representations
keywords: [similarity, dot product, cosine similarity, recommendation, normalization, keras]
---

# ğŸ“ NormalizedDotProductSimilarity

<div class="layer-hero">
  <div class="layer-hero-content">
    <h1>ğŸ“ NormalizedDotProductSimilarity</h1>
    <div class="layer-badges">
      <span class="badge badge-beginner">ğŸŸ¢ Beginner</span>
      <span class="badge badge-stable">âœ… Stable</span>
      <span class="badge badge-recommendation">ğŸ“Š Recommendation</span>
    </div>
  </div>
</div>

## ğŸ¯ Overview

The `NormalizedDotProductSimilarity` layer computes normalized dot product (cosine) similarity between user and item representations. It measures how similar each user is to each item on a normalized scale [-1, 1], forming the basis for similarity-based recommendation scoring.

This layer is crucial for two-tower models and collaborative filtering approaches, providing interpretable similarity scores that are invariant to vector magnitude.

## ğŸ” How It Works

The layer computes cosine similarity via normalization and dot product:

1. **User Representation**: (batch_size, 1, embedding_dim)
2. **Item Representations**: (batch_size, num_items, embedding_dim)
3. **Normalize User**: Divide by L2 norm â†’ unit vectors
4. **Normalize Items**: Divide by L2 norm â†’ unit vectors
5. **Dot Product**: Matrix multiplication
6. **Output Similarities**: (batch_size, num_items) in range [-1, 1]

```mermaid
graph TD
    A[User Repr<br/>batch Ã— 1 Ã— dim] --> B[L2 Normalize<br/>User]
    C[Item Reprs<br/>batch Ã— items Ã— dim] --> D[L2 Normalize<br/>Items]
    B --> E[Batched<br/>MatMul]
    D --> E
    E --> F[Similarities<br/>batch Ã— items]
    
    style A fill:#e6f3ff,stroke:#4a86e8
    style C fill:#e6f3ff,stroke:#4a86e8
    style F fill:#e8f5e9,stroke:#66bb6a
```

## ğŸ’¡ Why Use This Layer?

| Challenge | Traditional Approach | Solution |
|-----------|---------------------|----------|
| **Similarity Computation** | Manual dot products | ğŸ¯ **Automatic** similarity |
| **Normalization** | Manual L2 normalization | âš¡ **Built-in** normalization |
| **Scale Invariance** | Magnitude-dependent | ğŸ§  **Magnitude-invariant** |
| **Interpretability** | Raw dot products [-âˆ, âˆ] | ğŸ“Š **Interpretable** [-1, 1] range |

## ğŸ“Š Use Cases

- **Two-Tower Models**: Computing user-item similarity
- **Collaborative Filtering**: Similarity-based recommendations
- **Content-Based Filtering**: Feature similarity computation
- **Ranking**: Scoring for recommendation ranking
- **Retrieval**: Finding similar items or users

## ğŸš€ Quick Start

```python
import keras
from kmr.layers import NormalizedDotProductSimilarity

# Create similarity layer
similarity_layer = NormalizedDotProductSimilarity()

# Compute similarities
user_repr = keras.random.normal((32, 1, 64))  # (batch, 1, dim)
item_repr = keras.random.normal((32, 100, 64))  # (batch, items, dim)
similarities = similarity_layer([user_repr, item_repr])

print(f"Similarities shape: {similarities.shape}")  # (32, 100)
print(f"Similarities range: [{similarities.min():.2f}, {similarities.max():.2f}]")  # ~[-1, 1]
```

## ğŸ“– API Reference

::: kmr.layers.NormalizedDotProductSimilarity

## ğŸ“ˆ Performance Characteristics

- **Speed**: âš¡âš¡âš¡âš¡ Very fast - O(batch Ã— items Ã— dim)
- **Memory**: ğŸ’¾ Minimal - no additional buffers
- **Accuracy**: ğŸ¯ğŸ¯ğŸ¯ğŸ¯ Mathematically precise cosine similarity
- **Scalability**: Linear with number of items

## ğŸ¨ Examples

### Example 1: Basic Similarity Computation

```python
import keras
from kmr.layers import NormalizedDotProductSimilarity

# Create layer
similarity = NormalizedDotProductSimilarity()

# Representations
user = keras.random.normal((8, 1, 32))
items = keras.random.normal((8, 50, 32))

# Compute
sims = similarity([user, items])
print(f"Shape: {sims.shape}")  # (8, 50)
print(f"Min: {sims.min():.3f}, Max: {sims.max():.3f}")
```

### Example 2: Batch Processing

```python
import keras
import numpy as np
from kmr.layers import NormalizedDotProductSimilarity

similarity = NormalizedDotProductSimilarity()

# Different batch sizes
for batch_size in [1, 32, 256]:
    user = keras.random.normal((batch_size, 1, 64))
    items = keras.random.normal((batch_size, 1000, 64))
    sims = similarity([user, items])
    print(f"Batch {batch_size}: {sims.shape} - range [{sims.min():.3f}, {sims.max():.3f}]")
```

## ğŸ’¡ Tips & Best Practices

- **Normalization**: Output is always in [-1, 1] range for interpretability
- **Batch Processing**: Efficiently handles multiple users and items
- **Score Range**: Works with any embedding dimension
- **Integration**: Perfect middle layer between embeddings and ranking

## âš ï¸ Common Pitfalls

- **Input Shapes**: User must be (batch, 1, dim); items must be (batch, num_items, dim)
- **Dimension Mismatch**: User and item embedding dimensions must match
- **Zero Vectors**: Division by zero handled via epsilon in normalization

## ğŸ”— Related Layers

- [CollaborativeUserItemEmbedding](collaborative-user-item-embedding.md) - Get embeddings
- [DeepFeatureTower](deep-feature-tower.md) - Process features
- [TopKRecommendationSelector](top-k-recommendation-selector.md) - Select top-K

## ğŸ“š Further Reading

- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
- [Embedding Spaces](https://arxiv.org/abs/1906.04289)
- [Recommendation Systems](https://arxiv.org/abs/1707.07435)
