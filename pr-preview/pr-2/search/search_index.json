{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Welcome to KMR - Keras Model Registry","text":"<p>\ud83e\udde9 Reusable Model Architecture Bricks in Keras 3</p> <p>\ud83c\udfe2 Provided and maintained by UnicoLab</p> <p>\ud83c\udfaf Production-Ready Tabular AI</p> <p>Build sophisticated tabular models with 38+ specialized layers, smart preprocessing, and intelligent feature engineering - all designed exclusively for Keras 3.</p>"},{"location":"#what-is-kmr","title":"\ud83c\udfaf What is KMR?","text":"<p>KMR (Keras Model Registry) is a comprehensive collection of production-ready layers and models designed specifically for tabular data processing with Keras 3. Our library provides:</p> <ul> <li>\ud83e\udde0 Advanced Attention Mechanisms for tabular data</li> <li>\ud83d\udd27 Feature Engineering Layers for data preprocessing  </li> <li>\ud83c\udfd7\ufe0f Pre-built Models for common ML tasks</li> <li>\ud83d\udcca Data Analysis Tools for intelligent layer recommendations</li> <li>\u26a1 Keras 3 Native - No TensorFlow dependencies in production code</li> </ul> <p>Why KMR?</p> <p>KMR eliminates the need to build complex tabular models from scratch. Our layers are battle-tested, well-documented, and designed to work seamlessly together.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Install from PyPI (recommended)\npip install kmr\n\n# Or install from source using Poetry\ngit clone https://github.com/UnicoLab/keras-model-registry\ncd keras-model-registry\npoetry install\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import DistributionTransformLayer, GatedFeatureFusion\n\n# Create sample tabular data\ninputs = keras.Input(shape=(10,))  # 10 features\n\n# Smart data preprocessing\ntransformed = DistributionTransformLayer(transform_type='auto')(inputs)\n\n# Create two feature representations\nlinear_features = keras.layers.Dense(16, activation='relu')(transformed)\nnonlinear_features = keras.layers.Dense(16, activation='tanh')(transformed)\n\n# Intelligently combine features\nfused = GatedFeatureFusion()([linear_features, nonlinear_features])\n\n# Final prediction\noutputs = keras.layers.Dense(1, activation='sigmoid')(fused)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nprint(\"\u2705 Model ready! Smart preprocessing + intelligent feature fusion.\")\n</code></pre> <p>That's it!</p> <p>In just a few lines, you've created a sophisticated tabular model with automatic data transformation and intelligent feature fusion!</p>"},{"location":"#whats-inside-kmr","title":"\ud83e\udde9 What's Inside KMR?","text":"<ul> <li> <p>:material-brain:{ .lg .middle } 38+ Production Layers</p> <p>Advanced attention mechanisms, feature processing, and specialized architectures ready for production use.</p> <p>:octicons-arrow-right-24: Explore Layers</p> </li> <li> <p>:material-cog:{ .lg .middle } Smart Preprocessing</p> <p>Automatic data transformation, date encoding, and intelligent feature engineering layers.</p> <p>:octicons-arrow-right-24: See Examples</p> </li> <li> <p>:material-rocket-launch:{ .lg .middle } Pre-built Models</p> <p>Ready-to-use models like BaseFeedForwardModel and SFNEBlock for common ML tasks.</p> <p>:octicons-arrow-right-24: View Models</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Data Analyzer</p> <p>Intelligent CSV analysis tool that recommends the best layers for your specific data.</p> <p>:octicons-arrow-right-24: Try Analyzer</p> </li> </ul>"},{"location":"#documentation-highlights","title":"\ud83d\udcda Documentation Highlights","text":"<p>Our documentation is designed to be developer-friendly with:</p> <ul> <li>\u2728 Rich Docstrings: Every layer includes comprehensive examples, best practices, and performance notes</li> <li>\ud83c\udfaf Usage Examples: Multiple scenarios from basic to advanced</li> <li>\u26a1 Performance Tips: Memory usage, scalability, and optimization guidance</li> <li>\ud83d\udd17 Cross-references: Easy navigation between related components</li> </ul> <p>Try the Interactive Examples</p> <p>Check out our Rich Docstrings Showcase to see the comprehensive documentation in action!</p>"},{"location":"#key-features","title":"\ud83c\udfa8 Key Features","text":"\ud83e\udde0 Advanced Architecture\u26a1 Performance Optimized\ud83d\udd27 Developer Friendly <ul> <li>Graph-based Processing: Learn feature relationships dynamically</li> <li>Multi-head Attention: Capture complex feature interactions  </li> <li>Hierarchical Aggregation: Efficient processing of large feature sets</li> <li>Residual Connections: Stable training and better gradients</li> </ul> <ul> <li>Keras 3 Native: Latest Keras features and optimizations</li> <li>Memory Efficient: Optimized for large-scale tabular data</li> <li>GPU Ready: Full GPU acceleration support</li> <li>Serializable: Save and load models seamlessly</li> </ul> <ul> <li>Type Annotations: Complete type hints for better IDE support</li> <li>Comprehensive Testing: Extensive test coverage</li> <li>Clear Documentation: Rich docstrings with examples</li> <li>Modular Design: Mix and match layers as needed</li> </ul>"},{"location":"#why-choose-kmr","title":"\ud83d\ude80 Why Choose KMR?","text":"<p>Production Ready</p> <p>All layers are battle-tested, well-documented, and designed for production use with comprehensive error handling and validation.</p> <p>Keras 3 Exclusive</p> <p>Built exclusively for Keras 3 with no TensorFlow dependencies in production code, ensuring compatibility and performance.</p> <p>Rich Documentation</p> <p>Every layer includes comprehensive examples, best practices, performance notes, and usage guidance.</p> <p>Modular Design</p> <p>Mix and match layers to build custom architectures that fit your specific use case.</p>"},{"location":"#perfect-for","title":"\ud83c\udfaf Perfect For","text":"### \ud83c\udfe2 Enterprise ML Teams - **Scalable Architecture**: Handle large-scale tabular datasets - **Production Ready**: Battle-tested layers with comprehensive testing - **Team Collaboration**: Clear documentation and consistent APIs     ### \ud83d\udd2c Research &amp; Development - **Cutting-edge Techniques**: Latest attention mechanisms and graph processing - **Experimentation**: Easy to combine and modify layers - **Reproducibility**: Well-documented with examples     ### \ud83c\udf93 Learning &amp; Education - **Rich Examples**: Comprehensive documentation with real-world examples - **Best Practices**: Learn from production-ready implementations - **Interactive**: Try examples and modify them for learning"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Whether you're:</p> <ul> <li>\ud83d\udc1b Reporting bugs or suggesting improvements</li> <li>\ud83e\udde9 Adding new layers or models</li> <li>\ud83d\udcdd Improving documentation or examples</li> <li>\ud83d\udd0d Enhancing data analysis tools</li> </ul> <p>Check out our Contributing Guide to get started!</p>"},{"location":"#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<ol> <li>\ud83d\udccb Browse Examples: Start with our Examples Overview</li> <li>\ud83e\udde9 Explore Layers: Check out the Layers API</li> <li>\ud83c\udfd7\ufe0f Build Models: See available Models</li> <li>\ud83d\udd0d Analyze Data: Try our Data Analyzer</li> </ol> <p> Ready to build amazing tabular models? Let's get started! \ud83d\ude80 </p>"},{"location":"404/","title":"404 - Page Not Found404","text":"Page Not Found <p>       The page you're looking for doesn't exist or has been moved.     </p>          \ud83c\udfe0 Go Home                 \ud83d\ude80 Quick Start                 \ud83d\udcda API Reference"},{"location":"404/#what-you-might-be-looking-for","title":"\ud83d\udd0d What you might be looking for:","text":""},{"location":"404/#getting-started","title":"Getting Started","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Installation Guide - Detailed installation instructions</li> <li>Core Concepts - Understand KMR fundamentals</li> </ul>"},{"location":"404/#tutorials","title":"Tutorials","text":"<ul> <li>Basic Workflows - Common patterns and best practices</li> <li>Feature Engineering - Advanced preprocessing techniques</li> <li>Model Building - Build sophisticated models</li> </ul>"},{"location":"404/#api-reference","title":"API Reference","text":"<ul> <li>Layers API - 38+ production-ready layers</li> <li>Models API - Pre-built model architectures</li> <li>Utils API - Utility functions and tools</li> </ul>"},{"location":"404/#examples","title":"Examples","text":"<ul> <li>Examples Overview - Real-world use cases</li> <li>Rich Docstrings Showcase - Comprehensive documentation examples</li> </ul>"},{"location":"404/#need-help","title":"\ud83c\udd98 Need Help?","text":"<ul> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udcac Discussions: GitHub Discussions</li> <li>\ud83d\udce7 Support: contact@unicolab.ai</li> </ul> <p>Lost? Start with our Quick Start Guide to get back on track! \ud83d\ude80</p>"},{"location":"contributing/","title":"Contributing to KMR","text":"<p>Thank you for your interest in contributing to the Keras Model Registry (KMR)! This guide will help you get started with contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Poetry (for dependency management)</li> <li>Git</li> <li>Basic knowledge of Keras 3 and deep learning</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork and Clone the Repository <pre><code>git clone https://github.com/UnicoLab/keras-model-registry.git\ncd keras-model-registry\n</code></pre></p> </li> <li> <p>Install Dependencies <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Install Pre-commit Hooks <pre><code>pre-commit install\n</code></pre></p> </li> <li> <p>Run Tests <pre><code>make all_tests\n</code></pre></p> </li> </ol>"},{"location":"contributing/#types-of-contributions","title":"\ud83d\udccb Types of Contributions","text":""},{"location":"contributing/#adding-new-layers","title":"\ud83e\udde9 Adding New Layers","text":"<p>New layers are the core of KMR. Follow these guidelines:</p>"},{"location":"contributing/#layer-requirements","title":"Layer Requirements","text":"<ul> <li>Keras 3 Only: No TensorFlow dependencies in production code</li> <li>Inherit from BaseLayer: All layers must inherit from <code>kmr.layers._base_layer.BaseLayer</code></li> <li>Full Serialization: Implement <code>get_config()</code> and <code>from_config()</code> methods</li> <li>Type Annotations: Use Python 3.12+ type hints</li> <li>Comprehensive Documentation: Google-style docstrings</li> <li>Parameter Validation: Implement <code>_validate_params()</code> method</li> </ul>"},{"location":"contributing/#file-structure","title":"File Structure","text":"<ul> <li>File Name: <code>YourLayer.py</code> (PascalCase)</li> <li>Location: <code>kmr/layers/YourLayer.py</code></li> <li>Export: Add to <code>kmr/layers/__init__.py</code></li> </ul>"},{"location":"contributing/#adding-new-models","title":"\ud83c\udfd7\ufe0f Adding New Models","text":"<p>Models should inherit from <code>kmr.models._base.BaseModel</code> and follow similar patterns to layers.</p>"},{"location":"contributing/#adding-tests","title":"\ud83e\uddea Adding Tests","text":"<p>Every layer and model must have comprehensive tests:</p>"},{"location":"contributing/#test-file-structure","title":"Test File Structure","text":"<ul> <li>File Name: <code>test__YourLayer.py</code> (note the double underscore)</li> <li>Location: <code>tests/layers/test__YourLayer.py</code> or <code>tests/models/test__YourModel.py</code></li> </ul>"},{"location":"contributing/#required-tests","title":"Required Tests","text":"<ul> <li>Initialization tests</li> <li>Invalid parameter tests</li> <li>Build tests</li> <li>Output shape tests</li> <li>Serialization tests</li> <li>Training mode tests</li> <li>Model integration tests</li> </ul>"},{"location":"contributing/#development-workflow","title":"\ud83d\udd04 Development Workflow","text":""},{"location":"contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write your code following the guidelines above</li> <li>Add comprehensive tests</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nmake all_tests\n\n# Run specific test file\npoetry run python -m pytest tests/layers/test__YourLayer.py -v\n\n# Run with coverage\nmake coverage\n</code></pre>"},{"location":"contributing/#4-documentation","title":"4. Documentation","text":"<p>Documentation is automatically generated from docstrings using MkDocs and mkdocstrings. Simply ensure your docstrings follow Google style format and the documentation will be updated automatically when the site is built.</p>"},{"location":"contributing/#5-commit-changes","title":"5. Commit Changes","text":"<p>Use conventional commit messages: <pre><code>git add .\ngit commit -m \"feat(layers): add YourLayer for feature processing\"\n</code></pre></p>"},{"location":"contributing/#6-push-and-create-pull-request","title":"6. Push and Create Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre>"},{"location":"contributing/#commit-convention","title":"\ud83d\udcdd Commit Convention","text":"<p>We use conventional commits:</p> <ul> <li><code>feat(layers): add new layer for feature processing</code></li> <li><code>fix(models): resolve serialization issue in TerminatorModel</code></li> <li><code>docs(readme): update installation instructions</code></li> <li><code>test(layers): add tests for YourLayer</code></li> <li><code>refactor(utils): improve data analyzer performance</code></li> </ul>"},{"location":"contributing/#testing-guidelines","title":"\ud83e\uddea Testing Guidelines","text":""},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Minimum 90%: All new code must have 90%+ test coverage</li> <li>All Paths: Test both success and failure cases</li> <li>Edge Cases: Test boundary conditions and edge cases</li> </ul>"},{"location":"contributing/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests: Individual layer/model functionality</li> <li>Integration Tests: Layer combinations and model workflows</li> <li>Serialization Tests: Save/load functionality</li> <li>Performance Tests: For computationally intensive components</li> </ol>"},{"location":"contributing/#what-not-to-include","title":"\ud83d\udeab What Not to Include","text":""},{"location":"contributing/#experimental-components","title":"Experimental Components","text":"<ul> <li>Location: <code>experimental/</code> directory (outside package)</li> <li>Purpose: Research and development</li> <li>Status: Not included in PyPI package</li> <li>Dependencies: May use TensorFlow for testing</li> </ul>"},{"location":"contributing/#prohibited-dependencies","title":"Prohibited Dependencies","text":"<ul> <li>TensorFlow: Only allowed in test files</li> <li>PyTorch: Not allowed</li> <li>Other ML Frameworks: Keras 3 only</li> </ul>"},{"location":"contributing/#getting-help","title":"\ud83d\udcde Getting Help","text":"<ul> <li>GitHub Issues: GitHub Issues</li> <li>GitHub Discussions: GitHub Discussions</li> <li>Documentation: Check the docs first</li> </ul>"},{"location":"contributing/#code-review-process","title":"\ud83c\udfaf Code Review Process","text":""},{"location":"contributing/#review-criteria","title":"Review Criteria","text":"<ol> <li>Functionality: Does the code work as intended?</li> <li>Tests: Are there comprehensive tests?</li> <li>Documentation: Is the code well-documented?</li> <li>Style: Does it follow project conventions?</li> <li>Performance: Is it efficient?</li> <li>Security: Are there any security concerns?</li> </ol>"},{"location":"contributing/#review-timeline","title":"Review Timeline","text":"<ul> <li>Initial Review: Within 48 hours</li> <li>Follow-up: Within 24 hours of changes</li> <li>Merge: After approval and CI passes</li> </ul>"},{"location":"contributing/#recognition","title":"\ud83c\udfc6 Recognition","text":"<p>Contributors will be recognized in: - README: Listed as contributors - Release Notes: Mentioned in relevant releases - Documentation: Credited for significant contributions</p>"},{"location":"contributing/#license","title":"\ud83d\udcc4 License","text":"<p>By contributing to KMR, you agree that your contributions will be licensed under the MIT License.</p> <p>Thank you for contributing to KMR! Your contributions help make tabular data processing with Keras more accessible and powerful for everyone.</p>"},{"location":"data_analyzer/","title":"\ud83d\udd0d KMR Data Analyzer","text":"<p>The KMR Data Analyzer is an intelligent utility that analyzes your tabular data and automatically recommends the best KMR layers for your specific dataset.</p> <p>Smart Recommendations</p> <p>Just provide your CSV file, and the analyzer will suggest the most appropriate layers based on your data characteristics!</p>"},{"location":"data_analyzer/#features","title":"\u2728 Features","text":"<ul> <li>\ud83d\udcca Automatic Analysis: Analyzes single CSV files or entire directories</li> <li>\ud83c\udfaf Feature Detection: Identifies numerical, categorical, date, and text features</li> <li>\ud83d\udd0d Data Insights: Detects high cardinality, missing values, correlations, and patterns</li> <li>\ud83e\udde9 Layer Recommendations: Suggests the best KMR layers for your data</li> <li>\ud83d\udd27 Extensible: Add custom recommendation rules</li> <li>\ud83d\udcbb CLI &amp; API: Command-line interface and Python API</li> <li>\ud83d\udcc8 Performance Tips: Guidance on layer configuration and optimization</li> </ul>"},{"location":"data_analyzer/#installation","title":"\ud83d\ude80 Installation","text":"<p>The Data Analyzer is included with the Keras Model Registry package.</p> <pre><code># Install from PyPI (recommended)\npip install kmr\n\n# Or install from source using Poetry\ngit clone https://github.com/UnicoLab/keras-model-registry\ncd keras-model-registry\npoetry install\n</code></pre>"},{"location":"data_analyzer/#usage","title":"\ud83d\udcbb Usage","text":""},{"location":"data_analyzer/#command-line-interface","title":"\ud83d\udda5\ufe0f Command-line Interface","text":"<p>The Data Analyzer can be used from the command line:</p> <pre><code># Analyze a single CSV file\npython -m kmr.utils.data_analyzer_cli path/to/data.csv\n\n# Analyze a directory of CSV files\npython -m kmr.utils.data_analyzer_cli path/to/data_dir/\n\n# Save results to a JSON file\npython -m kmr.utils.data_analyzer_cli path/to/data.csv --output results.json\n\n# Get only layer recommendations without detailed statistics\npython -m kmr.utils.data_analyzer_cli path/to/data.csv --recommendations-only\n</code></pre>"},{"location":"data_analyzer/#python-api","title":"\ud83d\udc0d Python API","text":"<p>You can also use the Data Analyzer in your Python code:</p> <pre><code>from kmr.utils import DataAnalyzer, analyze_data\n\n# Quick usage\nresults = analyze_data(\"path/to/data.csv\")\nrecommendations = results[\"recommendations\"]\n\n# Or using the class directly\nanalyzer = DataAnalyzer()\nresult = analyzer.analyze_and_recommend(\"path/to/data.csv\")\n\n# Add custom layer recommendations\nanalyzer.register_recommendation(\n    characteristic=\"continuous_features\",\n    layer_name=\"MyCustomLayer\",\n    description=\"Custom layer for continuous features\",\n    use_case=\"Special continuous feature processing\"\n)\n\n# Analyze multiple files in a directory\nresult = analyzer.analyze_and_recommend(\"path/to/directory\", pattern=\"*.csv\")\n</code></pre>"},{"location":"data_analyzer/#data-characteristics","title":"Data Characteristics","text":"<p>The analyzer identifies the following data characteristics:</p> <ul> <li><code>continuous_features</code>: Numerical features</li> <li><code>categorical_features</code>: Categorical features</li> <li><code>date_features</code>: Date and time features</li> <li><code>text_features</code>: Text features</li> <li><code>high_cardinality_categorical</code>: Categorical features with high cardinality</li> <li><code>high_missing_value_features</code>: Features with many missing values</li> <li><code>feature_interaction</code>: Highly correlated feature pairs</li> <li><code>time_series</code>: Date features that may indicate time series data</li> <li><code>general_tabular</code>: General tabular data characteristics</li> </ul>"},{"location":"data_analyzer/#layer-recommendations","title":"Layer Recommendations","text":"<p>For each data characteristic, the analyzer recommends appropriate KMR layers along with descriptions and use cases.</p>"},{"location":"data_analyzer/#example","title":"Example","text":"<p>For continuous features, the following layers might be recommended:</p> <ul> <li><code>AdvancedNumericalEmbedding</code>: Embeds continuous features using both MLP and discretization approaches</li> <li><code>DifferentialPreprocessingLayer</code>: Applies various normalizations and transformations to numerical features</li> </ul>"},{"location":"data_analyzer/#extending-layer-recommendations","title":"Extending Layer Recommendations","text":"<p>You can extend the layer recommendations by registering new layers:</p> <pre><code>from kmr.utils import DataAnalyzer\n\nanalyzer = DataAnalyzer()\nanalyzer.register_recommendation(\n    characteristic=\"continuous_features\",\n    layer_name=\"MyCustomLayer\",\n    description=\"Custom layer for continuous features\",\n    use_case=\"Special continuous feature processing\"\n)\n</code></pre>"},{"location":"data_analyzer/#example-script","title":"Example Script","text":"<p>Check out the example script at <code>examples/data_analyzer_example.py</code> for a complete demonstration.</p>"},{"location":"data_analyzer/#output-format","title":"Output Format","text":"<p>The analyzer returns a dictionary with the following structure:</p> <pre><code>{\n  \"analysis\": {\n    \"file\": \"filename.csv\",  # For single file analysis\n    \"stats\": {\n      \"row_count\": 1000,\n      \"column_count\": 10,\n      \"column_types\": { ... },\n      \"characteristics\": {\n        \"continuous_features\": [\"feature1\", \"feature2\", ...],\n        \"categorical_features\": [\"feature3\", \"feature4\", ...],\n        ...\n      },\n      \"missing_values\": { ... },\n      \"cardinality\": { ... },\n      \"numeric_stats\": { ... }\n    }\n  },\n  \"recommendations\": {\n    \"continuous_features\": [\n      [\"LayerName1\", \"Description1\", \"UseCase1\"],\n      [\"LayerName2\", \"Description2\", \"UseCase2\"],\n      ...\n    ],\n    \"categorical_features\": [ ... ],\n    ...\n  }\n}\n</code></pre>"},{"location":"data_analyzer/#caveats","title":"Caveats","text":"<ul> <li>The analyzer relies on heuristics to identify feature types, which may not always be accurate.</li> <li>Recommendations are based on general patterns and may need adjustment for specific use cases.</li> <li>Performance may degrade with very large CSV files due to memory constraints. </li> </ul>"},{"location":"layers-explorer/","title":"Layers Explorer","text":"\ud83e\udde9 Layers Explorer      Discover and explore 38+ powerful layers in the Keras Model Registry.      Find the perfect layer for your tabular modeling needs with our interactive explorer.    38+ Powerful Layers 5 Categories 100% Keras 3 Native 0% TensorFlow Lock-in \ud83d\udd0d Smart Search &amp; Advanced Filtering \u2715 \ud83d\udcc1 Category All (38) \ud83e\udde0 Attention \ud83d\udd27 Preprocessing \u2699\ufe0f Feature Eng. \ud83c\udfd7\ufe0f Specialized \ud83d\udee0\ufe0f Utility \ud83d\udcca Complexity All \ud83d\udfe2 Beginner \ud83d\udfe1 Intermediate \ud83d\udd34 Advanced \u26a1 Performance All \u26a1 Fast \ud83d\udcbe Memory Eff. \ud83c\udfaf Accurate Showing all 38+ layers \u229e Grid View \u2630 List View"},{"location":"layers-explorer/#featured-layers","title":"\ud83c\udfc6 Featured Layers","text":"\ud83e\udde0 TabularAttention \ud83d\udd25 Popular \u2705 Stable \u2b50 Featured <p>Dual attention mechanism for inter-feature and inter-sample relationships in tabular data. Automatically discovers complex feature interactions.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import TabularAttention\n\nattention = TabularAttention(\n    num_heads=8,\n    key_dim=64,\n    dropout=0.1\n)\noutput = attention(inputs)</code></pre> \u26a1 Fast \ud83d\udcbe Memory Efficient \ud83c\udfaf High Accuracy \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83e\udde0 MultiResolutionTabularAttention \ud83d\ude80 Advanced \u2705 Stable <p>Multi-resolution attention mechanism that processes numerical and categorical features separately, then combines them with cross-attention.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import MultiResolutionTabularAttention\n\nattention = MultiResolutionTabularAttention(\n    num_heads=4,\n    key_dim=32,\n    dropout=0.1\n)\noutput = attention(inputs)</code></pre> \ud83c\udfaf High Accuracy \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \u2699\ufe0f VariableSelection \ud83d\udd25 Popular \u2705 Stable \ud83d\udfe2 Beginner <p>Intelligently selects the most relevant features for your model using learnable gating mechanisms and residual connections.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import VariableSelection\n\nselector = VariableSelection(\n    num_features=10,\n    dropout=0.1\n)\noutput = selector(inputs)</code></pre> \u26a1 Fast \ud83d\udcbe Memory Efficient \ud83c\udfaf High Accuracy \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \u2699\ufe0f GatedFeatureFusion \u2705 Stable \ud83d\udfe1 Intermediate <p>Combines two feature representations using a learned gating mechanism to balance their contributions optimally.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import GatedFeatureFusion\n\nfusion = GatedFeatureFusion(\n    hidden_dim=64,\n    dropout=0.1\n)\noutput = fusion([features1, features2])</code></pre> \ud83c\udfaf High Accuracy \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83d\udd27 DifferentiableTabularPreprocessor \ud83d\udd25 Popular \u2705 Stable <p>End-to-end differentiable preprocessing for tabular data with learnable imputation and normalization.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import DifferentiableTabularPreprocessor\n\npreprocessor = DifferentiableTabularPreprocessor(\n    numerical_features=['age', 'income'],\n    categorical_features=['category', 'region']\n)\noutput = preprocessor(inputs)</code></pre> \u26a1 Fast \ud83d\udcbe Memory Efficient \ud83c\udfaf High Accuracy \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83d\udd27 DateEncodingLayer \u2705 Stable \ud83d\udfe2 Beginner <p>Encodes date components into cyclical features using sine and cosine transformations for better temporal modeling.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import DateEncodingLayer\n\ndate_encoder = DateEncodingLayer(\n    min_year=1900,\n    max_year=2100\n)\noutput = date_encoder(date_components)</code></pre> \u26a1 Fast \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83c\udfd7\ufe0f GatedResidualNetwork \ud83d\udd25 Popular \u2705 Stable \ud83d\ude80 Advanced <p>Gated residual network for complex feature interactions with improved gradient flow and feature transformation.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import GatedResidualNetwork\n\ngrn = GatedResidualNetwork(\n    units=128,\n    dropout_rate=0.1\n)\noutput = grn(inputs)</code></pre> \ud83c\udfaf High Accuracy \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83c\udfd7\ufe0f TabularMoELayer \ud83d\ude80 Advanced \u2705 Stable <p>Mixture of Experts layer that routes input features through multiple expert networks with learnable gating.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import TabularMoELayer\n\nmoe = TabularMoELayer(\n    num_experts=4,\n    expert_units=16\n)\noutput = moe(inputs)</code></pre> \ud83c\udfaf High Accuracy \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83d\udee0\ufe0f CastToFloat32Layer \u2705 Stable \ud83d\udfe2 Beginner <p>Simple utility layer that casts input tensors to float32 data type for consistent data types in models.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import CastToFloat32Layer\n\ncast_layer = CastToFloat32Layer()\noutput = cast_layer(inputs)</code></pre> \u26a1 Fast \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples \ud83d\udee0\ufe0f NumericalAnomalyDetection \u2705 Stable \ud83d\udfe1 Intermediate <p>Detects numerical anomalies using statistical methods and learned thresholds for robust data processing.</p> Quick Example \ud83d\udccb <pre><code>from kmr.layers import NumericalAnomalyDetection\n\nanomaly_detector = NumericalAnomalyDetection(\n    threshold=2.0,\n    method='zscore'\n)\noutput = anomaly_detector(inputs)</code></pre> \ud83c\udfaf High Accuracy \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udca1 Examples"},{"location":"layers-explorer/#browse-by-category","title":"\ud83d\udcda Browse by Category","text":""},{"location":"layers-explorer/#attention-layers","title":"\ud83e\udde0 Attention Layers","text":"<p>8 layers available</p> <p>Core attention mechanisms for tabular data</p> <ul> <li>TabularAttention</li> <li>MultiResolutionTabularAttention</li> <li>ColumnAttention</li> <li>RowAttention</li> <li>SparseAttentionWeighting</li> </ul> <p>Explore All \u2192</p>"},{"location":"layers-explorer/#preprocessing-layers","title":"\ud83d\udd27 Preprocessing Layers","text":"<p>6 layers available</p> <p>Data preprocessing and transformation</p> <ul> <li>DifferentiableTabularPreprocessor</li> <li>DifferentialPreprocessingLayer</li> <li>DateParsingLayer</li> <li>DateEncodingLayer</li> <li>CastToFloat32Layer</li> <li>SeasonLayer</li> </ul> <p>Explore All \u2192</p>"},{"location":"layers-explorer/#feature-engineering","title":"\u2699\ufe0f Feature Engineering","text":"<p>8 layers available</p> <p>Advanced feature engineering and selection</p> <ul> <li>VariableSelection</li> <li>GatedFeatureSelection</li> <li>GatedFeatureFusion</li> <li>AdvancedNumericalEmbedding</li> <li>DistributionAwareEncoder</li> <li>DistributionTransformLayer</li> </ul> <p>Explore All \u2192</p>"},{"location":"layers-explorer/#specialized-layers","title":"\ud83c\udfd7\ufe0f Specialized Layers","text":"<p>10 layers available</p> <p>Specialized architectures and techniques</p> <ul> <li>GatedResidualNetwork</li> <li>GatedLinearUnit</li> <li>TransformerBlock</li> <li>TabularMoELayer</li> <li>BoostingBlock</li> <li>BoostingEnsembleLayer</li> </ul> <p>Explore All \u2192</p>"},{"location":"layers-explorer/#utility-layers","title":"\ud83d\udee0\ufe0f Utility Layers","text":"<p>6 layers available</p> <p>Utility and helper layers</p> <ul> <li>BusinessRulesLayer</li> <li>FeatureCutout</li> <li>StochasticDepth</li> <li>SlowNetwork</li> <li>HyperZZWOperator</li> <li>GraphFeatureAggregation</li> </ul> <p>Explore All \u2192</p>"},{"location":"layers-explorer/#all-layers","title":"\ud83d\udcca All Layers","text":""},{"location":"layers-explorer/#attention-layers_1","title":"Attention Layers","text":"<p>Core attention mechanisms for tabular data</p> <ul> <li>TabularAttention - Dual attention for feature and sample relationships</li> <li>MultiResolutionTabularAttention - Multi-resolution attention</li> <li>ColumnAttention - Column-wise attention mechanism</li> <li>RowAttention - Row-wise attention mechanism</li> <li>SparseAttentionWeighting - Sparse attention weights</li> </ul>"},{"location":"layers-explorer/#preprocessing-layers_1","title":"Preprocessing Layers","text":"<p>Data preprocessing and transformation</p> <ul> <li>DifferentiableTabularPreprocessor - End-to-end differentiable preprocessing</li> <li>DifferentialPreprocessingLayer - Differential preprocessing</li> <li>DateParsingLayer - Date parsing and extraction</li> <li>DateEncodingLayer - Date feature encoding</li> <li>CastToFloat32Layer - Type casting utility</li> <li>SeasonLayer - Seasonal feature extraction</li> </ul>"},{"location":"layers-explorer/#feature-engineering-layers","title":"Feature Engineering Layers","text":"<p>Advanced feature engineering and selection</p> <ul> <li>VariableSelection - Intelligent feature selection</li> <li>GatedFeatureSelection - Gated feature selection</li> <li>GatedFeatureFusion - Gated feature fusion</li> <li>AdvancedNumericalEmbedding - Advanced numerical embeddings</li> <li>DistributionAwareEncoder - Distribution-aware encoding</li> <li>DistributionTransformLayer - Distribution transformation</li> </ul>"},{"location":"layers-explorer/#specialized-layers_1","title":"Specialized Layers","text":"<p>Specialized architectures and techniques</p> <ul> <li>GatedResidualNetwork - Gated residual network</li> <li>GatedLinearUnit - Gated linear unit</li> <li>TransformerBlock - Transformer block for tabular data</li> <li>TabularMoELayer - Mixture of Experts for tabular data</li> <li>BoostingBlock - Boosting block implementation</li> <li>BoostingEnsembleLayer - Boosting ensemble layer</li> </ul>"},{"location":"layers-explorer/#utility-layers_1","title":"Utility Layers","text":"<p>Utility and helper layers</p> <ul> <li>BusinessRulesLayer - Business rules enforcement</li> <li>FeatureCutout - Feature cutout augmentation</li> <li>StochasticDepth - Stochastic depth regularization</li> <li>SlowNetwork - Multi-layer slow network</li> <li>HyperZZWOperator - HyperZZW operator</li> <li>GraphFeatureAggregation - Graph feature aggregation</li> </ul>"},{"location":"layers-explorer/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Getting Started</p> <ol> <li>Browse the categories above to find relevant layers</li> <li>Search for specific functionality using the search box</li> <li>Click on any layer to view detailed documentation</li> <li>Copy code examples to get started quickly</li> </ol> <p>Example Usage</p> <pre><code>import keras\nfrom kmr.layers import TabularAttention, VariableSelection\n\n# Create a simple model\ninputs = keras.Input(shape=(10,))\nx = VariableSelection(num_features=5)(inputs)\nx = TabularAttention(num_heads=4, key_dim=32)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers-explorer/#performance-comparison","title":"\ud83d\udcc8 Performance Comparison","text":"Layer Performance Characteristics <p>Compare layers by their performance metrics and characteristics</p> \ud83e\udde0 Attention Layers 5 layers Speed \u26a1\u26a1\u26a1 Memory \ud83d\udcbe\ud83d\udcbe Accuracy \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Complex Interactions Feature Relationships Sample Relationships \ud83d\udd27 Preprocessing 6 layers Speed \u26a1\u26a1\u26a1\u26a1 Memory \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Accuracy \ud83c\udfaf\ud83c\udfaf Data Preparation Type Conversion Date Processing \u2699\ufe0f Feature Engineering 8 layers Speed \u26a1\u26a1 Memory \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Accuracy \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Feature Selection Feature Fusion Embeddings \ud83c\udfd7\ufe0f Specialized 11 layers Speed \u26a1\u26a1 Memory \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Accuracy \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Advanced Architectures Mixture of Experts Boosting \ud83d\udee0\ufe0f Utility 8 layers Speed \u26a1\u26a1\u26a1\u26a1 Memory \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Accuracy \ud83c\udfaf Helper Functions Anomaly Detection Graph Processing"},{"location":"layers-explorer/#layer-selection-guide","title":"\ud83c\udfaf Layer Selection Guide","text":"\ud83d\ude80 Quick Start <p>New to KMR? Start with these beginner-friendly layers:</p> VariableSelection <p>Automatic feature selection for any tabular dataset</p> \ud83d\udfe2 Beginner CastToFloat32Layer <p>Simple data type conversion utility</p> \ud83d\udfe2 Beginner DateEncodingLayer <p>Encode temporal features for better modeling</p> \ud83d\udfe2 Beginner \ud83c\udfaf Performance Optimization <p>Need speed and efficiency? These layers are optimized for performance:</p> DifferentiableTabularPreprocessor <p>Fast, end-to-end preprocessing</p> \u26a1 Fast TabularAttention <p>Efficient attention for tabular data</p> \u26a1 Fast FeatureCutout <p>Lightweight data augmentation</p> \u26a1 Fast \ud83c\udfc6 Maximum Accuracy <p>For the best possible results, use these high-accuracy layers:</p> GatedResidualNetwork <p>Advanced feature interactions</p> \ud83c\udfaf High Accuracy TabularMoELayer <p>Mixture of experts for complex patterns</p> \ud83c\udfaf High Accuracy MultiResolutionTabularAttention <p>Multi-resolution attention mechanism</p> \ud83c\udfaf High Accuracy"},{"location":"layers-explorer/#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Tutorials - Step-by-step guides</li> <li>Examples - Real-world examples</li> <li>Contributing - How to contribute new layers</li> </ul>"},{"location":"layers_implementation_guide/","title":"Layer Implementation Guide for Keras Model Registry (KMR)","text":"<p>This document outlines the standard patterns and best practices for implementing new layers in the Keras Model Registry project.</p>"},{"location":"layers_implementation_guide/#key-requirements","title":"Key Requirements","text":"<ol> <li>Keras 3 Only: All layer implementations MUST use only Keras 3 operations. NO TensorFlow dependencies are allowed in layer implementations.</li> <li>TensorFlow Usage: TensorFlow can ONLY be used in test files for validation purposes.</li> </ol>"},{"location":"layers_implementation_guide/#layer-structure","title":"Layer Structure","text":"<p>All layers in the KMR project should follow this structure:</p> <ol> <li>Module Docstring: Describe the purpose and functionality of the layer.</li> <li>Imports: Import necessary dependencies (Keras only, no TensorFlow).</li> <li>Class Definition: Define the layer class inheriting from <code>BaseLayer</code>.</li> <li>Class Docstring: Comprehensive documentation including:</li> <li>General description</li> <li>Parameters with types and defaults</li> <li>Input/output shapes</li> <li>Usage examples</li> <li>Implementation: The actual layer implementation using only Keras 3 operations.</li> </ol>"},{"location":"layers_implementation_guide/#required-components","title":"Required Components","text":"<p>Every layer must include:</p> <ol> <li>Keras Serialization: Use the <code>@register_keras_serializable(package=\"kmr.layers\")</code> decorator.</li> <li>BaseLayer Inheritance: Inherit from <code>kmr.layers._base_layer.BaseLayer</code>.</li> <li>Type Annotations: Use proper type hints for all methods and parameters.</li> <li>Parameter Validation: Validate input parameters in <code>__init__</code> or <code>_validate_params</code>.</li> <li>Logging: Use loguru for logging important information.</li> <li>Serialization Support: Implement <code>get_config()</code> method properly.</li> </ol>"},{"location":"layers_implementation_guide/#implementation-pattern","title":"Implementation Pattern","text":"<p>Follow this pattern for implementing layers:</p> <pre><code>\"\"\"\nModule docstring describing the layer's purpose and functionality.\n\"\"\"\n\nfrom typing import Any\nfrom loguru import logger\nfrom keras import layers, ops\nfrom keras import KerasTensor\nfrom keras.saving import register_keras_serializable\nfrom kmr.layers._base_layer import BaseLayer\n\n@register_keras_serializable(package=\"kmr.layers\")\nclass MyCustomLayer(BaseLayer):\n    \"\"\"Detailed class docstring with description, parameters, and examples.\n\n    Args:\n        param1: Description of param1.\n        param2: Description of param2.\n        ...\n\n    Input shape:\n        Description of input shape.\n\n    Output shape:\n        Description of output shape.\n\n    Example:\n        ```python\n        import keras\n        # Usage example with Keras operations only\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        param1: type = default,\n        param2: type = default,\n        name: str | None = None,\n        **kwargs: Any\n    ) -&gt; None:\n        # Set private attributes before calling parent's __init__\n        self._param1 = param1\n        self._param2 = param2\n\n        # Validate parameters\n        if not valid_condition:\n            raise ValueError(\"Error message\")\n\n        # IMPORTANT: Set public attributes BEFORE calling parent's __init__\n        # This is necessary because BaseLayer._log_initialization() calls get_config()\n        # which accesses these public attributes\n        self.param1 = self._param1\n        self.param2 = self._param2\n\n        # Initialize any other instance variables\n        self.some_variable = None\n\n        # Call parent's __init__ after setting public attributes\n        super().__init__(name=name, **kwargs)\n\n    def _validate_params(self) -&gt; None:\n        \"\"\"Validate layer parameters.\"\"\"\n        if not valid_condition:\n            raise ValueError(\"Error message\")\n\n    def build(self, input_shape: tuple[int, ...]) -&gt; None:\n        \"\"\"Builds the layer with the given input shape.\n\n        Args:\n            input_shape: Tuple of integers defining the input shape.\n        \"\"\"\n        # Create weights and sublayers\n\n        logger.debug(f\"Layer built with params: {self.param1}, {self.param2}\")\n        super().build(input_shape)\n\n    def call(self, inputs: KerasTensor, training: bool | None = None) -&gt; KerasTensor:\n        \"\"\"Forward pass of the layer.\n\n        Args:\n            inputs: Input tensor.\n            training: Boolean indicating whether the layer should behave in\n                training mode or inference mode.\n\n        Returns:\n            Output tensor.\n        \"\"\"\n        # Implement forward pass using ONLY Keras operations\n        # Use ops.xxx instead of tf.xxx\n        return output\n\n    def get_config(self) -&gt; dict[str, Any]:\n        \"\"\"Returns the config of the layer.\n\n        Returns:\n            Python dictionary containing the layer configuration.\n        \"\"\"\n        config = super().get_config()\n        config.update({\n            \"param1\": self.param1,\n            \"param2\": self.param2,\n        })\n        return config\n</code></pre>"},{"location":"layers_implementation_guide/#attribute-initialization-order","title":"Attribute Initialization Order","text":"<p>Pay special attention to the order of operations in the <code>__init__</code> method:</p> <ol> <li>Set private attributes first (<code>self._param1 = param1</code>)</li> <li>Validate parameters</li> <li>Set public attributes (<code>self.param1 = self._param1</code>)</li> <li>Initialize any other instance variables</li> <li>Call <code>super().__init__(name=name, **kwargs)</code></li> </ol> <p>This order is critical because <code>BaseLayer._log_initialization()</code> is called during <code>super().__init__()</code> and it calls <code>get_config()</code>, which accesses the public attributes. If the public attributes are not set before calling <code>super().__init__()</code>, you'll get an <code>AttributeError</code>.</p>"},{"location":"layers_implementation_guide/#keras-3-operations-reference","title":"Keras 3 Operations Reference","text":"<p>When implementing layers, use Keras 3 operations instead of TensorFlow operations:</p> TensorFlow Keras 3 tf.stack keras.ops.stack tf.reshape keras.ops.reshape tf.reduce_sum keras.ops.sum tf.reduce_mean keras.ops.mean tf.reduce_max keras.ops.max tf.reduce_min keras.ops.min tf.nn.softmax keras.ops.softmax tf.concat keras.ops.concatenate tf.math.pow keras.ops.power tf.abs keras.ops.absolute <p>For a more comprehensive list, refer to the KERAS_DICT.md file.</p>"},{"location":"layers_implementation_guide/#testing","title":"Testing","text":"<p>Each layer should have a corresponding test file in <code>tests/layers/</code> with the naming pattern <code>test__LayerName.py</code>. Tests should include:</p> <ol> <li>Initialization Tests: Test default and custom initialization.</li> <li>Invalid Parameter Tests: Test error handling for invalid parameters.</li> <li>Build Tests: Test layer building with different configurations.</li> <li>Output Shape Tests: Test that output shape matches expectations.</li> <li>Training Mode Tests: Test behavior in training vs inference modes.</li> <li>Serialization Tests: Test serialization and deserialization.</li> <li>Functional Tests: Test specific functionality of the layer.</li> <li>Integration Tests: Test integration with a simple model.</li> </ol> <p>Note: TensorFlow can be used in test files for validation purposes, but should be clearly marked as such.</p>"},{"location":"layers_implementation_guide/#registration","title":"Registration","text":"<p>After implementing a new layer:</p> <ol> <li>Add an import statement in <code>kmr/layers/__init__.py</code></li> <li>Add the layer name to the <code>__all__</code> list in the same file.</li> </ol>"},{"location":"layers_implementation_guide/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>TensorFlow Dependencies: NEVER use TensorFlow operations in layer implementations.</li> <li>Incorrect Attribute Initialization Order: Always set public attributes BEFORE calling <code>super().__init__()</code>.</li> <li>Missing Imports: Ensure all necessary imports are included.</li> <li>Incomplete Serialization: Make sure all parameters are included in <code>get_config()</code>.</li> <li>Missing Type Hints: Always include proper type annotations.</li> <li>Insufficient Documentation: Always provide comprehensive docstrings.</li> <li>Improper Validation: Always validate input parameters. </li> </ol>"},{"location":"api/layers/","title":"Layers API Reference - KMR","text":"\ud83e\udde9 Layers API Reference 38+ production-ready layers designed exclusively for Keras 3.     Build sophisticated tabular models with advanced attention, feature engineering, and preprocessing layers.    38+ Production Layers 5 Categories 100% Keras 3 Native 0% TensorFlow Lock-in"},{"location":"api/layers/#why-use-kmr-layers","title":"\ud83c\udfaf Why Use KMR Layers?","text":"Challenge Traditional Approach KMR's Solution \ud83d\udd17 Feature Interactions Manual feature crosses \ud83d\udc41\ufe0f Tabular Attention - Automatic relationship discovery \ud83c\udff7\ufe0f Mixed Feature Types Uniform processing \ud83e\udde9 Feature-wise Layers - Specialized processing per feature \ud83d\udcca Complex Distributions Fixed strategies \ud83d\udcca Distribution-Aware Encoding - Adaptive transformations \u26a1 Performance Optimization Post-hoc analysis \ud83c\udfaf Built-in Selection - Learned during training \ud83d\udd12 Production Readiness Extra tooling needed \u2705 Battle-Tested - Used in production models"},{"location":"api/layers/#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":"\ud83e\udde0 Attention Layers <ul> <li>5 layers for feature relationships</li> <li>TabularAttention</li> <li>MultiResolution</li> <li>Column &amp; Row Attention</li> <li>Interpretable Heads</li> </ul> \ud83d\udd27 Preprocessing Layers <ul> <li>6 layers for data prep</li> <li>Tabular Preprocessor</li> <li>Date Encoding</li> <li>Season Layer</li> <li>Type Casting</li> </ul> \u2699\ufe0f Feature Engineering <ul> <li>8 layers for features</li> <li>Variable Selection</li> <li>Advanced Embedding</li> <li>Distribution Aware</li> <li>Feature Cutout</li> </ul> \ud83c\udfd7\ufe0f Specialized Architectures <ul> <li>11 advanced layers</li> <li>Gated Residual Network</li> <li>Transformer Block</li> <li>Mixture of Experts</li> <li>Boosting Ensemble</li> </ul> \ud83d\udee0\ufe0f Utility Layers <ul> <li>8 utility layers</li> <li>Graph Features</li> <li>Anomaly Detection</li> <li>Text Processing</li> <li>Graph Preprocessing</li> </ul>"},{"location":"api/layers/#key-features","title":"\u2728 Key Features","text":"\ud83d\udc41\ufe0f Attention Mechanisms <p>Automatically discover feature relationships and sample importance with advanced attention layers.</p> \ud83e\udde9 Feature-wise Processing <p>Each feature receives specialized processing through mixture of experts and dedicated layers.</p> \ud83d\udcca Distribution-Aware <p>Automatically adapt to different distributions with intelligent encoding and transformations.</p> \u26a1 Performance Ready <p>Optimized for production with built-in regularization and efficient memory usage.</p> \ud83c\udfaf Built-in Optimization <p>Learn which features matter during training, not after with integrated feature selection.</p> \ud83d\udd12 Production Proven <p>Battle-tested in real-world ML pipelines with comprehensive testing and documentation.</p> \ud83d\udca1 Pro Tip: Start with TabularAttention for feature relationships, VariableSelection for feature importance, and DifferentiableTabularPreprocessor for end-to-end preprocessing. Combine them for powerful custom architectures."},{"location":"api/layers/#attention-layers","title":"\ud83e\udde0 Attention Layers\ud83e\udde0 Attention Mechanisms","text":"<p>Advanced attention layers for capturing complex feature relationships and dependencies in tabular data.</p> 5 layers High Performance Complex Interactions TabularAttention \ud83d\udd25 Popular \u2705 Stable <p>Dual attention mechanism for inter-feature and inter-sample relationships in tabular data.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83d\udcbe Memory Efficient | \ud83c\udfaf High Accuracy \ud83d\udcd6 Full Docs \ud83d\udccb API MultiResolutionTabularAttention \ud83d\ude80 Advanced \u2705 Stable <p>Multi-resolution attention mechanism that processes numerical and categorical features separately.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf High Accuracy | \ud83d\udcbe Memory Efficient \ud83d\udcd6 Full Docs \ud83d\udccb API ColumnAttention \u2705 Stable <p>Column-wise attention for tabular data to capture feature-level relationships.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Interpretable \ud83d\udcd6 Full Docs \ud83d\udccb API RowAttention \u2705 Stable <p>Row-wise attention mechanisms for sample-level pattern recognition.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Sample Relationships \ud83d\udcd6 Full Docs \ud83d\udccb API InterpretableMultiHeadAttention \ud83d\ude80 Advanced \u2705 Stable <p>Interpretable multi-head attention with attention weight analysis and visualization.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf Interpretable | \ud83d\udd0d Explainable \ud83d\udcd6 Full Docs \ud83d\udccb API"},{"location":"api/layers/#preprocessing-layers","title":"\ud83d\udd27 Preprocessing Layers\ud83d\udd27 Preprocessing &amp; Data Transformation","text":"<p>Essential preprocessing layers for data cleaning, transformation, and preparation for optimal model performance.</p> 6 layers Beginner Friendly Data Quality DifferentiableTabularPreprocessor \ud83d\udd25 Popular \u2705 Stable <p>End-to-end differentiable preprocessing for tabular data with learnable imputation and normalization.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Learnable | \ud83d\udd27 End-to-End \ud83d\udcd6 Full Docs \ud83d\udccb API DifferentialPreprocessingLayer \u2705 Stable <p>Advanced preprocessing with multiple candidate transformations and learnable combination.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf Adaptive | \ud83d\udd27 Multiple Transforms \ud83d\udcd6 Full Docs \ud83d\udccb API DateParsingLayer \u2705 Stable <p>Flexible date parsing and extraction from various date formats and strings.</p> Input: (batch_size, 1) - date strings Output: (batch_size, 4) - [year, month, day, dow] Performance: \u26a1 Fast | \ud83d\udcc5 Date Processing \ud83d\udcd6 Full Docs \ud83d\udccb API DateEncodingLayer \ud83d\udd25 Popular \u2705 Stable <p>Comprehensive date and time feature encoding with cyclical representations.</p> Input: (batch_size, 4) - [year, month, day, dow] Output: (batch_size, 8) - cyclical encodings Performance: \u26a1 Fast | \ud83d\udcc5 Cyclical Encoding \ud83d\udcd6 Full Docs \ud83d\udccb API SeasonLayer \u2705 Stable <p>Seasonal feature extraction from date/time data for temporal pattern recognition.</p> Input: (batch_size, 4) - [year, month, day, dow] Output: (batch_size, 8) - original + 4 seasons Performance: \u26a1 Fast | \ud83c\udf38 Seasonal Patterns \ud83d\udcd6 Full Docs \ud83d\udccb API CastToFloat32Layer \u2705 Stable <p>Type casting utility layer for ensuring consistent data types throughout the model.</p> Input: Any numeric tensor Output: float32 tensor Performance: \u26a1 Fast | \ud83d\udd27 Utility \ud83d\udcd6 Full Docs \ud83d\udccb API"},{"location":"api/layers/#feature-engineering-layers","title":"\u2699\ufe0f Feature Engineering Layers\u2699\ufe0f Feature Engineering &amp; Selection","text":"<p>Advanced feature engineering layers for intelligent feature selection, transformation, and representation learning.</p> 8 layers Feature Intelligence High Performance VariableSelection \ud83d\udd25 Popular \u2705 Stable <p>Intelligent variable selection network for identifying important features using gated residual networks.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Interpretable | \ud83d\udd27 Feature Selection \ud83d\udcd6 Full Docs \ud83d\udccb API GatedFeatureSelection \u2705 Stable <p>Learnable feature selection with gating network and residual connection for adaptive feature importance.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Learnable | \ud83d\udd27 Adaptive \ud83d\udcd6 Full Docs \ud83d\udccb API GatedFeatureFusion \ud83d\udd25 Popular \u2705 Stable <p>Gated mechanism for intelligently fusing multiple feature representations with learnable weights.</p> Input: List of feature tensors Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83d\udcbe Memory Efficient | \ud83d\udd27 Flexible \ud83d\udcd6 Full Docs \ud83d\udccb API AdvancedNumericalEmbedding \ud83d\ude80 Advanced \u2705 Stable <p>Advanced numerical feature embedding with dual-branch architecture for continuous and discrete features.</p> Input: (batch_size, num_features) Output: (batch_size, embedding_dim) Performance: \u26a1 Fast | \ud83c\udfaf High Accuracy | \ud83d\udd27 Flexible \ud83d\udcd6 Full Docs \ud83d\udccb API DistributionAwareEncoder \u2705 Stable <p>Distribution-aware feature encoding that automatically detects distribution type and applies appropriate transformations.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf Adaptive | \ud83d\udd27 Auto Detection \ud83d\udcd6 Full Docs \ud83d\udccb API DistributionTransformLayer \ud83d\udd25 Popular \u2705 Stable <p>Automatic distribution transformation for numerical features to improve model performance.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Accurate | \ud83d\ude0a Easy to Use \ud83d\udcd6 Full Docs \ud83d\udccb API SparseAttentionWeighting \u2705 Stable <p>Sparse attention weighting mechanisms for efficient computation and selective feature combination.</p> Input: List of feature tensors Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83d\udcbe Memory Efficient | \ud83d\udd27 Sparse \ud83d\udcd6 Full Docs \ud83d\udccb API FeatureCutout \u2705 Stable <p>Feature cutout for data augmentation and regularization in tabular data by randomly masking features.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Regularization | \ud83d\udd27 Augmentation \ud83d\udcd6 Full Docs \ud83d\udccb API"},{"location":"api/layers/#specialized-layers","title":"\ud83c\udfd7\ufe0f Specialized Layers\ud83c\udfd7\ufe0f Specialized Architectures","text":"<p>Advanced specialized layers for specific use cases including gated networks, boosting, business rules, and anomaly detection.</p> 11 layers Advanced Specialized Use Cases GatedResidualNetwork \u2705 Stable <p>Gated residual network combining residual connections with gated linear units for improved gradient flow.</p> Input: (batch_size, input_dim) Output: (batch_size, units) Performance: \u26a1 Fast | \ud83c\udfaf High Accuracy | \ud83d\udd27 Gradient Flow \ud83d\udcd6 Full Docs \ud83d\udccb API GatedLinearUnit \u2705 Stable <p>Gated linear unit for intelligent feature gating and selective information flow control.</p> Input: (batch_size, input_dim) Output: (batch_size, units) Performance: \u26a1 Fast | \ud83c\udfaf Selective | \ud83d\udd27 Gating \ud83d\udcd6 Full Docs \ud83d\udccb API TransformerBlock \ud83d\ude80 Advanced \u2705 Stable <p>Standard transformer block with multi-head attention and feed-forward networks for tabular data.</p> Input: (batch_size, dim_model) or (batch_size, seq_len, dim_model) Output: Same as input Performance: \u26a1 Fast | \ud83c\udfaf High Accuracy | \ud83d\udd27 Flexible \ud83d\udcd6 Full Docs \ud83d\udccb API TabularMoELayer \ud83d\ude80 Advanced \u2705 Stable <p>Mixture of Experts for tabular data with adaptive expert selection and routing.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf High Accuracy | \ud83d\udd27 Adaptive | \ud83d\udcbe Scalable \ud83d\udcd6 Full Docs \ud83d\udccb API BoostingBlock \u2705 Stable <p>Gradient boosting inspired neural network block for sequential learning and residual correction.</p> Input: (batch_size, input_dim) Output: (batch_size, input_dim) Performance: \u26a1 Fast | \ud83c\udfaf Boosting | \ud83d\udd27 Sequential \ud83d\udcd6 Full Docs \ud83d\udccb API BoostingEnsembleLayer \ud83d\ude80 Advanced \u2705 Stable <p>Ensemble of boosting blocks for improved performance and robustness through parallel weak learners.</p> Input: (batch_size, input_dim) Output: (batch_size, input_dim) Performance: \ud83c\udfaf High Accuracy | \ud83d\udd27 Ensemble | \ud83d\udcbe Parallel \ud83d\udcd6 Full Docs \ud83d\udccb API BusinessRulesLayer \u2705 Stable <p>Integration of business rules and domain knowledge into neural networks for anomaly detection.</p> Input: (batch_size, 1) Output: Dictionary with anomaly flags and violations Performance: \u26a1 Fast | \ud83c\udfaf Domain Knowledge | \ud83d\udd27 Rules \ud83d\udcd6 Full Docs \ud83d\udccb API StochasticDepth \u2705 Stable <p>Stochastic depth regularization for improved training and generalization in deep networks.</p> Input: Any tensor Output: Same as input Performance: \u26a1 Fast | \ud83c\udfaf Better Generalization | \ud83d\udd27 Regularization \ud83d\udcd6 Full Docs \ud83d\udccb API SlowNetwork \ud83d\ude80 Advanced \u2705 Stable <p>Slow network architecture for careful and deliberate feature processing with controlled information flow.</p> Input: (batch_size, input_dim) Output: (batch_size, output_dim) Performance: \ud83c\udfaf High Accuracy | \ud83d\udd27 Controlled | \u23f1\ufe0f Deliberate \ud83d\udcd6 Full Docs \ud83d\udccb API HyperZZWOperator \ud83d\ude80 Advanced \u2705 Stable <p>Hyperparameter-aware operator for adaptive model behavior and dynamic parameter adjustment.</p> Input: (batch_size, input_dim) Output: (batch_size, output_dim) Performance: \ud83c\udfaf Adaptive | \ud83d\udd27 Hyperparameter-aware | \u2699\ufe0f Dynamic \ud83d\udcd6 Full Docs \ud83d\udccb API TextPreprocessingLayer \u2705 Stable <p>Text preprocessing utilities for natural language features in tabular data with tokenization and encoding.</p> Input: (batch_size, 1) - text strings Output: (batch_size, max_length) - tokenized sequences Performance: \u26a1 Fast | \ud83d\udcdd Text Processing | \ud83d\udd27 NLP \ud83d\udcd6 Full Docs \ud83d\udccb API"},{"location":"api/layers/#utility-layers","title":"\ud83d\udee0\ufe0f Utility Layers\ud83d\udee0\ufe0f Utility &amp; Graph Layers","text":"<p>Essential utility layers for data processing, graph operations, and anomaly detection in tabular data.</p> 8 layers Essential Data Processing CastToFloat32Layer \u2705 Stable <p>Type casting utility layer for ensuring consistent data types throughout the model.</p> Input: Any numeric tensor Output: float32 tensor Performance: \u26a1 Fast | \ud83d\udd27 Utility | \ud83d\udcca Type Safety \ud83d\udcd6 Full Docs \ud83d\udccb API AdvancedGraphFeature \ud83d\ude80 Advanced \u2705 Stable <p>Advanced graph-based feature processing with dynamic adjacency learning and relationship modeling.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf High Accuracy | \ud83d\udd27 Graph Processing | \ud83d\udd78\ufe0f Relationships \ud83d\udcd6 Full Docs \ud83d\udccb API GraphFeatureAggregation \u2705 Stable <p>Graph feature aggregation mechanisms for relationship modeling and feature interaction learning.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \u26a1 Fast | \ud83c\udfaf Aggregation | \ud83d\udd78\ufe0f Graph Features \ud83d\udcd6 Full Docs \ud83d\udccb API MultiHeadGraphFeaturePreprocessor \ud83d\ude80 Advanced \u2705 Stable <p>Multi-head graph feature preprocessing for complex feature interactions and relationship learning.</p> Input: (batch_size, num_features) Output: (batch_size, num_features) Performance: \ud83c\udfaf High Accuracy | \ud83d\udd27 Multi-Head | \ud83d\udd78\ufe0f Complex Interactions \ud83d\udcd6 Full Docs \ud83d\udccb API NumericalAnomalyDetection \u2705 Stable <p>Anomaly detection for numerical features using statistical and machine learning methods.</p> Input: (batch_size, num_features) Output: Dictionary with anomaly scores and flags Performance: \u26a1 Fast | \ud83c\udfaf Anomaly Detection | \ud83d\udd0d Statistical \ud83d\udcd6 Full Docs \ud83d\udccb API CategoricalAnomalyDetectionLayer \u2705 Stable <p>Anomaly detection for categorical features with pattern recognition and frequency analysis.</p> Input: (batch_size, num_features) Output: Dictionary with anomaly scores and flags Performance: \u26a1 Fast | \ud83c\udfaf Categorical | \ud83d\udd0d Pattern Recognition \ud83d\udcd6 Full Docs \ud83d\udccb API"},{"location":"api/layers/#complete-api-reference","title":"\ud83d\udccb Complete API Reference\ud83d\udccb Complete API Reference","text":"<p>Detailed API documentation for all 38+ layers with parameters, methods, and examples.</p> \ud83e\udde0 Attention \ud83d\udd27 Preprocessing \u2699\ufe0f Feature Engineering \ud83c\udfd7\ufe0f Specialized \ud83d\udee0\ufe0f Utility TabularAttention <code>TabularAttention(num_heads=8, key_dim=64, dropout=0.1, use_attention_weights=True, attention_activation='softmax')</code> <p>Dual attention mechanism for inter-feature and inter-sample relationships in tabular data.</p> Parameters <ul> <li><code>num_heads</code> (int): Number of attention heads</li> <li><code>key_dim</code> (int): Dimension of key vectors</li> <li><code>dropout</code> (float): Dropout rate</li> <li><code>use_attention_weights</code> (bool): Whether to return attention weights</li> <li><code>attention_activation</code> (str): Activation function for attention</li> </ul> MultiResolutionTabularAttention <code>MultiResolutionTabularAttention(num_heads=8, key_dim=64, dropout=0.1, numerical_heads=4, categorical_heads=4)</code> <p>Multi-resolution attention mechanism that processes numerical and categorical features separately.</p> ColumnAttention <code>ColumnAttention(hidden_dim=64, dropout=0.1, activation='relu')</code> <p>Column-wise attention for tabular data to capture feature-level relationships.</p> RowAttention <code>RowAttention(hidden_dim=64, dropout=0.1, activation='relu')</code> <p>Row-wise attention mechanisms for sample-level pattern recognition.</p> InterpretableMultiHeadAttention <code>InterpretableMultiHeadAttention(num_heads=8, key_dim=64, dropout=0.1, return_attention_scores=True)</code> <p>Interpretable multi-head attention with attention weight analysis and visualization.</p> DifferentiableTabularPreprocessor <code>DifferentiableTabularPreprocessor(imputation_strategy='learnable', normalization='learnable', dropout=0.1)</code> <p>End-to-end differentiable preprocessing for tabular data with learnable imputation and normalization.</p> DifferentialPreprocessingLayer <code>DifferentialPreprocessingLayer(transform_types=['identity', 'affine', 'mlp', 'log'], dropout=0.1)</code> <p>Advanced preprocessing with multiple candidate transformations and learnable combination.</p> DateParsingLayer <code>DateParsingLayer(date_formats=None, default_format='%Y-%m-%d')</code> <p>Flexible date parsing and extraction from various date formats and strings.</p> DateEncodingLayer <code>DateEncodingLayer(min_year=1900, max_year=2100)</code> <p>Comprehensive date and time feature encoding with cyclical representations.</p> SeasonLayer <code>SeasonLayer()</code> <p>Seasonal feature extraction from date/time data for temporal pattern recognition.</p> CastToFloat32Layer <code>CastToFloat32Layer()</code> <p>Type casting utility layer for ensuring consistent data types throughout the model.</p> VariableSelection <code>VariableSelection(hidden_dim=64, dropout=0.1, use_context=False, context_dim=None)</code> <p>Intelligent variable selection network for identifying important features using gated residual networks.</p> GatedFeatureSelection <code>GatedFeatureSelection(hidden_dim=64, dropout=0.1, activation='relu')</code> <p>Learnable feature selection with gating network and residual connection for adaptive feature importance.</p> GatedFeatureFusion <code>GatedFeatureFusion(hidden_dim=128, dropout=0.1, activation='relu')</code> <p>Gated mechanism for intelligently fusing multiple feature representations with learnable weights.</p> AdvancedNumericalEmbedding <code>AdvancedNumericalEmbedding(embedding_dim=64, num_bins=10, hidden_dim=128, dropout=0.1)</code> <p>Advanced numerical feature embedding with dual-branch architecture for continuous and discrete features.</p> DistributionAwareEncoder <code>DistributionAwareEncoder(encoding_dim=64, dropout=0.1, detection_method='auto')</code> <p>Distribution-aware feature encoding that automatically detects distribution type and applies appropriate transformations.</p> DistributionTransformLayer <code>DistributionTransformLayer(transform_type='auto', epsilon=1e-8, method='box-cox')</code> <p>Automatic distribution transformation for numerical features to improve model performance.</p> SparseAttentionWeighting <code>SparseAttentionWeighting(temperature=1.0, dropout=0.1, sparsity_threshold=0.1)</code> <p>Sparse attention weighting mechanisms for efficient computation and selective feature combination.</p> FeatureCutout <code>FeatureCutout(cutout_prob=0.1, noise_value=0.0, training_only=True)</code> <p>Feature cutout for data augmentation and regularization in tabular data by randomly masking features.</p> GatedResidualNetwork <code>GatedResidualNetwork(units, dropout_rate=0.2, name=None)</code> <p>Gated residual network combining residual connections with gated linear units for improved gradient flow.</p> GatedLinearUnit <code>GatedLinearUnit(units, name=None)</code> <p>Gated linear unit for intelligent feature gating and selective information flow control.</p> TransformerBlock <code>TransformerBlock(dim_model=32, num_heads=3, ff_units=16, dropout_rate=0.2)</code> <p>Standard transformer block with multi-head attention and feed-forward networks for tabular data.</p> TabularMoELayer <code>TabularMoELayer(num_experts=4, expert_units=16, name=None)</code> <p>Mixture of Experts for tabular data with adaptive expert selection and routing.</p> BoostingBlock <code>BoostingBlock(hidden_units=64, hidden_activation='relu', gamma_trainable=True, dropout_rate=None)</code> <p>Gradient boosting inspired neural network block for sequential learning and residual correction.</p> BoostingEnsembleLayer <code>BoostingEnsembleLayer(num_learners=3, learner_units=64, hidden_activation='relu', dropout_rate=None)</code> <p>Ensemble of boosting blocks for improved performance and robustness through parallel weak learners.</p> BusinessRulesLayer <code>BusinessRulesLayer(rules, feature_type, trainable_weights=True, weight_initializer='ones')</code> <p>Integration of business rules and domain knowledge into neural networks for anomaly detection.</p> StochasticDepth <code>StochasticDepth(survival_prob=0.8, scale_at_test=True)</code> <p>Stochastic depth regularization for improved training and generalization in deep networks.</p> SlowNetwork <code>SlowNetwork(hidden_units=64, num_layers=3, activation='relu', dropout=0.1)</code> <p>Slow network architecture for careful and deliberate feature processing with controlled information flow.</p> HyperZZWOperator <code>HyperZZWOperator(hidden_units=64, hyperparameter_dim=32, activation='relu')</code> <p>Hyperparameter-aware operator for adaptive model behavior and dynamic parameter adjustment.</p> TextPreprocessingLayer <code>TextPreprocessingLayer(max_length=100, vocab_size=10000, tokenizer='word')</code> <p>Text preprocessing utilities for natural language features in tabular data with tokenization and encoding.</p> CastToFloat32Layer <code>CastToFloat32Layer(name=None)</code> <p>Type casting utility layer for ensuring consistent data types throughout the model.</p> AdvancedGraphFeature <code>AdvancedGraphFeature(hidden_dim=64, num_heads=4, dropout=0.1, use_attention=True)</code> <p>Advanced graph-based feature processing with dynamic adjacency learning and relationship modeling.</p> GraphFeatureAggregation <code>GraphFeatureAggregation(aggregation_method='mean', hidden_dim=64, dropout=0.1)</code> <p>Graph feature aggregation mechanisms for relationship modeling and feature interaction learning.</p> MultiHeadGraphFeaturePreprocessor <code>MultiHeadGraphFeaturePreprocessor(num_heads=4, hidden_dim=64, dropout=0.1, aggregation='concat')</code> <p>Multi-head graph feature preprocessing for complex feature interactions and relationship learning.</p> NumericalAnomalyDetection <code>NumericalAnomalyDetection(method='isolation_forest', contamination=0.1, threshold=0.5)</code> <p>Anomaly detection for numerical features using statistical and machine learning methods.</p> CategoricalAnomalyDetectionLayer <code>CategoricalAnomalyDetectionLayer(method='frequency', threshold=0.01, min_frequency=5)</code> <p>Anomaly detection for categorical features with pattern recognition and frequency analysis.</p>"},{"location":"api/metrics/","title":"\ud83d\udcca Metrics API Reference","text":"<p>Welcome to the KMR Metrics documentation! All metrics are designed to work exclusively with Keras 3 and provide custom implementations for specialized evaluation tasks.</p> <p>What You'll Find Here</p> <p>Each metric includes detailed documentation with: - \u2728 Complete parameter descriptions with types and defaults - \ud83c\udfaf Usage examples showing real-world applications - \u26a1 Best practices and performance considerations - \ud83c\udfa8 When to use guidance for each metric - \ud83d\udd27 Implementation notes for developers</p> <p>Ready-to-Use Metrics</p> <p>These metrics provide specialized functionality that extends Keras' built-in metrics for advanced use cases.</p> <p>Keras 3 Compatibility</p> <p>All metrics inherit from <code>keras.metrics.Metric</code> ensuring full compatibility with Keras 3 and TensorFlow backends.</p>"},{"location":"api/metrics/#custom-metrics","title":"\ud83d\udcca Custom Metrics","text":""},{"location":"api/metrics/#standarddeviation","title":"\ud83d\udcc8 StandardDeviation","text":"<p>Custom metric for calculating the standard deviation of predictions, useful for anomaly detection and uncertainty quantification.</p>"},{"location":"api/metrics/#kmr.metrics.standard_deviation.StandardDeviation","title":"kmr.metrics.standard_deviation.StandardDeviation","text":"<pre><code>StandardDeviation(\n    name: str = \"standard_deviation\", **kwargs: Any\n)\n</code></pre> <p>A custom Keras metric that calculates the standard deviation of the predicted values.</p> <p>This class is a custom implementation of a Keras metric, which calculates the standard deviation of the predicted values during model training. It's particularly useful for anomaly detection tasks where you need to track the variability of model predictions.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>Variable</code> <p>A trainable weight that stores the calculated standard deviation.</p> Example <pre><code>import keras\nfrom kmr.metrics import StandardDeviation\n\n# Create metric\nstd_metric = StandardDeviation(name=\"prediction_std\")\n\n# Update with predictions\npredictions = keras.ops.random.normal((100, 10))\nstd_metric.update_state(predictions)\n\n# Get result\nstd_value = std_metric.result()\nprint(f\"Standard deviation: {std_value}\")\n</code></pre> <p>Initializes the StandardDeviation metric with a given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric. Defaults to 'standard_deviation'.</p> <code>'standard_deviation'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api/metrics/#kmr.metrics.standard_deviation.StandardDeviation-functions","title":"Functions","text":""},{"location":"api/metrics/#kmr.metrics.standard_deviation.StandardDeviation.update_state","title":"update_state","text":"<pre><code>update_state(y_pred: keras.KerasTensor) -&gt; None\n</code></pre> <p>Updates the state of the metric with the standard deviation of the predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>KerasTensor</code> <p>The predicted values.</p> required"},{"location":"api/metrics/#kmr.metrics.standard_deviation.StandardDeviation.result","title":"result","text":"<pre><code>result() -&gt; keras.KerasTensor\n</code></pre> <p>Returns the current state of the metric, i.e., the current standard deviation.</p> <p>Returns:</p> Name Type Description <code>KerasTensor</code> <code>KerasTensor</code> <p>The current standard deviation.</p>"},{"location":"api/metrics/#kmr.metrics.standard_deviation.StandardDeviation.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: dict[str, Any]) -&gt; StandardDeviation\n</code></pre> <p>Creates a new instance of the metric from its config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration of the metric.</p> required <p>Returns:</p> Name Type Description <code>StandardDeviation</code> <code>StandardDeviation</code> <p>A new instance of the metric.</p>"},{"location":"api/metrics/#median","title":"\ud83d\udcca Median","text":"<p>Custom metric for calculating the median of predictions, providing robust central tendency measures for anomaly detection.</p>"},{"location":"api/metrics/#kmr.metrics.median.Median","title":"kmr.metrics.median.Median","text":"<pre><code>Median(name: str = 'median', **kwargs: Any)\n</code></pre> <p>A custom Keras metric that calculates the median of the predicted values.</p> <p>This class is a custom implementation of a Keras metric, which calculates the median of the predicted values during model training. The median is a robust measure of central tendency that is less sensitive to outliers compared to the mean, making it particularly useful for anomaly detection tasks.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>Variable</code> <p>A trainable weight that stores the calculated median.</p> Example <pre><code>import keras\nfrom kmr.metrics import Median\n\n# Create metric\nmedian_metric = Median(name=\"prediction_median\")\n\n# Update with predictions\npredictions = keras.ops.random.normal((100, 10))\nmedian_metric.update_state(predictions)\n\n# Get result\nmedian_value = median_metric.result()\nprint(f\"Median: {median_value}\")\n</code></pre> <p>Initializes the Median metric with a given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric. Defaults to 'median'.</p> <code>'median'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api/metrics/#kmr.metrics.median.Median-functions","title":"Functions","text":""},{"location":"api/metrics/#kmr.metrics.median.Median.update_state","title":"update_state","text":"<pre><code>update_state(y_pred: keras.KerasTensor) -&gt; None\n</code></pre> <p>Updates the state of the metric with the median of the predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>KerasTensor</code> <p>The predicted values.</p> required"},{"location":"api/metrics/#kmr.metrics.median.Median.result","title":"result","text":"<pre><code>result() -&gt; keras.KerasTensor\n</code></pre> <p>Returns the current state of the metric, i.e., the current median.</p> <p>Returns:</p> Name Type Description <code>KerasTensor</code> <code>KerasTensor</code> <p>The current median.</p>"},{"location":"api/metrics/#kmr.metrics.median.Median.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: dict[str, Any]) -&gt; Median\n</code></pre> <p>Creates a new instance of the metric from its config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration of the metric.</p> required <p>Returns:</p> Name Type Description <code>Median</code> <code>Median</code> <p>A new instance of the metric.</p>"},{"location":"api/metrics/#usage-examples","title":"\ud83d\udd27 Usage Examples","text":""},{"location":"api/metrics/#basic-usage","title":"Basic Usage","text":"<pre><code>from kmr.metrics import StandardDeviation, Median\nimport keras\n\n# Create metrics\nstd_metric = StandardDeviation()\nmedian_metric = Median()\n\n# Update with predictions\npredictions = keras.ops.random.normal((100, 1))\nstd_metric.update_state(predictions)\nmedian_metric.update_state(predictions)\n\n# Get results\nprint(f\"Standard Deviation: {std_metric.result()}\")\nprint(f\"Median: {median_metric.result()}\")\n</code></pre>"},{"location":"api/metrics/#integration-with-autoencoder","title":"Integration with Autoencoder","text":"<pre><code>from kmr.models import Autoencoder\nfrom kmr.metrics import StandardDeviation, Median\n\n# Create autoencoder\nmodel = Autoencoder(input_dim=100, encoding_dim=32)\n\n# Metrics are automatically used in threshold setup\nmodel.fit(data, epochs=10, auto_setup_threshold=True)\n</code></pre>"},{"location":"api/metrics/#best-practices","title":"\ud83c\udfaf Best Practices","text":"<ol> <li>Reset State: Always reset metric state between epochs or evaluation runs</li> <li>Batch Processing: Update metrics with batches for better performance</li> <li>Memory Management: Use <code>reset_state()</code> to clear accumulated values</li> <li>Integration: These metrics work seamlessly with Keras training loops</li> </ol>"},{"location":"api/metrics/#implementation-notes","title":"\ud83d\udd27 Implementation Notes","text":"<ul> <li>All metrics follow Keras 3 conventions</li> <li>Full serialization support with <code>get_config()</code> and <code>from_config()</code></li> <li>Compatible with TensorFlow and other Keras backends</li> <li>Optimized for both eager and graph execution modes</li> </ul>"},{"location":"api/models/","title":"\ud83c\udfd7\ufe0f Models API Reference","text":"<p>Welcome to the KMR Models documentation! All models are designed to work exclusively with Keras 3 and provide high-level abstractions for common tabular data processing tasks.</p> <p>What You'll Find Here</p> <p>Each model includes detailed documentation with: - \u2728 Complete parameter descriptions with types and defaults - \ud83c\udfaf Usage examples showing real-world applications - \u26a1 Best practices and performance considerations - \ud83c\udfa8 When to use guidance for each model - \ud83d\udd27 Implementation notes for developers</p> <p>Ready-to-Use Models</p> <p>These models provide complete architectures that you can use out-of-the-box or customize for your specific needs.</p> <p>Base Classes</p> <p>All models inherit from <code>BaseModel</code> ensuring consistent behavior and Keras 3 compatibility.</p>"},{"location":"api/models/#core-models","title":"\ud83c\udfd7\ufe0f Core Models","text":""},{"location":"api/models/#basefeedforwardmodel","title":"\ud83d\ude80 BaseFeedForwardModel","text":"<p>Flexible feed-forward model architecture for tabular data with customizable layers.</p>"},{"location":"api/models/#kmr.models.feed_forward.BaseFeedForwardModel","title":"kmr.models.feed_forward.BaseFeedForwardModel","text":"<pre><code>BaseFeedForwardModel(\n    feature_names: list[str],\n    hidden_units: list[int],\n    output_units: int = 1,\n    dropout_rate: float = 0.0,\n    activation: str = \"relu\",\n    preprocessing_model: Model | None = None,\n    kernel_initializer: str | Any | None = \"glorot_uniform\",\n    bias_initializer: str | Any | None = \"zeros\",\n    kernel_regularizer: str | Any | None = None,\n    bias_regularizer: str | Any | None = None,\n    activity_regularizer: str | Any | None = None,\n    kernel_constraint: str | Any | None = None,\n    bias_constraint: str | Any | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Base feed forward neural network model.</p> <p>This model implements a basic feed forward neural network with configurable hidden layers, activations, and regularization options.</p> Example <pre><code># Create a simple feed forward model\nmodel = BaseFeedForwardModel(\n    feature_names=['feature1', 'feature2'],\n    hidden_units=[64, 32],\n    output_units=1\n)\n\n# Compile and train the model\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(train_dataset, epochs=10)\n</code></pre> <p>Initialize Feed Forward Neural Network.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list[str]</code> <p>list of feature names.</p> required <code>hidden_units</code> <code>list[int]</code> <p>list of hidden layer units.</p> required <code>output_units</code> <code>int</code> <p>Number of output units.</p> <code>1</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.0</code> <code>activation</code> <code>str</code> <p>Activation function.</p> <code>'relu'</code> <code>preprocessing_model</code> <code>Model | None</code> <p>Optional preprocessing model.</p> <code>None</code> <code>kernel_initializer</code> <code>str | Any | None</code> <p>Weight initializer.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <code>str | Any | None</code> <p>Bias initializer.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <code>str | Any | None</code> <p>Weight regularizer.</p> <code>None</code> <code>bias_regularizer</code> <code>str | Any | None</code> <p>Bias regularizer.</p> <code>None</code> <code>activity_regularizer</code> <code>str | Any | None</code> <p>Activity regularizer.</p> <code>None</code> <code>kernel_constraint</code> <code>str | Any | None</code> <p>Weight constraint.</p> <code>None</code> <code>bias_constraint</code> <code>str | Any | None</code> <p>Bias constraint.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code>"},{"location":"api/models/#kmr.models.feed_forward.BaseFeedForwardModel-functions","title":"Functions","text":""},{"location":"api/models/#kmr.models.feed_forward.BaseFeedForwardModel.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: dict[str, Any]) -&gt; BaseFeedForwardModel\n</code></pre> <p>Create model from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Dict containing model configuration.</p> required <p>Returns:</p> Type Description <code>BaseFeedForwardModel</code> <p>Instantiated model.</p>"},{"location":"api/models/#advanced-models","title":"\ud83c\udfaf Advanced Models","text":""},{"location":"api/models/#sfneblock","title":"\ud83e\udde9 SFNEBlock","text":"<p>Sparse Feature Network Ensemble block for advanced feature processing and ensemble learning.</p>"},{"location":"api/models/#kmr.models.SFNEBlock","title":"kmr.models.SFNEBlock","text":"<p>This module implements a SFNEBlock (Slow-Fast Neural Engine Block) model that combines slow and fast processing paths for feature extraction. It's a building block for the Terminator model.</p>"},{"location":"api/models/#kmr.models.SFNEBlock-classes","title":"Classes","text":""},{"location":"api/models/#kmr.models.SFNEBlock.SFNEBlock","title":"SFNEBlock","text":"<pre><code>SFNEBlock(\n    input_dim: int,\n    output_dim: int = None,\n    hidden_dim: int = 64,\n    num_layers: int = 2,\n    slow_network_layers: int = 3,\n    slow_network_units: int = 128,\n    preprocessing_model: Model | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Slow-Fast Neural Engine Block for feature processing.</p> <p>This model combines a slow network path and a fast processing path to extract features. It uses a SlowNetwork to generate hyper-kernels, which are then used by a HyperZZWOperator to compute context-dependent weights. These weights are further processed by global and local convolutions before being combined.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of the output features. Default is same as input_dim.</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in the network. Default is 64.</p> <code>64</code> <code>num_layers</code> <code>int</code> <p>Number of layers in the network. Default is 2.</p> <code>2</code> <code>slow_network_layers</code> <code>int</code> <p>Number of layers in the slow network. Default is 3.</p> <code>3</code> <code>slow_network_units</code> <code>int</code> <p>Number of units per layer in the slow network. Default is 128.</p> <code>128</code> <code>preprocessing_model</code> <code>Model | None</code> <p>Optional preprocessing model to apply before the main processing.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the model.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, input_dim)</code> or a dictionary with feature inputs</p> Output shape <p>2D tensor with shape: <code>(batch_size, output_dim)</code></p> Example <pre><code>import keras\nfrom kmr.models import SFNEBlock\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\n\n# Create the model\nsfne = SFNEBlock(input_dim=16, output_dim=8)\ny = sfne(x)\nprint(\"Output shape:\", y.shape)  # (32, 8)\n</code></pre> <p>Initialize the SFNEBlock model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension.</p> <code>64</code> <code>num_layers</code> <code>int</code> <p>Number of layers.</p> <code>2</code> <code>slow_network_layers</code> <code>int</code> <p>Number of slow network layers.</p> <code>3</code> <code>slow_network_units</code> <code>int</code> <p>Number of units in slow network.</p> <code>128</code> <code>preprocessing_model</code> <code>Model | None</code> <p>Preprocessing model.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"api/models/#kmr.models.SFNEBlock.SFNEBlock-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; SFNEBlock\n</code></pre> <p>Creates a model from its configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Dictionary containing the model configuration.</p> required <p>Returns:</p> Type Description <code>SFNEBlock</code> <p>A new instance of the model.</p>"},{"location":"api/models/#terminatormodel","title":"\ud83c\udfad TerminatorModel","text":"<p>Comprehensive tabular model that combines multiple SFNE blocks for complex data tasks.</p>"},{"location":"api/models/#kmr.models.TerminatorModel","title":"kmr.models.TerminatorModel","text":"<p>This module implements a TerminatorModel that combines multiple SFNE blocks for advanced feature processing. It's designed for complex tabular data modeling tasks.</p>"},{"location":"api/models/#kmr.models.TerminatorModel-classes","title":"Classes","text":""},{"location":"api/models/#kmr.models.TerminatorModel.TerminatorModel","title":"TerminatorModel","text":"<pre><code>TerminatorModel(\n    input_dim: int,\n    context_dim: int,\n    output_dim: int,\n    hidden_dim: int = 64,\n    num_layers: int = 2,\n    num_blocks: int = 3,\n    slow_network_layers: int = 3,\n    slow_network_units: int = 128,\n    preprocessing_model: Model | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Terminator model for advanced feature processing.</p> <p>This model stacks multiple SFNE blocks to process features in a hierarchical manner. It's designed for complex tabular data modeling tasks where feature interactions are important.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>context_dim</code> <code>int</code> <p>Dimension of the context features.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of the output.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in the network. Default is 64.</p> <code>64</code> <code>num_layers</code> <code>int</code> <p>Number of layers in the network. Default is 2.</p> <code>2</code> <code>num_blocks</code> <code>int</code> <p>Number of SFNE blocks to stack. Default is 3.</p> <code>3</code> <code>slow_network_layers</code> <code>int</code> <p>Number of layers in each slow network. Default is 3.</p> <code>3</code> <code>slow_network_units</code> <code>int</code> <p>Number of units per layer in each slow network. Default is 128.</p> <code>128</code> <code>preprocessing_model</code> <code>Model | None</code> <p>Optional preprocessing model to apply before the main processing.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the model.</p> <code>None</code> Input shape <p>List of 2D tensors with shapes: <code>[(batch_size, input_dim), (batch_size, context_dim)]</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, output_dim)</code></p> Example <pre><code>import keras\nfrom kmr.models import TerminatorModel\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\ncontext = keras.random.normal((32, 8))  # 32 samples, 8 context features\n\n# Create the model\nterminator = TerminatorModel(input_dim=16, context_dim=8, output_dim=1)\ny = terminator([x, context])\nprint(\"Output shape:\", y.shape)  # (32, 1)\n</code></pre> <p>Initialize the TerminatorModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>context_dim</code> <code>int</code> <p>Context dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension.</p> <code>64</code> <code>num_layers</code> <code>int</code> <p>Number of layers.</p> <code>2</code> <code>num_blocks</code> <code>int</code> <p>Number of blocks.</p> <code>3</code> <code>slow_network_layers</code> <code>int</code> <p>Number of slow network layers.</p> <code>3</code> <code>slow_network_units</code> <code>int</code> <p>Number of units in slow network.</p> <code>128</code> <code>preprocessing_model</code> <code>Model | None</code> <p>Preprocessing model.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"api/models/#kmr.models.TerminatorModel.TerminatorModel-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; TerminatorModel\n</code></pre> <p>Creates a model from its configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Dictionary containing the model configuration.</p> required <p>Returns:</p> Type Description <code>TerminatorModel</code> <p>A new instance of the model.</p>"},{"location":"api/models/#autoencoder","title":"\ud83d\udd0d Autoencoder","text":"<p>Advanced autoencoder model for anomaly detection with optional preprocessing integration and automatic threshold configuration.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder","title":"kmr.models.autoencoder.Autoencoder","text":"<pre><code>Autoencoder(\n    input_dim: int,\n    encoding_dim: int = 64,\n    intermediate_dim: int = 32,\n    threshold: float = 2.0,\n    preprocessing_model: keras.Model | None = None,\n    inputs: dict[str, tuple[int, ...]] | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>An autoencoder model for anomaly detection with optional preprocessing integration.</p> <p>This class implements an autoencoder neural network model used for anomaly detection. It can optionally integrate with preprocessing models for production use, making it a single, unified model for both training and inference.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>The dimension of the input data.</p> <code>encoding_dim</code> <code>int</code> <p>The dimension of the encoded representation.</p> <code>intermediate_dim</code> <code>int</code> <p>The dimension of the intermediate layer.</p> <code>preprocessing_model</code> <code>Model | None</code> <p>Optional preprocessing model.</p> <code>_threshold</code> <code>Variable</code> <p>The threshold for anomaly detection.</p> <code>_median</code> <code>Variable</code> <p>The median of the anomaly scores.</p> <code>_std</code> <code>Variable</code> <p>The standard deviation of the anomaly scores.</p> <p>Initializes the Autoencoder model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the input data.</p> required <code>encoding_dim</code> <code>int</code> <p>The dimension of the encoded representation. Defaults to 64.</p> <code>64</code> <code>intermediate_dim</code> <code>int</code> <p>The dimension of the intermediate layer. Defaults to 32.</p> <code>32</code> <code>threshold</code> <code>float</code> <p>The initial threshold for anomaly detection. Defaults to 2.0.</p> <code>2.0</code> <code>preprocessing_model</code> <code>Model</code> <p>Optional preprocessing model for production use. Defaults to None.</p> <code>None</code> <code>inputs</code> <code>dict[str, tuple]</code> <p>Input shapes for preprocessing model. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>The name of the model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder-attributes","title":"Attributes","text":""},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.threshold","title":"threshold  <code>property</code>","text":"<pre><code>threshold: float\n</code></pre> <p>Gets the current threshold value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The current threshold value.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.median","title":"median  <code>property</code>","text":"<pre><code>median: float\n</code></pre> <p>Gets the current median value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The current median value.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.std","title":"std  <code>property</code>","text":"<pre><code>std: float\n</code></pre> <p>Gets the current standard deviation value.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The current standard deviation value.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder-functions","title":"Functions","text":""},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.setup_threshold","title":"setup_threshold","text":"<pre><code>setup_threshold(data: keras.KerasTensor | Any) -&gt; None\n</code></pre> <p>Sets up the threshold for anomaly detection based on the given data.</p> <p>This method automatically calculates the median and standard deviation of reconstruction errors from the provided data and sets up the threshold for anomaly detection.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>KerasTensor | Any</code> <p>The data to use for threshold calculation. Can be a tensor or a dataset.</p> required"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.auto_configure_threshold","title":"auto_configure_threshold","text":"<pre><code>auto_configure_threshold(\n    data: keras.KerasTensor | Any,\n    percentile: float = 0.95,\n    method: str = \"iqr\",\n) -&gt; None\n</code></pre> <p>Automatically configure threshold using statistical methods.</p> <p>This method provides different approaches to automatically set the anomaly detection threshold based on statistical properties of the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>KerasTensor | Any</code> <p>The data to use for threshold calculation.</p> required <code>percentile</code> <code>float</code> <p>Percentile to use for threshold calculation. Defaults to 0.95.</p> <code>0.95</code> <code>method</code> <code>str</code> <p>Method to use for threshold calculation. Options: 'iqr' (Interquartile Range), 'percentile', 'zscore'. Defaults to 'iqr'.</p> <code>'iqr'</code>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.fit","title":"fit","text":"<pre><code>fit(\n    x: Any = None,\n    y: Any = None,\n    epochs: int = 1,\n    callbacks: list | None = None,\n    auto_setup_threshold: bool = True,\n    threshold_method: str = \"iqr\",\n    **kwargs: Any\n) -&gt; keras.callbacks.History\n</code></pre> <p>Fits the model to the given data with optional automatic threshold setup.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>KerasTensor | Any</code> <p>The training data (features).</p> <code>None</code> <code>y</code> <code>Any</code> <p>The training targets (labels).</p> <code>None</code> <code>epochs</code> <code>int</code> <p>The number of epochs to train for.</p> <code>1</code> <code>auto_setup_threshold</code> <code>bool</code> <p>Whether to automatically setup threshold after training. Defaults to True.</p> <code>True</code> <code>threshold_method</code> <code>str</code> <p>Method for threshold setup. Defaults to \"iqr\".</p> <code>'iqr'</code> <code>callbacks</code> <code>list</code> <p>A list of callbacks to use during training. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the fit method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>History</code> <p>keras.callbacks.History: A History object containing training history.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.create_functional_model","title":"create_functional_model","text":"<pre><code>create_functional_model() -&gt; keras.Model | None\n</code></pre> <p>Create a functional model that combines preprocessing and autoencoder.</p> <p>This method creates a functional Keras model that integrates the preprocessing model (if provided) with the autoencoder for end-to-end inference.</p> <p>Returns:</p> Type Description <code>Model | None</code> <p>keras.Model: Functional model combining preprocessing and autoencoder, or None if no preprocessing.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.predict_anomaly_scores","title":"predict_anomaly_scores","text":"<pre><code>predict_anomaly_scores(\n    data: keras.KerasTensor,\n) -&gt; keras.KerasTensor\n</code></pre> <p>Predicts anomaly scores for the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>KerasTensor</code> <p>The input data to predict on.</p> required <p>Returns:</p> Name Type Description <code>KerasTensor</code> <code>KerasTensor</code> <p>An array of anomaly scores.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.predict","title":"predict","text":"<pre><code>predict(\n    data: keras.KerasTensor\n    | dict[str, keras.KerasTensor]\n    | Any,\n    **kwargs\n) -&gt; keras.KerasTensor | dict[str, keras.KerasTensor]\n</code></pre> <p>Predicts reconstruction or anomaly detection results.</p> <p>This method provides a unified interface for both reconstruction prediction and anomaly detection, depending on whether a preprocessing model is used.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>KerasTensor | dict | Any</code> <p>The input data to predict on.</p> required <code>**kwargs</code> <p>Additional keyword arguments (ignored for compatibility).</p> <code>{}</code> <p>Returns:</p> Type Description <code>KerasTensor | dict[str, KerasTensor]</code> <p>KerasTensor | dict: Reconstruction results or anomaly detection results.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.is_anomaly","title":"is_anomaly","text":"<pre><code>is_anomaly(\n    data: keras.KerasTensor\n    | dict[str, keras.KerasTensor]\n    | Any,\n    percentile_to_use: str = \"median\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Determines if the given data contains anomalies.</p> <p>This method can handle both individual samples and datasets, providing comprehensive anomaly detection results.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>KerasTensor | dict | Any</code> <p>The data to check for anomalies.</p> required <code>percentile_to_use</code> <code>str</code> <p>The percentile to use for anomaly detection. Defaults to \"median\".</p> <code>'median'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing anomaly scores, flags, and threshold information.</p>"},{"location":"api/models/#kmr.models.autoencoder.Autoencoder.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: dict[str, Any]) -&gt; Autoencoder\n</code></pre> <p>Creates a new instance of the model from its config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary containing the configuration of the model.</p> required <p>Returns:</p> Name Type Description <code>Autoencoder</code> <code>Autoencoder</code> <p>A new instance of the model.</p>"},{"location":"api/models/#base-classes","title":"\ud83d\udd27 Base Classes","text":""},{"location":"api/models/#basemodel","title":"\ud83c\udfdb\ufe0f BaseModel","text":"<p>Base class for all KMR models, providing common functionality and Keras 3 compatibility.</p>"},{"location":"api/models/#kmr.models._base.BaseModel","title":"kmr.models._base.BaseModel","text":"<pre><code>BaseModel(*args, **kwargs)\n</code></pre> <p>Base model class with comprehensive input handling and common features.</p> <p>This class extends the standard Keras Model to provide: - Universal input handling (supports any input format) - Preprocessing model integration with automatic fitting - Input validation and standardization - Common utility methods for all models - Automatic functional model creation</p> <p>Initialize the base model with preprocessing support.</p>"},{"location":"api/models/#kmr.models._base.BaseModel-attributes","title":"Attributes","text":""},{"location":"api/models/#kmr.models._base.BaseModel.preprocessing_model","title":"preprocessing_model  <code>property</code>","text":"<pre><code>preprocessing_model: Optional[Model]\n</code></pre> <p>Get the preprocessing model.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: Optional[dict]\n</code></pre> <p>Get the input shapes specification.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.preprocessing_fitted","title":"preprocessing_fitted  <code>property</code>","text":"<pre><code>preprocessing_fitted: bool\n</code></pre> <p>Check if the preprocessing model has been fitted.</p>"},{"location":"api/models/#kmr.models._base.BaseModel-functions","title":"Functions","text":""},{"location":"api/models/#kmr.models._base.BaseModel.filer_inputs","title":"filer_inputs","text":"<pre><code>filer_inputs(inputs: dict) -&gt; dict\n</code></pre> <p>Filter inputs based on the specified input shapes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>Dictionary of inputs to filter.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Filtered inputs.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.inspect_signatures","title":"inspect_signatures","text":"<pre><code>inspect_signatures(model: Model) -&gt; dict\n</code></pre> <p>Inspect the model signatures.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>Model to inspect signatures for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Signature information.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.fit","title":"fit","text":"<pre><code>fit(\n    x: Any = None,\n    y: Any = None,\n    epochs: int = 1,\n    callbacks: list | None = None,\n    **kwargs: Any\n) -&gt; keras.callbacks.History\n</code></pre> <p>Fits the model to the given data with preprocessing model integration.</p> <p>This method automatically handles preprocessing model fitting if needed, then calls the parent class fit method for training.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The training data (features).</p> <code>None</code> <code>y</code> <code>Any</code> <p>The training targets (labels).</p> <code>None</code> <code>epochs</code> <code>int</code> <p>The number of epochs to train for.</p> <code>1</code> <code>callbacks</code> <code>list</code> <p>A list of callbacks to use during training. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the fit method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>History</code> <p>keras.callbacks.History: A History object containing training history.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.get_input_info","title":"get_input_info","text":"<pre><code>get_input_info() -&gt; dict[str, Any]\n</code></pre> <p>Get comprehensive input information for the model.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing input information</p>"},{"location":"api/models/#kmr.models._base.BaseModel.validate_inputs","title":"validate_inputs","text":"<pre><code>validate_inputs(\n    inputs: Any, expected_keys: list[str] = None\n) -&gt; bool\n</code></pre> <p>Validate inputs against expected format.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>Input data to validate</p> required <code>expected_keys</code> <code>list[str]</code> <p>Expected feature names</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if inputs are valid, False otherwise</p>"},{"location":"api/models/#kmr.models._base.BaseModel.get_model_summary","title":"get_model_summary","text":"<pre><code>get_model_summary() -&gt; str\n</code></pre> <p>Get a comprehensive model summary.</p> <p>Returns:</p> Type Description <code>str</code> <p>String containing model summary information</p>"},{"location":"api/models/#kmr.models._base.BaseModel.create_functional_model","title":"create_functional_model","text":"<pre><code>create_functional_model() -&gt; Optional[keras.Model]\n</code></pre> <p>Create a functional model that combines preprocessing and main model.</p> <p>This is a public method that wraps the internal _create_functional_model.</p> <p>Returns:</p> Type Description <code>Optional[Model]</code> <p>Functional model or None if no preprocessing model</p>"},{"location":"api/models/#kmr.models._base.BaseModel.reset_preprocessing_fitted","title":"reset_preprocessing_fitted","text":"<pre><code>reset_preprocessing_fitted() -&gt; None\n</code></pre> <p>Reset the preprocessing fitted flag.</p> <p>Useful when you want to refit the preprocessing model.</p>"},{"location":"api/models/#kmr.models._base.BaseModel.set_preprocessing_model","title":"set_preprocessing_model","text":"<pre><code>set_preprocessing_model(preprocessing_model: Any) -&gt; None\n</code></pre> <p>Set a new preprocessing model.</p> <p>Parameters:</p> Name Type Description Default <code>preprocessing_model</code> <code>Any</code> <p>New preprocessing model to use</p> required"},{"location":"api/utils/","title":"\ud83d\udd27 Utils API Reference","text":"<p>Welcome to the KMR Utilities documentation! This page provides documentation for KMR utility functions and tools, including the powerful Data Analyzer that can recommend appropriate layers for your tabular data.</p> <p>What You'll Find Here</p> <p>Each utility includes detailed documentation with: - \u2728 Complete parameter descriptions with types and defaults - \ud83c\udfaf Usage examples showing real-world applications - \u26a1 Best practices and performance considerations - \ud83c\udfa8 When to use guidance for each utility - \ud83d\udd27 Implementation notes for developers</p> <p>Smart Data Analysis</p> <p>The Data Analyzer can automatically analyze your CSV files and recommend the best KMR layers for your specific dataset.</p> <p>CLI Integration</p> <p>Use the command-line interface for quick data analysis and layer recommendations.</p>"},{"location":"api/utils/#data-analyzer","title":"\ud83d\udd0d Data Analyzer","text":""},{"location":"api/utils/#dataanalyzer","title":"\ud83e\udde0 DataAnalyzer","text":"<p>Intelligent data analyzer that examines CSV files and recommends appropriate KMR layers based on data characteristics.</p>"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer","title":"kmr.utils.data_analyzer.DataAnalyzer","text":"<pre><code>DataAnalyzer()\n</code></pre> <p>Analyzes tabular data and recommends appropriate KMR layers.</p> <p>This class provides methods to analyze CSV files, extract statistics, and recommend layers from the Keras Model Registry based on data characteristics.</p> <p>Attributes:</p> Name Type Description <code>registrations</code> <code>dict[str, list[tuple[str, str, str]]]</code> <p>Dictionary mapping data characteristics to recommended layer classes.</p> <p>Initialize the data analyzer with layer registrations.</p>"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer-functions","title":"Functions","text":""},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer.register_recommendation","title":"register_recommendation","text":"<pre><code>register_recommendation(\n    characteristic: str,\n    layer_name: str,\n    description: str,\n    use_case: str,\n) -&gt; None\n</code></pre> <p>Register a new layer recommendation for a specific data characteristic.</p> <p>Parameters:</p> Name Type Description Default <code>characteristic</code> <code>str</code> <p>The data characteristic identifier (e.g., 'continuous_features')</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer class</p> required <code>description</code> <code>str</code> <p>Brief description of the layer</p> required <code>use_case</code> <code>str</code> <p>When to use this layer</p> required"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer.analyze_csv","title":"analyze_csv","text":"<pre><code>analyze_csv(filepath: str) -&gt; dict[str, Any]\n</code></pre> <p>Analyze a single CSV file and return statistics.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the CSV file</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing dataset statistics and characteristics</p>"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer.analyze_directory","title":"analyze_directory","text":"<pre><code>analyze_directory(\n    directory_path: str, pattern: str = \"*.csv\"\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Analyze all CSV files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>Path to the directory containing CSV files</p> required <code>pattern</code> <code>str</code> <p>Glob pattern to match files (default: \"*.csv\")</p> <code>'*.csv'</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping filenames to their analysis results</p>"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer.recommend_layers","title":"recommend_layers","text":"<pre><code>recommend_layers(\n    stats: dict[str, Any]\n) -&gt; dict[str, list[tuple[str, str, str]]]\n</code></pre> <p>Recommend layers based on data statistics.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any]</code> <p>Dictionary of dataset statistics from analyze_csv</p> required <p>Returns:</p> Type Description <code>dict[str, list[tuple[str, str, str]]]</code> <p>Dictionary mapping characteristics to recommended layers</p>"},{"location":"api/utils/#kmr.utils.data_analyzer.DataAnalyzer.analyze_and_recommend","title":"analyze_and_recommend","text":"<pre><code>analyze_and_recommend(\n    source: str, pattern: str = \"*.csv\"\n) -&gt; dict[str, Any]\n</code></pre> <p>Analyze data and provide layer recommendations.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to file or directory to analyze</p> required <code>pattern</code> <code>str</code> <p>File pattern if source is a directory</p> <code>'*.csv'</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with analysis results and recommendations</p>"},{"location":"api/utils/#usage-examples","title":"\ud83d\udccb Usage Examples","text":"<p>Basic Data Analysis</p> <pre><code>from kmr.utils.data_analyzer import DataAnalyzer\n\n# Initialize the analyzer\nanalyzer = DataAnalyzer()\n\n# Analyze a CSV file\nresults = analyzer.analyze_file(\"data/tabular_data.csv\")\n\n# Get layer recommendations\nrecommendations = results.get_layer_recommendations()\nprint(\"Recommended layers:\", recommendations)\n\n# Get data insights\ninsights = results.get_data_insights()\nprint(\"Data insights:\", insights)\n</code></pre> <p>Advanced Analysis with Custom Parameters</p> <pre><code>from kmr.utils.data_analyzer import DataAnalyzer\n\n# Initialize with custom parameters\nanalyzer = DataAnalyzer(\n    sample_size=1000,  # Analyze first 1000 rows\n    correlation_threshold=0.7,  # High correlation threshold\n    categorical_threshold=0.1   # 10% unique values = categorical\n)\n\n# Analyze with detailed output\nresults = analyzer.analyze_file(\n    \"data/large_dataset.csv\",\n    output_format=\"detailed\",\n    include_statistics=True\n)\n\n# Get specific recommendations\nattention_layers = results.get_recommendations_by_type(\"attention\")\nfeature_engineering = results.get_recommendations_by_type(\"feature_engineering\")\n\nprint(\"Attention layers:\", attention_layers)\nprint(\"Feature engineering:\", feature_engineering)\n</code></pre> <p>Batch Analysis of Multiple Files</p> <pre><code>from kmr.utils.data_analyzer import DataAnalyzer\nimport os\n\nanalyzer = DataAnalyzer()\n\n# Analyze multiple CSV files\ndata_dir = \"data/\"\ncsv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n\nall_results = {}\nfor file in csv_files:\n    file_path = os.path.join(data_dir, file)\n    results = analyzer.analyze_file(file_path)\n    all_results[file] = results.get_layer_recommendations()\n\n# Compare recommendations across datasets\nfor file, recommendations in all_results.items():\n    print(f\"{file}: {recommendations}\")\n</code></pre>"},{"location":"api/utils/#dataanalyzercli","title":"\ud83d\udcbb DataAnalyzerCLI","text":"<p>Command-line interface for the data analyzer, allowing easy analysis of datasets from the terminal.</p>"},{"location":"api/utils/#kmr.utils.data_analyzer_cli","title":"kmr.utils.data_analyzer_cli","text":"<p>Command-line interface for the Keras Model Registry Data Analyzer.</p> <p>This script provides a convenient way to analyze CSV data and get layer recommendations from the command line.</p>"},{"location":"api/utils/#kmr.utils.data_analyzer_cli-classes","title":"Classes","text":""},{"location":"api/utils/#kmr.utils.data_analyzer_cli-functions","title":"Functions","text":""},{"location":"api/utils/#kmr.utils.data_analyzer_cli.parse_args","title":"parse_args","text":"<pre><code>parse_args() -&gt; argparse.Namespace\n</code></pre> <p>Parse command line arguments.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>Parsed arguments namespace</p>"},{"location":"api/utils/#kmr.utils.data_analyzer_cli.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(verbose: bool) -&gt; None\n</code></pre> <p>Configure logging based on verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging</p> required"},{"location":"api/utils/#kmr.utils.data_analyzer_cli.format_result","title":"format_result","text":"<pre><code>format_result(\n    result: dict[str, Any], recommendations_only: bool\n) -&gt; dict[str, Any]\n</code></pre> <p>Format the result based on user preferences.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict[str, Any]</code> <p>The analysis result</p> required <code>recommendations_only</code> <code>bool</code> <p>Whether to include only recommendations</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Formatted result dictionary</p>"},{"location":"api/utils/#kmr.utils.data_analyzer_cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Main entry point for the script.</p>"},{"location":"api/utils/#cli-usage-examples","title":"\ud83d\udda5\ufe0f CLI Usage Examples","text":"<p>Basic CLI Analysis</p> <pre><code># Analyze a single CSV file\nkmr-analyze data/tabular_data.csv\n\n# Analyze with verbose output\nkmr-analyze data/tabular_data.csv --verbose\n\n# Save results to file\nkmr-analyze data/tabular_data.csv --output results.json\n</code></pre> <p>Advanced CLI Options</p> <pre><code># Analyze with custom parameters\nkmr-analyze data/large_dataset.csv \\\n    --sample-size 5000 \\\n    --correlation-threshold 0.8 \\\n    --output detailed_analysis.json \\\n    --format json\n\n# Analyze multiple files\nkmr-analyze data/*.csv --batch --output batch_results.json\n\n# Get specific layer recommendations\nkmr-analyze data/tabular_data.csv --layers attention,embedding\n</code></pre> <p>Integration with Jupyter Notebooks</p> <pre><code># In a Jupyter notebook, you can use the CLI output\nimport json\nimport subprocess\n\n# Run CLI analysis\nresult = subprocess.run([\n    'kmr-analyze', 'data/tabular_data.csv', \n    '--output', 'analysis.json', '--format', 'json'\n], capture_output=True, text=True)\n\n# Load results\nwith open('analysis.json', 'r') as f:\n    analysis = json.load(f)\n\n# Use results in your notebook\nprint(\"Recommended layers:\", analysis['recommendations'])\nprint(\"Data statistics:\", analysis['statistics'])\n</code></pre>"},{"location":"api/utils/#complete-workflow-example","title":"\ud83d\udd04 Complete Workflow Example","text":"<p>End-to-End Data Analysis to Model Building</p> <pre><code>from kmr.utils.data_analyzer import DataAnalyzer\nfrom kmr.layers import TabularAttention, AdvancedNumericalEmbedding\nfrom kmr.models import BaseFeedForwardModel\nimport keras\n\n# Step 1: Analyze your data\nanalyzer = DataAnalyzer()\nanalysis = analyzer.analyze_file(\"data/my_dataset.csv\")\n\n# Step 2: Get recommendations\nrecommendations = analysis.get_layer_recommendations()\nprint(\"Recommended layers:\", recommendations)\n\n# Step 3: Build model based on recommendations\nif \"TabularAttention\" in recommendations:\n    # Use tabular attention for feature relationships\n    attention_layer = TabularAttention(\n        num_heads=8,\n        d_model=64,\n        dropout_rate=0.1\n    )\n\nif \"AdvancedNumericalEmbedding\" in recommendations:\n    # Use advanced embedding for numerical features\n    embedding_layer = AdvancedNumericalEmbedding(\n        embedding_dim=32,\n        mlp_hidden_units=64,\n        num_bins=20\n    )\n\n# Step 4: Create your model architecture\ninputs = keras.Input(shape=(100, 20))  # Based on your data shape\n\n# Apply recommended layers\nif 'embedding_layer' in locals():\n    x = embedding_layer(inputs)\nelse:\n    x = inputs\n\nif 'attention_layer' in locals():\n    x = attention_layer(x)\n\n# Add final layers\nx = keras.layers.Dense(64, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n# Create and compile model\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Model built with recommended KMR layers!\")\nmodel.summary()\n</code></pre> <p>Automated Model Architecture Selection</p> <pre><code>from kmr.utils.data_analyzer import DataAnalyzer\nfrom kmr.layers import *\nfrom kmr.models import BaseFeedForwardModel\nimport keras\n\ndef build_recommended_model(csv_file):\n    \"\"\"Automatically build a model based on data analysis.\"\"\"\n\n    # Analyze data\n    analyzer = DataAnalyzer()\n    analysis = analyzer.analyze_file(csv_file)\n    recommendations = analysis.get_layer_recommendations()\n\n    # Get data shape from analysis\n    data_shape = analysis.get_data_shape()\n    num_features = data_shape[1]\n\n    # Build model based on recommendations\n    inputs = keras.Input(shape=(num_features,))\n\n    # Apply recommended layers\n    x = inputs\n    for layer_name in recommendations:\n        if layer_name == \"TabularAttention\":\n            x = TabularAttention(num_heads=4, d_model=32)(x)\n        elif layer_name == \"AdvancedNumericalEmbedding\":\n            x = AdvancedNumericalEmbedding(embedding_dim=16)(x)\n        elif layer_name == \"VariableSelection\":\n            x = VariableSelection(nr_features=num_features, units=32)(x)\n        # Add more layer mappings as needed\n\n    # Add final layers\n    x = keras.layers.Dense(32, activation='relu')(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n    return model, analysis\n\n# Use the function\nmodel, analysis = build_recommended_model(\"data/my_dataset.csv\")\nprint(\"Automatically built model with layers:\", analysis.get_layer_recommendations())\n</code></pre>"},{"location":"api/utils/#decorators","title":"\ud83c\udfa8 Decorators","text":""},{"location":"api/utils/#decorators_1","title":"\u2728 Decorators","text":"<p>Utility decorators for common functionality in KMR components and enhanced development experience.</p>"},{"location":"api/utils/#kmr.utils.decorators","title":"kmr.utils.decorators","text":""},{"location":"api/utils/#kmr.utils.decorators-functions","title":"Functions","text":""},{"location":"api/utils/#kmr.utils.decorators.log_init","title":"log_init","text":"<pre><code>log_init(cls: type[T]) -&gt; type[T]\n</code></pre> <p>Class decorator to log initialization arguments.</p>"},{"location":"api/utils/#kmr.utils.decorators.log_method","title":"log_method","text":"<pre><code>log_method(func: Callable) -&gt; Callable\n</code></pre> <p>Method decorator to log method calls with their arguments.</p>"},{"location":"api/utils/#kmr.utils.decorators.log_property","title":"log_property","text":"<pre><code>log_property(func: Callable) -&gt; Callable\n</code></pre> <p>Property decorator to log property access.</p>"},{"location":"api/utils/#kmr.utils.decorators.add_serialization","title":"add_serialization","text":"<pre><code>add_serialization(cls: T) -&gt; T\n</code></pre> <p>Decorator to add serialization methods to a Keras model class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>T</code> <p>The class to decorate.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The decorated class.</p>"},{"location":"api/utils/#usage-examples_1","title":"\ud83d\udd27 Usage Examples","text":"<p>Layer Validation Decorator</p> <pre><code>from kmr.utils.decorators import validate_inputs\n\n@validate_inputs\ndef custom_layer_call(self, inputs, training=None):\n    \"\"\"Custom layer with automatic input validation.\"\"\"\n    # Your layer logic here\n    return processed_outputs\n</code></pre> <p>Performance Monitoring Decorator</p> <pre><code>from kmr.utils.decorators import monitor_performance\n\n@monitor_performance\ndef expensive_computation(self, data):\n    \"\"\"Function with automatic performance monitoring.\"\"\"\n    # Your computation here\n    return result\n</code></pre> <p>Serialization Helper Decorator</p> <pre><code>from kmr.utils.decorators import serializable\n\n@serializable\nclass CustomLayer:\n    \"\"\"Layer with automatic serialization support.\"\"\"\n    def __init__(self, param1, param2):\n        self.param1 = param1\n        self.param2 = param2\n</code></pre>"},{"location":"examples/","title":"\ud83d\udcda Examples","text":"<p>Real-world examples and use cases demonstrating KMR layers in action. These examples show how to build production-ready tabular models for various domains and applications.</p>"},{"location":"examples/#quick-navigation","title":"\ud83c\udfaf Quick Navigation","text":"<ul> <li>Rich Docstrings Showcase - Comprehensive examples with detailed documentation</li> <li>BaseFeedForwardModel Guide - Building feed-forward models with KMR</li> <li>KDP Integration Guide - Integrating with Keras Data Processor</li> <li>Data Analyzer Examples - Data analysis and preprocessing workflows</li> </ul>"},{"location":"examples/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"examples/#1-basic-classification","title":"1. Basic Classification","text":"<pre><code>import keras\nfrom kmr.layers import TabularAttention, VariableSelection\n\n# Simple classification model\ndef create_classifier(input_dim, num_classes):\n    inputs = keras.Input(shape=(input_dim,))\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n\n# Usage\nmodel = create_classifier(input_dim=20, num_classes=3)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"examples/#2-regression-with-feature-engineering","title":"2. Regression with Feature Engineering","text":"<pre><code>from kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    AdvancedNumericalEmbedding,\n    GatedFeatureFusion\n)\n\ndef create_regressor(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n    outputs = keras.layers.Dense(1)(x)\n    return keras.Model(inputs, outputs)\n\n# Usage\nmodel = create_regressor(input_dim=20)\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n</code></pre>"},{"location":"examples/#architecture-examples","title":"\ud83c\udfd7\ufe0f Architecture Examples","text":""},{"location":"examples/#1-attention-based-architecture","title":"1. Attention-Based Architecture","text":"<pre><code>from kmr.layers import (\n    MultiResolutionTabularAttention,\n    InterpretableMultiHeadAttention,\n    GatedFeatureFusion\n)\n\ndef create_attention_model(input_dim, num_classes):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Multi-resolution attention\n    x = MultiResolutionTabularAttention(\n        num_heads=8,\n        numerical_heads=4,\n        categorical_heads=4\n    )(inputs)\n\n    # Interpretable attention\n    x = InterpretableMultiHeadAttention(\n        num_heads=8,\n        key_dim=64\n    )(x)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#2-residual-network-architecture","title":"2. Residual Network Architecture","text":"<pre><code>from kmr.layers import GatedResidualNetwork, GatedLinearUnit\n\ndef create_residual_model(input_dim, num_classes):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Residual blocks\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(inputs)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n\n    # Gated linear unit\n    x = GatedLinearUnit(units=64)(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#3-ensemble-architecture","title":"3. Ensemble Architecture","text":"<pre><code>from kmr.layers import TabularMoELayer, BoostingEnsembleLayer\n\ndef create_ensemble_model(input_dim, num_classes):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Mixture of experts\n    x = TabularMoELayer(num_experts=4, expert_units=16)(inputs)\n\n    # Boosting ensemble\n    x = BoostingEnsembleLayer(\n        num_learners=3,\n        learner_units=64\n    )(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#feature-engineering-examples","title":"\ud83d\udd27 Feature Engineering Examples","text":""},{"location":"examples/#1-complete-feature-pipeline","title":"1. Complete Feature Pipeline","text":"<pre><code>from kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    AdvancedNumericalEmbedding,\n    DistributionAwareEncoder,\n    VariableSelection,\n    SparseAttentionWeighting\n)\n\ndef create_feature_pipeline(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Numerical embedding\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n\n    # Distribution-aware encoding\n    x = DistributionAwareEncoder(encoding_dim=64)(x)\n\n    # Variable selection\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Sparse attention weighting\n    x = SparseAttentionWeighting(temperature=1.0)(x)\n\n    return keras.Model(inputs, x)\n</code></pre>"},{"location":"examples/#2-temporal-feature-processing","title":"2. Temporal Feature Processing","text":"<pre><code>from kmr.layers import (\n    DateParsingLayer,\n    DateEncodingLayer,\n    SeasonLayer\n)\n\ndef create_temporal_pipeline():\n    # Date parsing\n    date_parser = DateParsingLayer()\n\n    # Date encoding\n    date_encoder = DateEncodingLayer(min_year=1900, max_year=2100)\n\n    # Season extraction\n    season_layer = SeasonLayer()\n\n    return date_parser, date_encoder, season_layer\n\n# Usage\ndate_parser, date_encoder, season_layer = create_temporal_pipeline()\n</code></pre>"},{"location":"examples/#domain-specific-examples","title":"\ud83c\udfaf Domain-Specific Examples","text":""},{"location":"examples/#1-financial-modeling","title":"1. Financial Modeling","text":"<pre><code>def create_financial_model(input_dim, num_classes):\n    \"\"\"Model for financial risk assessment.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing for financial data\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Feature selection for risk factors\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Attention for complex relationships\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n\n    # Business rules integration\n    x = BusinessRulesLayer(\n        rules=[\n            {'feature': 'credit_score', 'operator': '&gt;', 'value': 600, 'weight': 1.0},\n            {'feature': 'debt_ratio', 'operator': '&lt;', 'value': 0.4, 'weight': 0.8}\n        ],\n        feature_type='numerical'\n    )(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#2-healthcare-analytics","title":"2. Healthcare Analytics","text":"<pre><code>def create_healthcare_model(input_dim, num_classes):\n    \"\"\"Model for healthcare outcome prediction.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Advanced numerical embedding for medical features\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n\n    # Distribution-aware encoding for lab values\n    x = DistributionAwareEncoder(encoding_dim=64)(x)\n\n    # Attention for symptom relationships\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n\n    # Anomaly detection for outliers\n    x, anomalies = NumericalAnomalyDetection()(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, [outputs, anomalies])\n</code></pre>"},{"location":"examples/#3-e-commerce-recommendation","title":"3. E-commerce Recommendation","text":"<pre><code>def create_recommendation_model(input_dim, num_classes):\n    \"\"\"Model for e-commerce product recommendation.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Feature selection for user preferences\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Multi-resolution attention for different feature types\n    x = MultiResolutionTabularAttention(\n        num_heads=8,\n        numerical_heads=4,\n        categorical_heads=4\n    )(x)\n\n    # Feature fusion for recommendation\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#performance-examples","title":"\ud83d\ude80 Performance Examples","text":""},{"location":"examples/#1-memory-efficient-model","title":"1. Memory-Efficient Model","text":"<pre><code>def create_memory_efficient_model(input_dim, num_classes):\n    \"\"\"Memory-efficient model for large datasets.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use smaller dimensions\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = TabularAttention(num_heads=4, key_dim=32)(x)\n    x = GatedFeatureFusion(hidden_dim=64)(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#2-speed-optimized-model","title":"2. Speed-Optimized Model","text":"<pre><code>def create_speed_optimized_model(input_dim, num_classes):\n    \"\"\"Speed-optimized model for real-time inference.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Minimal layers for speed\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = TabularAttention(num_heads=4, key_dim=32)(x)\n\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/#analysis-and-interpretation","title":"\ud83d\udd0d Analysis and Interpretation","text":""},{"location":"examples/#1-model-interpretation","title":"1. Model Interpretation","text":"<pre><code>def interpret_model(model, X_test, layer_name='tabular_attention'):\n    \"\"\"Interpret model using attention weights.\"\"\"\n\n    # Get attention weights\n    attention_model = keras.Model(\n        inputs=model.input,\n        outputs=model.get_layer(layer_name).output\n    )\n\n    attention_weights = attention_model.predict(X_test)\n\n    # Analyze attention patterns\n    mean_attention = np.mean(attention_weights, axis=0)\n    print(\"Mean attention weights:\", mean_attention)\n\n    return attention_weights\n</code></pre>"},{"location":"examples/#2-feature-importance-analysis","title":"2. Feature Importance Analysis","text":"<pre><code>def analyze_feature_importance(model, X_test, feature_names):\n    \"\"\"Analyze feature importance using attention weights.\"\"\"\n\n    # Get attention weights\n    attention_weights = interpret_model(model, X_test)\n\n    # Calculate feature importance\n    feature_importance = np.mean(attention_weights, axis=(0, 1))\n\n    # Create importance dataframe\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': feature_importance\n    }).sort_values('importance', ascending=False)\n\n    return importance_df\n</code></pre>"},{"location":"examples/#evaluation-examples","title":"\ud83d\udcca Evaluation Examples","text":""},{"location":"examples/#1-comprehensive-evaluation","title":"1. Comprehensive Evaluation","text":"<pre><code>def evaluate_model_comprehensive(model, X_test, y_test):\n    \"\"\"Comprehensive model evaluation.\"\"\"\n\n    # Basic evaluation\n    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n\n    # Predictions\n    predictions = model.predict(X_test)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test, axis=1)\n\n    # Additional metrics\n    from sklearn.metrics import classification_report, confusion_matrix\n\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_classes, predicted_classes))\n\n    return test_accuracy, test_loss\n</code></pre>"},{"location":"examples/#2-cross-validation","title":"2. Cross-Validation","text":"<pre><code>from sklearn.model_selection import cross_val_score\n\ndef cross_validate_model(model, X, y, cv=5):\n    \"\"\"Cross-validation for model evaluation.\"\"\"\n\n    # Compile model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Cross-validation\n    scores = cross_val_score(\n        model, X, y, \n        cv=cv, \n        scoring='accuracy',\n        verbose=0\n    )\n\n    print(f\"Cross-validation scores: {scores}\")\n    print(f\"Mean accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n\n    return scores\n</code></pre>"},{"location":"examples/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Rich Docstrings Showcase: See comprehensive examples with detailed documentation</li> <li>BaseFeedForwardModel Guide: Learn about feed-forward model architectures</li> <li>KDP Integration Guide: Integrate with Keras Data Processor</li> <li>Data Analyzer Examples: Explore data analysis workflows</li> </ol> <p>Ready to dive deeper? Check out the Rich Docstrings Showcase for comprehensive examples!</p>"},{"location":"examples/data_analyzer_examples/","title":"\ud83d\udcca Data Analyzer Examples","text":"<p>Comprehensive examples demonstrating data analysis workflows with KMR layers. Learn how to analyze, visualize, and understand your tabular data before building models.</p>"},{"location":"examples/data_analyzer_examples/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Data Exploration</li> <li>Feature Analysis</li> <li>Model Interpretation</li> <li>Performance Analysis</li> </ol>"},{"location":"examples/data_analyzer_examples/#data-exploration","title":"\ud83d\udd0d Data Exploration","text":""},{"location":"examples/data_analyzer_examples/#basic-data-analysis","title":"Basic Data Analysis","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\ndef analyze_dataset(X, y, feature_names=None):\n    \"\"\"Comprehensive dataset analysis.\"\"\"\n\n    # Basic statistics\n    print(\"Dataset Shape:\", X.shape)\n    print(\"Target Distribution:\", np.bincount(y))\n\n    # Missing values\n    missing_values = pd.DataFrame(X).isnull().sum()\n    print(\"Missing Values:\")\n    print(missing_values)\n\n    # Data types\n    print(\"Data Types:\")\n    print(pd.DataFrame(X).dtypes)\n\n    # Basic statistics\n    print(\"Basic Statistics:\")\n    print(pd.DataFrame(X).describe())\n\n    return True\n\n# Usage\nanalyze_dataset(X_train, y_train, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#feature-distribution-analysis","title":"Feature Distribution Analysis","text":"<pre><code>def analyze_feature_distributions(X, feature_names=None):\n    \"\"\"Analyze feature distributions.\"\"\"\n\n    if feature_names is None:\n        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n\n    # Create DataFrame\n    df = pd.DataFrame(X, columns=feature_names)\n\n    # Plot distributions\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    axes = axes.ravel()\n\n    for i, feature in enumerate(feature_names[:4]):\n        df[feature].hist(ax=axes[i], bins=30)\n        axes[i].set_title(f'Distribution of {feature}')\n        axes[i].set_xlabel(feature)\n        axes[i].set_ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Statistical analysis\n    for feature in feature_names:\n        print(f\"\\n{feature}:\")\n        print(f\"  Mean: {df[feature].mean():.4f}\")\n        print(f\"  Std: {df[feature].std():.4f}\")\n        print(f\"  Skewness: {df[feature].skew():.4f}\")\n        print(f\"  Kurtosis: {df[feature].kurtosis():.4f}\")\n\n# Usage\nanalyze_feature_distributions(X_train, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#feature-analysis","title":"\ud83d\udd27 Feature Analysis","text":""},{"location":"examples/data_analyzer_examples/#feature-importance-analysis","title":"Feature Importance Analysis","text":"<pre><code>from kmr.layers import VariableSelection, TabularAttention\n\ndef analyze_feature_importance(model, X_test, feature_names=None):\n    \"\"\"Analyze feature importance using model weights.\"\"\"\n\n    if feature_names is None:\n        feature_names = [f'feature_{i}' for i in range(X_test.shape[1])]\n\n    # Get variable selection layer\n    try:\n        selection_layer = model.get_layer('variable_selection')\n        selection_weights = selection_layer.get_weights()\n\n        # Calculate feature importance\n        feature_importance = np.mean(selection_weights[0], axis=1)\n\n        # Create importance DataFrame\n        importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': feature_importance\n        }).sort_values('importance', ascending=False)\n\n        print(\"Feature Importance:\")\n        print(importance_df)\n\n        # Plot importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(data=importance_df, x='importance', y='feature')\n        plt.title('Feature Importance')\n        plt.xlabel('Importance Score')\n        plt.tight_layout()\n        plt.show()\n\n        return importance_df\n\n    except Exception as e:\n        print(f\"Could not analyze feature importance: {e}\")\n        return None\n\n# Usage\nimportance_df = analyze_feature_importance(model, X_test, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#attention-weight-analysis","title":"Attention Weight Analysis","text":"<pre><code>def analyze_attention_weights(model, X_test, feature_names=None):\n    \"\"\"Analyze attention weights for model interpretation.\"\"\"\n\n    if feature_names is None:\n        feature_names = [f'feature_{i}' for i in range(X_test.shape[1])]\n\n    # Get attention layer\n    try:\n        attention_layer = model.get_layer('tabular_attention')\n\n        # Create model that outputs attention weights\n        attention_model = keras.Model(\n            inputs=model.input,\n            outputs=attention_layer.output\n        )\n\n        # Get attention weights\n        attention_weights = attention_model.predict(X_test)\n\n        # Analyze attention patterns\n        mean_attention = np.mean(attention_weights, axis=0)\n        std_attention = np.std(attention_weights, axis=0)\n\n        # Create attention DataFrame\n        attention_df = pd.DataFrame({\n            'feature': feature_names,\n            'mean_attention': mean_attention,\n            'std_attention': std_attention\n        }).sort_values('mean_attention', ascending=False)\n\n        print(\"Attention Weights Analysis:\")\n        print(attention_df)\n\n        # Plot attention weights\n        plt.figure(figsize=(12, 6))\n        sns.barplot(data=attention_df, x='mean_attention', y='feature')\n        plt.title('Mean Attention Weights')\n        plt.xlabel('Attention Weight')\n        plt.tight_layout()\n        plt.show()\n\n        return attention_df\n\n    except Exception as e:\n        print(f\"Could not analyze attention weights: {e}\")\n        return None\n\n# Usage\nattention_df = analyze_attention_weights(model, X_test, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#model-interpretation","title":"\ud83e\udde0 Model Interpretation","text":""},{"location":"examples/data_analyzer_examples/#layer-output-analysis","title":"Layer Output Analysis","text":"<pre><code>def analyze_layer_outputs(model, X_test, layer_names):\n    \"\"\"Analyze outputs from different model layers.\"\"\"\n\n    layer_outputs = {}\n\n    for layer_name in layer_names:\n        try:\n            # Get layer\n            layer = model.get_layer(layer_name)\n\n            # Create model that outputs layer activations\n            layer_model = keras.Model(\n                inputs=model.input,\n                outputs=layer.output\n            )\n\n            # Get layer outputs\n            layer_output = layer_model.predict(X_test)\n            layer_outputs[layer_name] = layer_output\n\n            print(f\"{layer_name} output shape: {layer_output.shape}\")\n\n        except Exception as e:\n            print(f\"Could not analyze layer {layer_name}: {e}\")\n\n    return layer_outputs\n\n# Usage\nlayer_names = ['variable_selection', 'tabular_attention', 'gated_feature_fusion']\nlayer_outputs = analyze_layer_outputs(model, X_test, layer_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#model-decision-analysis","title":"Model Decision Analysis","text":"<pre><code>def analyze_model_decisions(model, X_test, y_test, feature_names=None):\n    \"\"\"Analyze model decision-making process.\"\"\"\n\n    if feature_names is None:\n        feature_names = [f'feature_{i}' for i in range(X_test.shape[1])]\n\n    # Get predictions\n    predictions = model.predict(X_test)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test, axis=1)\n\n    # Analyze prediction confidence\n    prediction_confidence = np.max(predictions, axis=1)\n\n    print(\"Prediction Confidence Analysis:\")\n    print(f\"Mean confidence: {np.mean(prediction_confidence):.4f}\")\n    print(f\"Std confidence: {np.std(prediction_confidence):.4f}\")\n    print(f\"Min confidence: {np.min(prediction_confidence):.4f}\")\n    print(f\"Max confidence: {np.max(prediction_confidence):.4f}\")\n\n    # Analyze misclassifications\n    misclassified = predicted_classes != true_classes\n    misclassified_indices = np.where(misclassified)[0]\n\n    print(f\"\\nMisclassified samples: {len(misclassified_indices)}\")\n    print(f\"Misclassification rate: {len(misclassified_indices) / len(y_test):.4f}\")\n\n    # Analyze confidence of misclassified samples\n    if len(misclassified_indices) &gt; 0:\n        misclassified_confidence = prediction_confidence[misclassified_indices]\n        print(f\"Mean confidence of misclassified: {np.mean(misclassified_confidence):.4f}\")\n\n    return {\n        'predictions': predictions,\n        'predicted_classes': predicted_classes,\n        'true_classes': true_classes,\n        'confidence': prediction_confidence,\n        'misclassified_indices': misclassified_indices\n    }\n\n# Usage\ndecision_analysis = analyze_model_decisions(model, X_test, y_test, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#performance-analysis","title":"\ud83d\udcc8 Performance Analysis","text":""},{"location":"examples/data_analyzer_examples/#training-performance-analysis","title":"Training Performance Analysis","text":"<pre><code>def analyze_training_performance(history):\n    \"\"\"Analyze training performance and convergence.\"\"\"\n\n    # Plot training curves\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Loss curve\n    axes[0].plot(history.history['loss'], label='Training Loss')\n    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n    axes[0].set_title('Model Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n\n    # Accuracy curve\n    axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n    axes[1].set_title('Model Accuracy')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    axes[1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Analyze convergence\n    final_train_loss = history.history['loss'][-1]\n    final_val_loss = history.history['val_loss'][-1]\n    final_train_acc = history.history['accuracy'][-1]\n    final_val_acc = history.history['val_accuracy'][-1]\n\n    print(\"Final Performance:\")\n    print(f\"Training Loss: {final_train_loss:.4f}\")\n    print(f\"Validation Loss: {final_val_loss:.4f}\")\n    print(f\"Training Accuracy: {final_train_acc:.4f}\")\n    print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n\n    # Check for overfitting\n    if final_val_loss &gt; final_train_loss * 1.1:\n        print(\"Warning: Possible overfitting detected!\")\n\n    return {\n        'final_train_loss': final_train_loss,\n        'final_val_loss': final_val_loss,\n        'final_train_acc': final_train_acc,\n        'final_val_acc': final_val_acc\n    }\n\n# Usage\nperformance_analysis = analyze_training_performance(history)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#model-comparison-analysis","title":"Model Comparison Analysis","text":"<pre><code>def compare_models(models, X_test, y_test, model_names=None):\n    \"\"\"Compare performance of multiple models.\"\"\"\n\n    if model_names is None:\n        model_names = [f'model_{i}' for i in range(len(models))]\n\n    results = []\n\n    for model, name in zip(models, model_names):\n        # Evaluate model\n        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n\n        # Get predictions\n        predictions = model.predict(X_test)\n        predicted_classes = np.argmax(predictions, axis=1)\n        true_classes = np.argmax(y_test, axis=1)\n\n        # Calculate additional metrics\n        from sklearn.metrics import precision_score, recall_score, f1_score\n\n        precision = precision_score(true_classes, predicted_classes, average='weighted')\n        recall = recall_score(true_classes, predicted_classes, average='weighted')\n        f1 = f1_score(true_classes, predicted_classes, average='weighted')\n\n        results.append({\n            'model': name,\n            'accuracy': test_accuracy,\n            'loss': test_loss,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1\n        })\n\n    # Create comparison DataFrame\n    comparison_df = pd.DataFrame(results)\n\n    print(\"Model Comparison:\")\n    print(comparison_df)\n\n    # Plot comparison\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Accuracy comparison\n    sns.barplot(data=comparison_df, x='model', y='accuracy', ax=axes[0, 0])\n    axes[0, 0].set_title('Accuracy Comparison')\n    axes[0, 0].tick_params(axis='x', rotation=45)\n\n    # Loss comparison\n    sns.barplot(data=comparison_df, x='model', y='loss', ax=axes[0, 1])\n    axes[0, 1].set_title('Loss Comparison')\n    axes[0, 1].tick_params(axis='x', rotation=45)\n\n    # Precision comparison\n    sns.barplot(data=comparison_df, x='model', y='precision', ax=axes[1, 0])\n    axes[1, 0].set_title('Precision Comparison')\n    axes[1, 0].tick_params(axis='x', rotation=45)\n\n    # F1 comparison\n    sns.barplot(data=comparison_df, x='model', y='f1', ax=axes[1, 1])\n    axes[1, 1].set_title('F1 Score Comparison')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    return comparison_df\n\n# Usage\nmodels = [model1, model2, model3]\nmodel_names = ['Attention Model', 'Residual Model', 'Ensemble Model']\ncomparison_df = compare_models(models, X_test, y_test, model_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#advanced-analysis","title":"\ud83d\udd0d Advanced Analysis","text":""},{"location":"examples/data_analyzer_examples/#feature-interaction-analysis","title":"Feature Interaction Analysis","text":"<pre><code>def analyze_feature_interactions(X, feature_names=None):\n    \"\"\"Analyze feature interactions and correlations.\"\"\"\n\n    if feature_names is None:\n        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n\n    # Create DataFrame\n    df = pd.DataFrame(X, columns=feature_names)\n\n    # Calculate correlation matrix\n    correlation_matrix = df.corr()\n\n    # Plot correlation heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n    plt.title('Feature Correlation Matrix')\n    plt.tight_layout()\n    plt.show()\n\n    # Find highly correlated features\n    high_corr_pairs = []\n    for i in range(len(correlation_matrix.columns)):\n        for j in range(i+1, len(correlation_matrix.columns)):\n            corr_value = correlation_matrix.iloc[i, j]\n            if abs(corr_value) &gt; 0.7:  # High correlation threshold\n                high_corr_pairs.append({\n                    'feature1': correlation_matrix.columns[i],\n                    'feature2': correlation_matrix.columns[j],\n                    'correlation': corr_value\n                })\n\n    if high_corr_pairs:\n        print(\"Highly Correlated Feature Pairs:\")\n        for pair in high_corr_pairs:\n            print(f\"{pair['feature1']} - {pair['feature2']}: {pair['correlation']:.4f}\")\n\n    return correlation_matrix, high_corr_pairs\n\n# Usage\ncorrelation_matrix, high_corr_pairs = analyze_feature_interactions(X_train, feature_names)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#model-robustness-analysis","title":"Model Robustness Analysis","text":"<pre><code>def analyze_model_robustness(model, X_test, y_test, noise_levels=[0.01, 0.05, 0.1]):\n    \"\"\"Analyze model robustness to noise.\"\"\"\n\n    results = []\n\n    for noise_level in noise_levels:\n        # Add noise to test data\n        X_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)\n\n        # Evaluate model on noisy data\n        test_loss, test_accuracy = model.evaluate(X_noisy, y_test, verbose=0)\n\n        results.append({\n            'noise_level': noise_level,\n            'accuracy': test_accuracy,\n            'loss': test_loss\n        })\n\n    # Create results DataFrame\n    robustness_df = pd.DataFrame(results)\n\n    print(\"Model Robustness Analysis:\")\n    print(robustness_df)\n\n    # Plot robustness\n    plt.figure(figsize=(10, 6))\n    plt.plot(robustness_df['noise_level'], robustness_df['accuracy'], 'o-', label='Accuracy')\n    plt.plot(robustness_df['noise_level'], robustness_df['loss'], 's-', label='Loss')\n    plt.xlabel('Noise Level')\n    plt.ylabel('Performance')\n    plt.title('Model Robustness to Noise')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return robustness_df\n\n# Usage\nrobustness_df = analyze_model_robustness(model, X_test, y_test)\n</code></pre>"},{"location":"examples/data_analyzer_examples/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Rich Docstrings Showcase: See comprehensive examples</li> <li>BaseFeedForwardModel Guide: Learn about feed-forward architectures</li> <li>KDP Integration Guide: Integrate with Keras Data Processor</li> <li>API Reference: Deep dive into layer parameters</li> </ol> <p>Ready for more examples? Check out the Rich Docstrings Showcase next!</p>"},{"location":"examples/feed_forward_guide/","title":"\ud83c\udfd7\ufe0f BaseFeedForwardModel Guide","text":"<p>Learn how to build feed-forward models using KMR layers. This guide covers the fundamentals of creating efficient feed-forward architectures for tabular data.</p>"},{"location":"examples/feed_forward_guide/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Basic Feed-Forward Architecture</li> <li>Advanced Feed-Forward Patterns</li> <li>Performance Optimization</li> <li>Real-World Examples</li> </ol>"},{"location":"examples/feed_forward_guide/#basic-feed-forward-architecture","title":"\ud83c\udfdb\ufe0f Basic Feed-Forward Architecture","text":""},{"location":"examples/feed_forward_guide/#simple-feed-forward-model","title":"Simple Feed-Forward Model","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import VariableSelection, GatedFeatureFusion\n\ndef create_basic_feedforward(input_dim, num_classes):\n    \"\"\"Create a basic feed-forward model with KMR layers.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Feature selection\n    x = VariableSelection(hidden_dim=64)(inputs)\n\n    # Dense layers\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmodel = create_basic_feedforward(input_dim=20, num_classes=3)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"examples/feed_forward_guide/#feed-forward-with-feature-engineering","title":"Feed-Forward with Feature Engineering","text":"<pre><code>from kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    AdvancedNumericalEmbedding,\n    GatedFeatureFusion\n)\n\ndef create_engineered_feedforward(input_dim, num_classes):\n    \"\"\"Create a feed-forward model with feature engineering.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Feature engineering\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Dense layers\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#advanced-feed-forward-patterns","title":"\ud83d\ude80 Advanced Feed-Forward Patterns","text":""},{"location":"examples/feed_forward_guide/#residual-feed-forward","title":"Residual Feed-Forward","text":"<pre><code>from kmr.layers import GatedResidualNetwork\n\ndef create_residual_feedforward(input_dim, num_classes):\n    \"\"\"Create a residual feed-forward model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Residual blocks\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(inputs)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#multi-branch-feed-forward","title":"Multi-Branch Feed-Forward","text":"<pre><code>def create_multibranch_feedforward(input_dim, num_classes):\n    \"\"\"Create a multi-branch feed-forward model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Branch 1: Feature selection\n    branch1 = VariableSelection(hidden_dim=64)(inputs)\n    branch1 = keras.layers.Dense(64, activation='relu')(branch1)\n\n    # Branch 2: Direct processing\n    branch2 = keras.layers.Dense(64, activation='relu')(inputs)\n    branch2 = keras.layers.Dense(64, activation='relu')(branch2)\n\n    # Combine branches\n    x = keras.layers.Concatenate()([branch1, branch2])\n    x = keras.layers.Dense(128, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"examples/feed_forward_guide/#memory-efficient-feed-forward","title":"Memory-Efficient Feed-Forward","text":"<pre><code>def create_memory_efficient_feedforward(input_dim, num_classes):\n    \"\"\"Create a memory-efficient feed-forward model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use smaller dimensions\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#speed-optimized-feed-forward","title":"Speed-Optimized Feed-Forward","text":"<pre><code>def create_speed_optimized_feedforward(input_dim, num_classes):\n    \"\"\"Create a speed-optimized feed-forward model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Minimal layers for speed\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#real-world-examples","title":"\ud83c\udf0d Real-World Examples","text":""},{"location":"examples/feed_forward_guide/#financial-risk-assessment","title":"Financial Risk Assessment","text":"<pre><code>def create_financial_risk_model(input_dim, num_classes):\n    \"\"\"Create a financial risk assessment model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Feature selection\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Risk assessment layers\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#healthcare-outcome-prediction","title":"Healthcare Outcome Prediction","text":"<pre><code>def create_healthcare_model(input_dim, num_classes):\n    \"\"\"Create a healthcare outcome prediction model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Feature engineering\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(inputs)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Medical processing layers\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n</code></pre>"},{"location":"examples/feed_forward_guide/#training-and-evaluation","title":"\ud83d\udcca Training and Evaluation","text":""},{"location":"examples/feed_forward_guide/#training-configuration","title":"Training Configuration","text":"<pre><code>def train_feedforward_model(model, X_train, y_train, X_val, y_val):\n    \"\"\"Train a feed-forward model with proper configuration.\"\"\"\n\n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Callbacks\n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5\n        )\n    ]\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=100,\n        batch_size=32,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    return history\n</code></pre>"},{"location":"examples/feed_forward_guide/#model-evaluation","title":"Model Evaluation","text":"<pre><code>def evaluate_feedforward_model(model, X_test, y_test):\n    \"\"\"Evaluate a feed-forward model.\"\"\"\n\n    # Basic evaluation\n    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n\n    # Predictions\n    predictions = model.predict(X_test)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test, axis=1)\n\n    # Additional metrics\n    from sklearn.metrics import classification_report\n\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_classes, predicted_classes))\n\n    return test_accuracy, test_loss\n</code></pre>"},{"location":"examples/feed_forward_guide/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>KDP Integration Guide: Learn about Keras Data Processor integration</li> <li>Data Analyzer Examples: Explore data analysis workflows</li> <li>Rich Docstrings Showcase: See comprehensive examples</li> <li>API Reference: Deep dive into layer parameters</li> </ol> <p>Ready for more examples? Check out the KDP Integration Guide next!</p>"},{"location":"examples/kdp_integration_guide/","title":"\ud83d\udd17 KDP Integration Guide","text":"<p>Learn how to integrate KMR layers with Keras Data Processor (KDP) for comprehensive tabular data processing workflows.</p>"},{"location":"examples/kdp_integration_guide/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>KDP Overview</li> <li>Basic Integration</li> <li>Advanced Workflows</li> <li>Best Practices</li> </ol>"},{"location":"examples/kdp_integration_guide/#kdp-overview","title":"\ud83c\udfaf KDP Overview","text":"<p>Keras Data Processor (KDP) provides powerful data preprocessing capabilities that complement KMR layers perfectly. This integration allows for:</p> <ul> <li>Seamless data preprocessing before KMR layer processing</li> <li>End-to-end pipelines from raw data to predictions</li> <li>Production-ready workflows with proper data validation</li> <li>Scalable processing for large datasets</li> </ul>"},{"location":"examples/kdp_integration_guide/#basic-integration","title":"\ud83d\udd27 Basic Integration","text":""},{"location":"examples/kdp_integration_guide/#simple-kdp-kmr-pipeline","title":"Simple KDP + KMR Pipeline","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TabularAttention, VariableSelection\nfrom keras_data_processor import DataProcessor\n\ndef create_kdp_kmr_pipeline(input_dim, num_classes):\n    \"\"\"Create a pipeline combining KDP preprocessing with KMR layers.\"\"\"\n\n    # KDP preprocessing\n    processor = DataProcessor(\n        numerical_features=['feature_1', 'feature_2', 'feature_3'],\n        categorical_features=['category_1', 'category_2'],\n        target_column='target'\n    )\n\n    # KMR model\n    inputs = keras.Input(shape=(input_dim,))\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    return processor, model\n\n# Usage\nprocessor, model = create_kdp_kmr_pipeline(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#end-to-end-training-pipeline","title":"End-to-End Training Pipeline","text":"<pre><code>def train_kdp_kmr_pipeline(processor, model, X_train, y_train, X_val, y_val):\n    \"\"\"Train a complete KDP + KMR pipeline.\"\"\"\n\n    # Preprocess data with KDP\n    X_train_processed = processor.fit_transform(X_train, y_train)\n    X_val_processed = processor.transform(X_val)\n\n    # Compile model\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Train model\n    history = model.fit(\n        X_train_processed, y_train,\n        validation_data=(X_val_processed, y_val),\n        epochs=100,\n        batch_size=32,\n        verbose=1\n    )\n\n    return history\n\n# Usage\nhistory = train_kdp_kmr_pipeline(processor, model, X_train, y_train, X_val, y_val)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#advanced-workflows","title":"\ud83d\ude80 Advanced Workflows","text":""},{"location":"examples/kdp_integration_guide/#multi-stage-processing","title":"Multi-Stage Processing","text":"<pre><code>from kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    AdvancedNumericalEmbedding,\n    GatedFeatureFusion\n)\n\ndef create_advanced_kdp_kmr_pipeline(input_dim, num_classes):\n    \"\"\"Create an advanced multi-stage pipeline.\"\"\"\n\n    # Stage 1: KDP preprocessing\n    processor = DataProcessor(\n        numerical_features=['feature_1', 'feature_2', 'feature_3'],\n        categorical_features=['category_1', 'category_2'],\n        target_column='target',\n        preprocessing_steps=[\n            'impute_missing',\n            'normalize_numerical',\n            'encode_categorical'\n        ]\n    )\n\n    # Stage 2: KMR feature engineering\n    inputs = keras.Input(shape=(input_dim,))\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n    x = VariableSelection(hidden_dim=64)(x)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    return processor, model\n</code></pre>"},{"location":"examples/kdp_integration_guide/#custom-preprocessing-integration","title":"Custom Preprocessing Integration","text":"<pre><code>def create_custom_preprocessing_pipeline(input_dim, num_classes):\n    \"\"\"Create a pipeline with custom preprocessing steps.\"\"\"\n\n    # Custom KDP configuration\n    processor = DataProcessor(\n        numerical_features=['feature_1', 'feature_2', 'feature_3'],\n        categorical_features=['category_1', 'category_2'],\n        target_column='target',\n        custom_preprocessing={\n            'feature_1': 'log_transform',\n            'feature_2': 'box_cox_transform',\n            'category_1': 'target_encoding'\n        }\n    )\n\n    # KMR model with preprocessing\n    inputs = keras.Input(shape=(input_dim,))\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n    x = VariableSelection(hidden_dim=64)(x)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    return processor, model\n</code></pre>"},{"location":"examples/kdp_integration_guide/#data-validation-and-quality","title":"\ud83d\udcca Data Validation and Quality","text":""},{"location":"examples/kdp_integration_guide/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code>def validate_data_quality(processor, X, y):\n    \"\"\"Validate data quality before processing.\"\"\"\n\n    # Check for missing values\n    missing_values = X.isnull().sum()\n    print(\"Missing values per feature:\")\n    print(missing_values)\n\n    # Check for outliers\n    numerical_features = processor.numerical_features\n    for feature in numerical_features:\n        Q1 = X[feature].quantile(0.25)\n        Q3 = X[feature].quantile(0.75)\n        IQR = Q3 - Q1\n        outliers = X[(X[feature] &lt; Q1 - 1.5 * IQR) | (X[feature] &gt; Q3 + 1.5 * IQR)]\n        print(f\"Outliers in {feature}: {len(outliers)}\")\n\n    # Check data types\n    print(\"Data types:\")\n    print(X.dtypes)\n\n    return True\n\n# Usage\nvalidate_data_quality(processor, X_train, y_train)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#preprocessing-validation","title":"Preprocessing Validation","text":"<pre><code>def validate_preprocessing(processor, X_train, y_train, X_test, y_test):\n    \"\"\"Validate preprocessing results.\"\"\"\n\n    # Fit and transform training data\n    X_train_processed = processor.fit_transform(X_train, y_train)\n    X_test_processed = processor.transform(X_test)\n\n    # Check for NaN values\n    print(\"NaN values in processed data:\")\n    print(f\"Training: {X_train_processed.isnull().sum().sum()}\")\n    print(f\"Test: {X_test_processed.isnull().sum().sum()}\")\n\n    # Check data ranges\n    print(\"Data ranges:\")\n    print(f\"Training min: {X_train_processed.min().min()}\")\n    print(f\"Training max: {X_train_processed.max().max()}\")\n    print(f\"Test min: {X_test_processed.min().min()}\")\n    print(f\"Test max: {X_test_processed.max().max()}\")\n\n    return X_train_processed, X_test_processed\n</code></pre>"},{"location":"examples/kdp_integration_guide/#production-workflows","title":"\ud83d\udd04 Production Workflows","text":""},{"location":"examples/kdp_integration_guide/#batch-processing","title":"Batch Processing","text":"<pre><code>def batch_process_data(processor, model, data_batches):\n    \"\"\"Process data in batches for large datasets.\"\"\"\n\n    results = []\n\n    for batch in data_batches:\n        # Preprocess batch\n        batch_processed = processor.transform(batch)\n\n        # Make predictions\n        predictions = model.predict(batch_processed)\n\n        results.append(predictions)\n\n    return np.concatenate(results, axis=0)\n\n# Usage\npredictions = batch_process_data(processor, model, data_batches)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#real-time-processing","title":"Real-Time Processing","text":"<pre><code>def real_time_prediction(processor, model, new_data):\n    \"\"\"Process new data in real-time.\"\"\"\n\n    # Preprocess new data\n    processed_data = processor.transform(new_data)\n\n    # Make prediction\n    prediction = model.predict(processed_data)\n\n    return prediction\n\n# Usage\nnew_prediction = real_time_prediction(processor, model, new_data)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#performance-monitoring","title":"\ud83d\udcc8 Performance Monitoring","text":""},{"location":"examples/kdp_integration_guide/#model-performance-tracking","title":"Model Performance Tracking","text":"<pre><code>def track_model_performance(processor, model, X_test, y_test):\n    \"\"\"Track model performance over time.\"\"\"\n\n    # Preprocess test data\n    X_test_processed = processor.transform(X_test)\n\n    # Make predictions\n    predictions = model.predict(X_test_processed)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test, axis=1)\n\n    # Calculate metrics\n    accuracy = np.mean(predicted_classes == true_classes)\n\n    # Log performance\n    print(f\"Model accuracy: {accuracy:.4f}\")\n\n    return accuracy\n\n# Usage\naccuracy = track_model_performance(processor, model, X_test, y_test)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#data-drift-detection","title":"Data Drift Detection","text":"<pre><code>def detect_data_drift(processor, X_train, X_new):\n    \"\"\"Detect data drift between training and new data.\"\"\"\n\n    # Preprocess both datasets\n    X_train_processed = processor.transform(X_train)\n    X_new_processed = processor.transform(X_new)\n\n    # Calculate statistical differences\n    train_mean = X_train_processed.mean()\n    new_mean = X_new_processed.mean()\n\n    train_std = X_train_processed.std()\n    new_std = X_new_processed.std()\n\n    # Calculate drift score\n    drift_score = np.mean(np.abs(train_mean - new_mean) / train_std)\n\n    print(f\"Data drift score: {drift_score:.4f}\")\n\n    if drift_score &gt; 0.1:\n        print(\"Warning: Significant data drift detected!\")\n\n    return drift_score\n\n# Usage\ndrift_score = detect_data_drift(processor, X_train, X_new)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#best-practices","title":"\ud83d\udee0\ufe0f Best Practices","text":""},{"location":"examples/kdp_integration_guide/#1-data-preprocessing-order","title":"1. Data Preprocessing Order","text":"<pre><code>def recommended_preprocessing_order():\n    \"\"\"Recommended order for data preprocessing.\"\"\"\n\n    # 1. Data validation and quality checks\n    # 2. Missing value imputation\n    # 3. Outlier detection and handling\n    # 4. Feature scaling and normalization\n    # 5. Categorical encoding\n    # 6. Feature engineering\n    # 7. Feature selection\n    # 8. Model training\n\n    pass\n</code></pre>"},{"location":"examples/kdp_integration_guide/#2-error-handling","title":"2. Error Handling","text":"<pre><code>def robust_preprocessing(processor, X, y):\n    \"\"\"Robust preprocessing with error handling.\"\"\"\n\n    try:\n        # Preprocess data\n        X_processed = processor.fit_transform(X, y)\n\n        # Validate results\n        if X_processed.isnull().any().any():\n            raise ValueError(\"NaN values found in processed data\")\n\n        return X_processed\n\n    except Exception as e:\n        print(f\"Preprocessing error: {e}\")\n        return None\n\n# Usage\nX_processed = robust_preprocessing(processor, X_train, y_train)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#3-memory-management","title":"3. Memory Management","text":"<pre><code>def memory_efficient_processing(processor, model, data_generator):\n    \"\"\"Memory-efficient processing for large datasets.\"\"\"\n\n    results = []\n\n    for batch in data_generator:\n        # Process batch\n        batch_processed = processor.transform(batch)\n\n        # Make predictions\n        predictions = model.predict(batch_processed)\n\n        results.append(predictions)\n\n        # Clear memory\n        del batch_processed\n        del predictions\n\n    return np.concatenate(results, axis=0)\n\n# Usage\npredictions = memory_efficient_processing(processor, model, data_generator)\n</code></pre>"},{"location":"examples/kdp_integration_guide/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Data Analyzer Examples: Explore data analysis workflows</li> <li>Rich Docstrings Showcase: See comprehensive examples</li> <li>BaseFeedForwardModel Guide: Learn about feed-forward architectures</li> <li>API Reference: Deep dive into layer parameters</li> </ol> <p>Ready for more examples? Check out the Data Analyzer Examples next!</p>"},{"location":"examples/rich_docstrings_showcase/","title":"\ud83d\udcd6 Rich Docstrings Showcase","text":"<p>Comprehensive examples demonstrating KMR layers with detailed documentation, best practices, and real-world use cases.</p>"},{"location":"examples/rich_docstrings_showcase/#overview","title":"\ud83c\udfaf Overview","text":"<p>This showcase provides in-depth examples of KMR layers with rich documentation, showing how to build production-ready tabular models. Each example includes:</p> <ul> <li>Detailed explanations of layer functionality</li> <li>Best practices for parameter selection</li> <li>Real-world use cases and applications</li> <li>Performance considerations and optimization tips</li> <li>Complete code examples ready to run</li> </ul>"},{"location":"examples/rich_docstrings_showcase/#attention-mechanisms","title":"\ud83e\udde0 Attention Mechanisms","text":""},{"location":"examples/rich_docstrings_showcase/#tabularattention-dual-attention-for-tabular-data","title":"TabularAttention - Dual Attention for Tabular Data","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TabularAttention\n\ndef create_tabular_attention_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using TabularAttention for dual attention mechanisms.\n\n    TabularAttention implements both inter-feature and inter-sample attention,\n    making it ideal for capturing complex relationships in tabular data.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    # Input layer\n    inputs = keras.Input(shape=(input_dim,), name='tabular_input')\n\n    # TabularAttention layer with comprehensive configuration\n    attention_layer = TabularAttention(\n        num_heads=8,                    # 8 attention heads for rich representation\n        key_dim=64,                     # 64-dimensional key vectors\n        dropout=0.1,                    # 10% dropout for regularization\n        use_attention_weights=True,     # Return attention weights for interpretation\n        attention_activation='softmax', # Softmax activation for attention weights\n        name='tabular_attention'\n    )\n\n    # Apply attention\n    x = attention_layer(inputs)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='tabular_attention_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_tabular_attention():\n    \"\"\"Demonstrate TabularAttention with sample data.\"\"\"\n\n    # Create sample data\n    X_train = np.random.random((1000, 20))\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_tabular_attention_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    # Evaluate model\n    test_loss, test_accuracy = model.evaluate(X_train, y_train, verbose=0)\n    print(f\"Model accuracy: {test_accuracy:.4f}\")\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_tabular_attention()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#multiresolutiontabularattention-multi-resolution-processing","title":"MultiResolutionTabularAttention - Multi-Resolution Processing","text":"<pre><code>from kmr.layers import MultiResolutionTabularAttention\n\ndef create_multi_resolution_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using MultiResolutionTabularAttention for different feature scales.\n\n    This layer processes numerical and categorical features separately with different\n    attention mechanisms, then combines them with cross-attention.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='multi_resolution_input')\n\n    # Multi-resolution attention with separate processing\n    attention_layer = MultiResolutionTabularAttention(\n        num_heads=8,                    # Total attention heads\n        key_dim=64,                     # Key dimension\n        dropout=0.1,                    # Dropout rate\n        numerical_heads=4,              # Heads for numerical features\n        categorical_heads=4,            # Heads for categorical features\n        name='multi_resolution_attention'\n    )\n\n    # Apply multi-resolution attention\n    x = attention_layer(inputs)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='multi_resolution_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_multi_resolution():\n    \"\"\"Demonstrate MultiResolutionTabularAttention with mixed data types.\"\"\"\n\n    # Create sample data with mixed types\n    X_numerical = np.random.random((1000, 10))\n    X_categorical = np.random.randint(0, 5, (1000, 10))\n    X_mixed = np.concatenate([X_numerical, X_categorical], axis=1)\n\n    y = np.random.randint(0, 3, (1000,))\n    y = keras.utils.to_categorical(y, 3)\n\n    # Create model\n    model = create_multi_resolution_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_mixed, y,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_multi_resolution()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#feature-engineering","title":"\ud83d\udd27 Feature Engineering","text":""},{"location":"examples/rich_docstrings_showcase/#variableselection-intelligent-feature-selection","title":"VariableSelection - Intelligent Feature Selection","text":"<pre><code>from kmr.layers import VariableSelection\n\ndef create_variable_selection_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using VariableSelection for intelligent feature selection.\n\n    VariableSelection uses gated residual networks to learn feature importance\n    and select the most relevant features for the task.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='variable_selection_input')\n\n    # Variable selection with context\n    selection_layer = VariableSelection(\n        hidden_dim=64,                  # Hidden dimension for GRN\n        dropout=0.1,                    # Dropout rate\n        use_context=True,               # Use context for selection\n        context_dim=32,                 # Context dimension\n        name='variable_selection'\n    )\n\n    # Apply variable selection\n    x = selection_layer(inputs)\n\n    # Additional processing\n    x = keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n    x = keras.layers.Dropout(0.2, name='dropout_1')(x)\n    x = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='variable_selection_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_variable_selection():\n    \"\"\"Demonstrate VariableSelection with feature importance analysis.\"\"\"\n\n    # Create sample data\n    X_train = np.random.random((1000, 20))\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_variable_selection_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    # Analyze feature importance\n    selection_layer = model.get_layer('variable_selection')\n    feature_weights = selection_layer.get_weights()\n\n    print(\"Feature selection weights shape:\", feature_weights[0].shape)\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_variable_selection()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#advancednumericalembedding-rich-numerical-representations","title":"AdvancedNumericalEmbedding - Rich Numerical Representations","text":"<pre><code>from kmr.layers import AdvancedNumericalEmbedding\n\ndef create_advanced_embedding_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using AdvancedNumericalEmbedding for rich numerical representations.\n\n    This layer combines continuous MLP processing with discrete binning/embedding,\n    providing a dual-branch architecture for numerical features.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='embedding_input')\n\n    # Advanced numerical embedding\n    embedding_layer = AdvancedNumericalEmbedding(\n        embedding_dim=64,               # Embedding dimension\n        num_bins=10,                    # Number of bins for discretization\n        hidden_dim=128,                 # Hidden dimension for MLP\n        dropout=0.1,                    # Dropout rate\n        name='advanced_embedding'\n    )\n\n    # Apply embedding\n    x = embedding_layer(inputs)\n\n    # Additional processing\n    x = keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n    x = keras.layers.Dropout(0.2, name='dropout_1')(x)\n    x = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='advanced_embedding_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_advanced_embedding():\n    \"\"\"Demonstrate AdvancedNumericalEmbedding with numerical data.\"\"\"\n\n    # Create sample numerical data\n    X_train = np.random.normal(0, 1, (1000, 20))\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_advanced_embedding_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_advanced_embedding()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#preprocessing","title":"\u2699\ufe0f Preprocessing","text":""},{"location":"examples/rich_docstrings_showcase/#differentiabletabularpreprocessor-end-to-end-preprocessing","title":"DifferentiableTabularPreprocessor - End-to-End Preprocessing","text":"<pre><code>from kmr.layers import DifferentiableTabularPreprocessor\n\ndef create_preprocessing_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using DifferentiableTabularPreprocessor for end-to-end preprocessing.\n\n    This layer integrates preprocessing into the model, allowing for learnable\n    imputation and normalization strategies.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='preprocessing_input')\n\n    # Differentiable preprocessing\n    preprocessor = DifferentiableTabularPreprocessor(\n        imputation_strategy='learnable',    # Learnable imputation\n        normalization='learnable',          # Learnable normalization\n        dropout=0.1,                        # Dropout rate\n        name='tabular_preprocessor'\n    )\n\n    # Apply preprocessing\n    x = preprocessor(inputs)\n\n    # Additional processing\n    x = keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n    x = keras.layers.Dropout(0.2, name='dropout_1')(x)\n    x = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='preprocessing_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_preprocessing():\n    \"\"\"Demonstrate DifferentiableTabularPreprocessor with missing data.\"\"\"\n\n    # Create sample data with missing values\n    X_train = np.random.random((1000, 20))\n    # Introduce missing values\n    missing_mask = np.random.random((1000, 20)) &lt; 0.1\n    X_train[missing_mask] = np.nan\n\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_preprocessing_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_preprocessing()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#specialized-architectures","title":"\ud83c\udfd7\ufe0f Specialized Architectures","text":""},{"location":"examples/rich_docstrings_showcase/#gatedresidualnetwork-advanced-residual-processing","title":"GatedResidualNetwork - Advanced Residual Processing","text":"<pre><code>from kmr.layers import GatedResidualNetwork\n\ndef create_gated_residual_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using GatedResidualNetwork for advanced residual processing.\n\n    This layer combines residual connections with gated linear units for\n    improved gradient flow and feature transformation.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='gated_residual_input')\n\n    # Gated residual networks\n    x = GatedResidualNetwork(\n        units=64,                        # Number of units\n        dropout_rate=0.1,                # Dropout rate\n        name='grn_1'\n    )(inputs)\n\n    x = GatedResidualNetwork(\n        units=64,\n        dropout_rate=0.1,\n        name='grn_2'\n    )(x)\n\n    x = GatedResidualNetwork(\n        units=64,\n        dropout_rate=0.1,\n        name='grn_3'\n    )(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='gated_residual_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_gated_residual():\n    \"\"\"Demonstrate GatedResidualNetwork with deep architecture.\"\"\"\n\n    # Create sample data\n    X_train = np.random.random((1000, 20))\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_gated_residual_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_gated_residual()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#tabularmoelayer-mixture-of-experts","title":"TabularMoELayer - Mixture of Experts","text":"<pre><code>from kmr.layers import TabularMoELayer\n\ndef create_moe_model(input_dim, num_classes):\n    \"\"\"\n    Create a model using TabularMoELayer for mixture of experts architecture.\n\n    This layer routes input features through multiple expert sub-networks\n    and aggregates their outputs via a learnable gating mechanism.\n\n    Args:\n        input_dim (int): Number of input features\n        num_classes (int): Number of output classes\n\n    Returns:\n        keras.Model: Compiled model ready for training\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='moe_input')\n\n    # Mixture of experts\n    moe_layer = TabularMoELayer(\n        num_experts=4,                   # Number of expert networks\n        expert_units=16,                 # Units per expert\n        name='tabular_moe'\n    )\n\n    # Apply MoE\n    x = moe_layer(inputs)\n\n    # Additional processing\n    x = keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n    x = keras.layers.Dropout(0.2, name='dropout_1')(x)\n    x = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='moe_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_moe():\n    \"\"\"Demonstrate TabularMoELayer with expert routing.\"\"\"\n\n    # Create sample data\n    X_train = np.random.random((1000, 20))\n    y_train = np.random.randint(0, 3, (1000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create model\n    model = create_moe_model(input_dim=20, num_classes=3)\n\n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=32,\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_moe()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#model-interpretation-and-analysis","title":"\ud83d\udd0d Model Interpretation and Analysis","text":""},{"location":"examples/rich_docstrings_showcase/#attention-weight-analysis","title":"Attention Weight Analysis","text":"<pre><code>def analyze_attention_weights(model, X_test, layer_name='tabular_attention'):\n    \"\"\"\n    Analyze attention weights to understand model behavior.\n\n    Args:\n        model: Trained model\n        X_test: Test data\n        layer_name: Name of attention layer\n\n    Returns:\n        dict: Analysis results\n    \"\"\"\n\n    # Get attention layer\n    attention_layer = model.get_layer(layer_name)\n\n    # Create model that outputs attention weights\n    attention_model = keras.Model(\n        inputs=model.input,\n        outputs=attention_layer.output\n    )\n\n    # Get attention weights\n    attention_weights = attention_model.predict(X_test)\n\n    # Analyze attention patterns\n    mean_attention = np.mean(attention_weights, axis=0)\n    std_attention = np.std(attention_weights, axis=0)\n\n    # Feature importance\n    feature_importance = np.mean(attention_weights, axis=(0, 1))\n\n    analysis = {\n        'attention_weights': attention_weights,\n        'mean_attention': mean_attention,\n        'std_attention': std_attention,\n        'feature_importance': feature_importance\n    }\n\n    return analysis\n\n# Usage example\ndef demonstrate_attention_analysis():\n    \"\"\"Demonstrate attention weight analysis.\"\"\"\n\n    # Create sample data\n    X_test = np.random.random((100, 20))\n\n    # Analyze attention weights\n    analysis = analyze_attention_weights(model, X_test)\n\n    print(\"Feature importance scores:\")\n    for i, importance in enumerate(analysis['feature_importance']):\n        print(f\"Feature {i}: {importance:.4f}\")\n\n    return analysis\n\n# Run demonstration\nanalysis = demonstrate_attention_analysis()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"examples/rich_docstrings_showcase/#memory-efficient-training","title":"Memory-Efficient Training","text":"<pre><code>def create_memory_efficient_model(input_dim, num_classes):\n    \"\"\"\n    Create a memory-efficient model for large datasets.\n\n    This model uses smaller dimensions and fewer parameters to reduce\n    memory usage while maintaining good performance.\n    \"\"\"\n\n    inputs = keras.Input(shape=(input_dim,), name='memory_efficient_input')\n\n    # Use smaller dimensions\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = TabularAttention(num_heads=4, key_dim=32)(x)\n    x = GatedFeatureFusion(hidden_dim=64)(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(\n        num_classes, \n        activation='softmax',\n        name='predictions'\n    )(x)\n\n    # Create and compile model\n    model = keras.Model(inputs, outputs, name='memory_efficient_model')\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage example\ndef demonstrate_memory_efficiency():\n    \"\"\"Demonstrate memory-efficient training.\"\"\"\n\n    # Create large dataset\n    X_train = np.random.random((10000, 50))\n    y_train = np.random.randint(0, 3, (10000,))\n    y_train = keras.utils.to_categorical(y_train, 3)\n\n    # Create memory-efficient model\n    model = create_memory_efficient_model(input_dim=50, num_classes=3)\n\n    # Train with smaller batch size\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=10,\n        batch_size=16,  # Smaller batch size\n        verbose=1\n    )\n\n    return model, history\n\n# Run demonstration\nmodel, history = demonstrate_memory_efficiency()\n</code></pre>"},{"location":"examples/rich_docstrings_showcase/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>BaseFeedForwardModel Guide: Learn about feed-forward architectures</li> <li>KDP Integration Guide: Integrate with Keras Data Processor</li> <li>Data Analyzer Examples: Explore data analysis workflows</li> <li>API Reference: Deep dive into layer parameters</li> </ol> <p>Ready for more examples? Check out the BaseFeedForwardModel Guide next!</p>"},{"location":"getting-started/concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>Understand the fundamental concepts behind KMR and how to effectively use its layers for tabular data modeling.</p>"},{"location":"getting-started/concepts/#what-is-kmr","title":"\ud83c\udfaf What is KMR?","text":"<p>KMR (Keras Model Registry) is a comprehensive collection of specialized layers designed exclusively for tabular data. Unlike traditional neural network layers that were designed for images or sequences, KMR layers understand the unique characteristics of tabular data.</p>"},{"location":"getting-started/concepts/#key-principles","title":"Key Principles","text":"<ol> <li>Tabular-First Design: Every layer is optimized for tabular data characteristics</li> <li>Production Ready: Battle-tested layers used in real-world applications</li> <li>Keras 3 Native: Built specifically for Keras 3 with modern best practices</li> <li>No TensorFlow Dependencies: Pure Keras implementation for maximum compatibility</li> </ol>"},{"location":"getting-started/concepts/#understanding-tabular-data","title":"\ud83d\udcca Understanding Tabular Data","text":""},{"location":"getting-started/concepts/#characteristics-of-tabular-data","title":"Characteristics of Tabular Data","text":"<pre><code># Example tabular dataset\nimport pandas as pd\nimport numpy as np\n\n# Sample tabular data\ndata = {\n    'age': [25, 30, 35, 40, 45],\n    'income': [50000, 75000, 90000, 110000, 130000],\n    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],\n    'city': ['NYC', 'SF', 'LA', 'Chicago', 'Boston']\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre> <p>Key Characteristics: - Mixed Data Types: Numerical and categorical features - No Spatial Structure: Unlike images, features don't have spatial relationships - Variable Importance: Some features are more important than others - Missing Values: Common in real-world datasets - Feature Interactions: Complex relationships between features</p>"},{"location":"getting-started/concepts/#layer-architecture","title":"\ud83c\udfd7\ufe0f Layer Architecture","text":""},{"location":"getting-started/concepts/#layer-categories","title":"Layer Categories","text":""},{"location":"getting-started/concepts/#1-attention-layers","title":"1. \ud83e\udde0 Attention Layers","text":"<p>Focus on important features and relationships:</p> <pre><code>from kmr.layers import TabularAttention, ColumnAttention, RowAttention\n\n# Tabular attention for feature relationships\nattention = TabularAttention(num_heads=8, key_dim=64)\n\n# Column attention for feature importance\ncol_attention = ColumnAttention(hidden_dim=64)\n\n# Row attention for sample relationships\nrow_attention = RowAttention(hidden_dim=64)\n</code></pre>"},{"location":"getting-started/concepts/#2-preprocessing-layers","title":"2. \u2699\ufe0f Preprocessing Layers","text":"<p>Handle data preparation and missing values:</p> <pre><code>from kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    DateParsingLayer,\n    DateEncodingLayer\n)\n\n# End-to-end preprocessing\npreprocessor = DifferentiableTabularPreprocessor(\n    imputation_strategy='learnable',\n    normalization='learnable'\n)\n\n# Date handling\ndate_parser = DateParsingLayer()\ndate_encoder = DateEncodingLayer()\n</code></pre>"},{"location":"getting-started/concepts/#3-feature-engineering-layers","title":"3. \ud83d\udd27 Feature Engineering Layers","text":"<p>Transform and select features intelligently:</p> <pre><code>from kmr.layers import (\n    VariableSelection,\n    GatedFeatureFusion,\n    AdvancedNumericalEmbedding\n)\n\n# Intelligent feature selection\nvar_sel = VariableSelection(hidden_dim=64)\n\n# Feature fusion\nfusion = GatedFeatureFusion(hidden_dim=128)\n\n# Advanced numerical embedding\nembedding = AdvancedNumericalEmbedding(embedding_dim=64)\n</code></pre>"},{"location":"getting-started/concepts/#4-specialized-layers","title":"4. \ud83c\udfd7\ufe0f Specialized Layers","text":"<p>Advanced architectures for specific use cases:</p> <pre><code>from kmr.layers import (\n    GatedResidualNetwork,\n    TransformerBlock,\n    TabularMoELayer\n)\n\n# Gated residual network\ngrn = GatedResidualNetwork(units=64, dropout_rate=0.2)\n\n# Transformer block\ntransformer = TransformerBlock(dim_model=64, num_heads=4)\n\n# Mixture of experts\nmoe = TabularMoELayer(num_experts=4, expert_units=16)\n</code></pre>"},{"location":"getting-started/concepts/#5-utility-layers","title":"5. \ud83d\udee0\ufe0f Utility Layers","text":"<p>Essential tools for data processing:</p> <pre><code>from kmr.layers import (\n    CastToFloat32Layer,\n    NumericalAnomalyDetection,\n    FeatureCutout\n)\n\n# Type casting\ncast_layer = CastToFloat32Layer()\n\n# Anomaly detection\nanomaly_detector = NumericalAnomalyDetection()\n\n# Data augmentation\ncutout = FeatureCutout(cutout_prob=0.1)\n</code></pre>"},{"location":"getting-started/concepts/#layer-composition-patterns","title":"\ud83d\udd04 Layer Composition Patterns","text":""},{"location":"getting-started/concepts/#1-sequential-composition","title":"1. Sequential Composition","text":"<p>Layers applied in sequence:</p> <pre><code>def create_sequential_model(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Sequential processing\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = VariableSelection(hidden_dim=64)(x)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    return keras.Model(inputs, x)\n</code></pre>"},{"location":"getting-started/concepts/#2-parallel-composition","title":"2. Parallel Composition","text":"<p>Multiple processing branches:</p> <pre><code>def create_parallel_model(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Parallel processing branches\n    branch1 = VariableSelection(hidden_dim=64)(inputs)\n    branch2 = TabularAttention(num_heads=8, key_dim=64)(inputs)\n\n    # Fusion\n    x = GatedFeatureFusion(hidden_dim=128)([branch1, branch2])\n\n    return keras.Model(inputs, x)\n</code></pre>"},{"location":"getting-started/concepts/#3-residual-composition","title":"3. Residual Composition","text":"<p>Skip connections for gradient flow:</p> <pre><code>def create_residual_model(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Residual block\n    x = GatedResidualNetwork(units=64)(inputs)\n    x = GatedResidualNetwork(units=64)(x)\n\n    # Skip connection\n    x = keras.layers.Add()([inputs, x])\n\n    return keras.Model(inputs, x)\n</code></pre>"},{"location":"getting-started/concepts/#layer-parameters","title":"\ud83c\udf9b\ufe0f Layer Parameters","text":""},{"location":"getting-started/concepts/#common-parameters","title":"Common Parameters","text":""},{"location":"getting-started/concepts/#hidden-dimensions","title":"Hidden Dimensions","text":"<pre><code># Control model capacity\nlayer = VariableSelection(hidden_dim=64)  # Small model\nlayer = VariableSelection(hidden_dim=256) # Large model\n</code></pre>"},{"location":"getting-started/concepts/#dropout-rates","title":"Dropout Rates","text":"<pre><code># Regularization\nlayer = TabularAttention(dropout=0.1)  # Light regularization\nlayer = TabularAttention(dropout=0.3)  # Heavy regularization\n</code></pre>"},{"location":"getting-started/concepts/#attention-heads","title":"Attention Heads","text":"<pre><code># Multi-head attention\nlayer = TabularAttention(num_heads=4)  # Few heads\nlayer = TabularAttention(num_heads=16) # Many heads\n</code></pre>"},{"location":"getting-started/concepts/#performance-considerations","title":"Performance Considerations","text":""},{"location":"getting-started/concepts/#memory-usage","title":"Memory Usage","text":"<pre><code># Memory-efficient configuration\nlayer = TabularAttention(\n    num_heads=4,      # Fewer heads\n    key_dim=32,       # Smaller key dimension\n    dropout=0.1\n)\n</code></pre>"},{"location":"getting-started/concepts/#computational-speed","title":"Computational Speed","text":"<pre><code># Fast configuration\nlayer = VariableSelection(\n    hidden_dim=32,    # Smaller hidden dimension\n    dropout=0.1       # Light dropout\n)\n</code></pre>"},{"location":"getting-started/concepts/#best-practices","title":"\ud83d\udd0d Best Practices","text":""},{"location":"getting-started/concepts/#1-start-simple","title":"1. Start Simple","text":"<p>Begin with basic layers and gradually add complexity:</p> <pre><code># Start with preprocessing\nx = DifferentiableTabularPreprocessor()(inputs)\n\n# Add feature selection\nx = VariableSelection(hidden_dim=64)(x)\n\n# Add attention\nx = TabularAttention(num_heads=8, key_dim=64)(x)\n</code></pre>"},{"location":"getting-started/concepts/#2-monitor-performance","title":"2. Monitor Performance","text":"<p>Track training metrics and adjust accordingly:</p> <pre><code># Monitor during training\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Use callbacks for monitoring\ncallbacks = [\n    keras.callbacks.EarlyStopping(patience=10),\n    keras.callbacks.ReduceLROnPlateau(factor=0.5)\n]\n</code></pre>"},{"location":"getting-started/concepts/#3-experiment-with-architectures","title":"3. Experiment with Architectures","text":"<p>Try different layer combinations:</p> <pre><code># Architecture 1: Attention-focused\ndef attention_model(inputs):\n    x = TabularAttention(num_heads=8)(inputs)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n    return x\n\n# Architecture 2: Selection-focused\ndef selection_model(inputs):\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = GatedResidualNetwork(units=64)(x)\n    return x\n</code></pre>"},{"location":"getting-started/concepts/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Explore Layers: Check out the Layer Explorer</li> <li>Read Documentation: Dive into specific layer documentation</li> <li>Try Examples: Run through practical examples</li> <li>Build Models: Start creating your own tabular models</li> </ol> <p>Ready to dive deeper? Explore the Layer Explorer to see all available layers!</p>"},{"location":"getting-started/installation/","title":"\ud83d\udce6 Installation Guide","text":"<p>Install KMR and get your development environment ready for tabular modeling with Keras 3.</p>"},{"location":"getting-started/installation/#quick-install","title":"\ud83c\udfaf Quick Install","text":"<pre><code>pip install keras-model-registry\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"\ud83d\udd27 Requirements","text":""},{"location":"getting-started/installation/#python-version","title":"Python Version","text":"<ul> <li>Python 3.8+ (recommended: Python 3.10+)</li> </ul>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>Keras 3.0+ (TensorFlow backend recommended)</li> <li>NumPy 1.21+</li> <li>Pandas 1.3+ (for data handling)</li> </ul>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>Matplotlib (for visualization)</li> <li>Seaborn (for statistical plots)</li> <li>Scikit-learn (for preprocessing utilities)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"\ud83d\ude80 Installation Methods","text":""},{"location":"getting-started/installation/#1-pip-install-recommended","title":"1. Pip Install (Recommended)","text":"<pre><code># Latest stable release\npip install keras-model-registry\n\n# With optional dependencies\npip install keras-model-registry[full]\n\n# Specific version\npip install keras-model-registry==1.0.0\n</code></pre>"},{"location":"getting-started/installation/#2-development-install","title":"2. Development Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-org/keras-model-registry.git\ncd keras-model-registry\n\n# Install in development mode\npip install -e .\n\n# Install with development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#3-conda-install","title":"3. Conda Install","text":"<pre><code># Create a new environment\nconda create -n kmr python=3.10\nconda activate kmr\n\n# Install KMR\npip install keras-model-registry\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"\ud83d\udd0d Verify Installation","text":"<p>Test your installation with this simple script:</p> <pre><code>import keras\nfrom kmr.layers import TabularAttention\n\n# Test basic import\nprint(\"\u2705 KMR imported successfully!\")\n\n# Test layer creation\nlayer = TabularAttention(num_heads=8, key_dim=64)\nprint(\"\u2705 TabularAttention layer created!\")\n\n# Test with sample data\nimport numpy as np\nx = np.random.random((32, 20))\noutput = layer(x)\nprint(f\"\u2705 Layer output shape: {output.shape}\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#importerror-no-module-named-keras","title":"ImportError: No module named 'keras'","text":"<pre><code># Install Keras 3\npip install keras&gt;=3.0.0\n</code></pre>"},{"location":"getting-started/installation/#tensorflow-backend-issues","title":"TensorFlow Backend Issues","text":"<pre><code># Install TensorFlow\npip install tensorflow&gt;=2.13.0\n\n# Or use JAX backend\npip install jax jaxlib\n</code></pre>"},{"location":"getting-started/installation/#memory-issues","title":"Memory Issues","text":"<pre><code># Set memory growth for TensorFlow\nimport tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    tf.config.experimental.set_memory_growth(gpus[0], True)\n</code></pre>"},{"location":"getting-started/installation/#backend-configuration","title":"Backend Configuration","text":"<p>KMR works with multiple Keras backends:</p> <pre><code># TensorFlow backend (default)\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\n# JAX backend\nos.environ['KERAS_BACKEND'] = 'jax'\n\n# PyTorch backend\nos.environ['KERAS_BACKEND'] = 'torch'\n</code></pre>"},{"location":"getting-started/installation/#system-requirements","title":"\ud83d\udccb System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>RAM: 4GB</li> <li>Storage: 1GB free space</li> <li>CPU: 2 cores</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>RAM: 8GB+</li> <li>Storage: 5GB+ free space</li> <li>CPU: 4+ cores</li> <li>GPU: NVIDIA GPU with CUDA support (optional)</li> </ul>"},{"location":"getting-started/installation/#updating-kmr","title":"\ud83d\udd04 Updating KMR","text":"<pre><code># Update to latest version\npip install --upgrade keras-model-registry\n\n# Check current version\npython -c \"import kmr; print(kmr.__version__)\"\n</code></pre>"},{"location":"getting-started/installation/#testing-installation","title":"\ud83e\uddea Testing Installation","text":"<p>Run the test suite to ensure everything works:</p> <pre><code># Run basic tests\npython -c \"\nimport kmr\nfrom kmr.layers import *\nprint('All layers imported successfully!')\n\"\n\n# Run comprehensive tests (if available)\npytest tests/\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Quick Start: Follow the Quick Start Guide</li> <li>Explore Layers: Check out the Layer Explorer</li> <li>Read Documentation: Browse the Layers section</li> <li>Try Examples: Run through the Examples</li> </ol> <p>Installation complete! Ready to start building with KMR? Head to the Quick Start Guide!</p>"},{"location":"getting-started/quickstart/","title":"\ud83d\ude80 Quick Start Guide","text":"<p>Get up and running with KMR in minutes! This guide will walk you through installing KMR and building your first tabular model.</p>"},{"location":"getting-started/quickstart/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install keras-model-registry\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-model","title":"\ud83c\udfaf Your First Model","text":"<p>Here's a complete example that demonstrates the power of KMR layers:</p> <pre><code>import keras\nfrom kmr.layers import (\n    TabularAttention, \n    VariableSelection, \n    GatedFeatureFusion,\n    DifferentiableTabularPreprocessor\n)\n\n# Create a simple tabular model\ndef create_tabular_model(input_dim, num_classes):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing layer\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Variable selection\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Attention mechanism\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Build and compile model\nmodel = create_tabular_model(input_dim=20, num_classes=3)\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Model created successfully!\")\nprint(f\"Total parameters: {model.count_params():,}\")\n</code></pre>"},{"location":"getting-started/quickstart/#key-concepts","title":"\ud83d\udd27 Key Concepts","text":""},{"location":"getting-started/quickstart/#1-layer-categories","title":"1. Layer Categories","text":"<ul> <li>\ud83e\udde0 Attention: Focus on important features and relationships</li> <li>\u2699\ufe0f Preprocessing: Handle missing values and data preparation</li> <li>\ud83d\udd27 Feature Engineering: Transform and select features intelligently</li> <li>\ud83c\udfd7\ufe0f Specialized: Advanced architectures for specific use cases</li> <li>\ud83d\udee0\ufe0f Utility: Essential tools for data processing</li> </ul>"},{"location":"getting-started/quickstart/#2-layer-composition","title":"2. Layer Composition","text":"<p>KMR layers are designed to work together seamlessly:</p> <pre><code># Example: Building a feature engineering pipeline\nfrom kmr.layers import (\n    AdvancedNumericalEmbedding,\n    DistributionAwareEncoder,\n    SparseAttentionWeighting\n)\n\n# Create feature processing pipeline\ndef feature_pipeline(inputs):\n    # Embed numerical features\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(inputs)\n\n    # Encode with distribution awareness\n    x = DistributionAwareEncoder(encoding_dim=64)(x)\n\n    # Apply sparse attention weighting\n    x = SparseAttentionWeighting(temperature=1.0)(x)\n\n    return x\n</code></pre>"},{"location":"getting-started/quickstart/#3-performance-optimization","title":"3. Performance Optimization","text":"<p>KMR layers are optimized for production use:</p> <pre><code># Example: Memory-efficient model\ndef create_efficient_model(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use memory-efficient layers\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = VariableSelection(hidden_dim=32)(x)  # Smaller hidden dim\n    x = TabularAttention(num_heads=4, key_dim=32)(x)  # Fewer heads\n\n    return keras.Model(inputs, x)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Explore Layers: Check out the Layer Explorer to see all available layers</li> <li>Read Documentation: Dive deep into specific layers in the Layers section</li> <li>Try Examples: Run through the Examples to see real-world applications</li> <li>API Reference: Consult the API Reference for detailed parameter information</li> </ol>"},{"location":"getting-started/quickstart/#need-help","title":"\ud83c\udd98 Need Help?","text":"<ul> <li>Documentation: Browse the comprehensive layer documentation</li> <li>Examples: Check out the examples directory for practical implementations</li> <li>GitHub: Report issues or contribute to the project</li> </ul> <p>Ready to build amazing tabular models? Start with the Layer Explorer to discover all available layers!</p>"},{"location":"layers/advanced-graph-feature/","title":"\ud83d\udd78\ufe0f AdvancedGraphFeatureLayer\ud83d\udd78\ufe0f AdvancedGraphFeatureLayer","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/advanced-graph-feature/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>AdvancedGraphFeatureLayer</code> is a sophisticated graph-based feature layer that projects scalar features into an embedding space and applies multi-head self-attention to compute data-dependent dynamic adjacencies between features. It learns edge attributes by considering both raw embeddings and their differences, with optional hierarchical aggregation.</p> <p>This layer is particularly powerful for tabular data where feature interactions are important, providing a way to learn complex, dynamic relationships between features that traditional methods cannot capture.</p>"},{"location":"layers/advanced-graph-feature/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The AdvancedGraphFeatureLayer processes data through a sophisticated graph-based transformation:</p> <ol> <li>Feature Embedding: Projects scalar features into embedding space</li> <li>Multi-Head Attention: Computes data-dependent dynamic adjacencies</li> <li>Edge Learning: Learns edge attributes from embeddings and differences</li> <li>Hierarchical Aggregation: Optionally groups features into clusters</li> <li>Residual Connection: Adds residual connection with layer normalization</li> <li>Output Projection: Projects back to original feature space</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Feature Embedding]\n    B --&gt; C[Multi-Head Attention]\n    C --&gt; D[Dynamic Adjacency Matrix]\n    D --&gt; E[Edge Learning]\n    E --&gt; F[Hierarchical Aggregation]\n    F --&gt; G[Residual Connection]\n    A --&gt; G\n    G --&gt; H[Layer Normalization]\n    H --&gt; I[Output Projection]\n    I --&gt; J[Transformed Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#e1f5fe,stroke:#03a9f4\n    style F fill:#fff3e0,stroke:#ff9800</code></pre>"},{"location":"layers/advanced-graph-feature/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach AdvancedGraphFeatureLayer's Solution Feature Interactions Manual feature crosses \ud83c\udfaf Automatic learning of feature relationships Dynamic Relationships Static feature processing \u26a1 Data-dependent dynamic adjacencies Complex Patterns Limited pattern recognition \ud83e\udde0 Multi-head attention for complex patterns Hierarchical Structure Flat feature processing \ud83d\udd17 Hierarchical aggregation for structured data"},{"location":"layers/advanced-graph-feature/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Tabular Data: Complex feature interaction modeling</li> <li>Feature Engineering: Automatic feature relationship learning</li> <li>Graph Neural Networks: Graph-based processing for tabular data</li> <li>Hierarchical Data: Data with known grouping structure</li> <li>Complex Patterns: Capturing complex feature relationships</li> </ul>"},{"location":"layers/advanced-graph-feature/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/advanced-graph-feature/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedGraphFeatureLayer\n\n# Create sample input data\nbatch_size, num_features = 32, 10\nx = keras.random.normal((batch_size, num_features))\n\n# Apply advanced graph feature layer\ngraph_layer = AdvancedGraphFeatureLayer(embed_dim=16, num_heads=4)\noutput = graph_layer(x, training=True)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {output.shape}\")     # (32, 10)\n</code></pre>"},{"location":"layers/advanced-graph-feature/#with-hierarchical-aggregation","title":"With Hierarchical Aggregation","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedGraphFeatureLayer\n\n# Create sample input data\nx = keras.random.normal((32, 20))  # 20 features\n\n# Apply with hierarchical aggregation\ngraph_layer = AdvancedGraphFeatureLayer(\n    embed_dim=16,\n    num_heads=4,\n    hierarchical=True,\n    num_groups=4\n)\noutput = graph_layer(x, training=True)\n\nprint(f\"Output shape: {output.shape}\")     # (32, 20)\n</code></pre>"},{"location":"layers/advanced-graph-feature/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedGraphFeatureLayer\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    AdvancedGraphFeatureLayer(embed_dim=16, num_heads=4),\n    keras.layers.Dense(16, activation='relu'),\n    AdvancedGraphFeatureLayer(embed_dim=8, num_heads=2),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/advanced-graph-feature/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedGraphFeatureLayer\n\n# Define inputs\ninputs = keras.Input(shape=(25,))  # 25 features\n\n# Apply advanced graph feature layer\nx = AdvancedGraphFeatureLayer(embed_dim=16, num_heads=4)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = AdvancedGraphFeatureLayer(embed_dim=16, num_heads=4)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/advanced-graph-feature/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with hierarchical aggregation\ndef create_hierarchical_graph_model():\n    inputs = keras.Input(shape=(30,))  # 30 features\n\n    # Multiple graph layers with different configurations\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=32,\n        num_heads=8,\n        dropout_rate=0.1,\n        hierarchical=True,\n        num_groups=6\n    )(inputs)\n\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=24,\n        num_heads=6,\n        dropout_rate=0.1,\n        hierarchical=True,\n        num_groups=4\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_hierarchical_graph_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/advanced-graph-feature/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/advanced-graph-feature/#class-advancedgraphfeaturelayer","title":"Class: AdvancedGraphFeatureLayer","text":"<p>Inherits from: <code>BaseLayer</code></p> <pre><code>class AdvancedGraphFeatureLayer(BaseLayer):\n    \"\"\"Advanced graph-based feature layer for tabular data.\"\"\"\n</code></pre>"},{"location":"layers/advanced-graph-feature/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li><code>embed_dim</code> (int): Dimensionality of the projected feature embeddings. Default: 16</li> <li><code>num_heads</code> (int): Number of attention heads in the multi-head self-attention. Default: 4</li> <li><code>hierarchical</code> (bool): Whether to apply hierarchical aggregation. Default: False</li> <li><code>num_groups</code> (int): Number of groups for hierarchical aggregation (only used if hierarchical=True). Default: 4</li> <li><code>dropout_rate</code> (float): Dropout rate for regularization. Default: 0.1</li> <li><code>name</code> (str, optional): Layer name. Default: None</li> </ul>"},{"location":"layers/advanced-graph-feature/#key-methods","title":"Key Methods","text":"<ul> <li><code>call(inputs)</code>: Forward pass that processes input tensor through graph-based transformations</li> <li><code>build(input_shape)</code>: Builds the layer with given input shape</li> <li><code>get_config()</code>: Returns layer configuration for serialization</li> <li><code>compute_output_shape(input_shape)</code>: Computes output shape given input shape</li> </ul>"},{"location":"layers/advanced-graph-feature/#inputoutput","title":"Input/Output","text":"<ul> <li>Input Shape: <code>(batch_size, num_features)</code></li> <li>Output Shape: <code>(batch_size, num_features)</code></li> </ul>"},{"location":"layers/advanced-graph-feature/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/advanced-graph-feature/#embed_dim-int","title":"<code>embed_dim</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the projected feature embeddings</li> <li>Range: 8 to 128+ (typically 16-64)</li> <li>Impact: Larger values = more expressive embeddings but more parameters</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/advanced-graph-feature/#num_heads-int","title":"<code>num_heads</code> (int)","text":"<ul> <li>Purpose: Number of attention heads</li> <li>Range: 1 to 16+ (typically 4-8)</li> <li>Impact: More heads = more diverse attention patterns</li> <li>Recommendation: Use 4-8 heads for most applications</li> </ul>"},{"location":"layers/advanced-graph-feature/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Dropout rate applied to attention weights</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Use 0.1-0.2 for regularization</li> </ul>"},{"location":"layers/advanced-graph-feature/#hierarchical-bool","title":"<code>hierarchical</code> (bool)","text":"<ul> <li>Purpose: Whether to apply hierarchical aggregation</li> <li>Default: False</li> <li>Impact: Enables feature grouping for large feature sets</li> <li>Recommendation: Use True for &gt;20 features or known grouping structure</li> </ul>"},{"location":"layers/advanced-graph-feature/#num_groups-int-optional","title":"<code>num_groups</code> (int, optional)","text":"<ul> <li>Purpose: Number of groups for hierarchical aggregation</li> <li>Range: 2 to 20+ (typically 4-8)</li> <li>Impact: Controls granularity of hierarchical aggregation</li> <li>Recommendation: Use 4-8 groups for most applications</li> </ul>"},{"location":"layers/advanced-graph-feature/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with features\u00b2</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe\ud83d\udcbe High memory usage due to attention computation</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex feature interactions</li> <li>Best For: Tabular data with complex feature relationships</li> </ul>"},{"location":"layers/advanced-graph-feature/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/advanced-graph-feature/#example-1-complex-feature-interactions","title":"Example 1: Complex Feature Interactions","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import AdvancedGraphFeatureLayer\n\n# Create a model for complex feature interactions\ndef create_feature_interaction_model():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Multiple graph layers for different interaction levels\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=32,\n        num_heads=8,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=24,\n        num_heads=6,\n        dropout_rate=0.1\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_feature_interaction_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Feature interaction predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/advanced-graph-feature/#example-2-hierarchical-feature-processing","title":"Example 2: Hierarchical Feature Processing","text":"<pre><code># Create a hierarchical feature processing model\ndef create_hierarchical_model():\n    inputs = keras.Input(shape=(40,))  # 40 features\n\n    # Hierarchical graph processing\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=32,\n        num_heads=8,\n        dropout_rate=0.1,\n        hierarchical=True,\n        num_groups=8\n    )(inputs)\n\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = AdvancedGraphFeatureLayer(\n        embed_dim=24,\n        num_heads=6,\n        dropout_rate=0.1,\n        hierarchical=True,\n        num_groups=4\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_hierarchical_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/advanced-graph-feature/#example-3-graph-analysis","title":"Example 3: Graph Analysis","text":"<pre><code># Analyze graph behavior\ndef analyze_graph_behavior():\n    # Create model with graph layer\n    inputs = keras.Input(shape=(15,))\n    x = AdvancedGraphFeatureLayer(embed_dim=16, num_heads=4)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Graph Behavior Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze graph behavior\n# model = analyze_graph_behavior()\n</code></pre>"},{"location":"layers/advanced-graph-feature/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Embedding Dimension: Start with 16-32, scale based on data complexity</li> <li>Attention Heads: Use 4-8 heads for most applications</li> <li>Hierarchical Mode: Enable for &gt;20 features or known grouping structure</li> <li>Dropout Rate: Use 0.1-0.2 for regularization</li> <li>Feature Normalization: Works best with normalized input features</li> <li>Memory Usage: Scales quadratically with number of features</li> </ul>"},{"location":"layers/advanced-graph-feature/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Embedding Dimension: Must be divisible by num_heads</li> <li>Hierarchical Mode: Must provide num_groups when hierarchical=True</li> <li>Memory Usage: Can be memory-intensive for large feature sets</li> <li>Overfitting: Monitor for overfitting with complex configurations</li> <li>Feature Count: Consider feature pre-selection for very large feature sets</li> </ul>"},{"location":"layers/advanced-graph-feature/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GraphFeatureAggregation - Graph feature aggregation</li> <li>MultiHeadGraphFeaturePreprocessor - Multi-head graph preprocessing</li> <li>TabularAttention - Tabular attention mechanisms</li> <li>VariableSelection - Variable selection</li> </ul>"},{"location":"layers/advanced-graph-feature/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Graph Neural Networks - Graph neural network concepts</li> <li>Multi-Head Attention - Multi-head attention mechanism</li> <li>Hierarchical Clustering - Hierarchical clustering concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/advanced-numerical-embedding/","title":"\ud83d\udd22 AdvancedNumericalEmbedding\ud83d\udd22 AdvancedNumericalEmbedding","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/advanced-numerical-embedding/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>AdvancedNumericalEmbedding</code> layer embeds continuous numerical features into a higher-dimensional space using a sophisticated dual-branch architecture. It combines continuous processing (via MLP) with discrete processing (via learnable binning and embedding lookup) to create rich feature representations.</p> <p>This layer is particularly powerful for tabular data where numerical features need sophisticated representation learning, combining the benefits of both continuous and discrete processing approaches.</p>"},{"location":"layers/advanced-numerical-embedding/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The AdvancedNumericalEmbedding layer processes numerical features through a dual-branch architecture:</p> <ol> <li>Continuous Branch: Each feature is processed via a small MLP with residual connection</li> <li>Discrete Branch: Features are discretized into learnable bins with embedding lookup</li> <li>Gating Mechanism: A learnable gate combines both branch outputs per feature</li> <li>Residual Connection: Optional batch normalization for training stability</li> <li>Output Generation: Produces rich embeddings combining both approaches</li> </ol> <pre><code>graph TD\n    A[Input Features: batch_size, num_features] --&gt; B[Continuous Branch]\n    A --&gt; C[Discrete Branch]\n\n    B --&gt; D[MLP + ReLU + BatchNorm]\n    D --&gt; E[Continuous Embeddings]\n\n    C --&gt; F[Learnable Binning]\n    F --&gt; G[Embedding Lookup]\n    G --&gt; H[Discrete Embeddings]\n\n    E --&gt; I[Gating Network]\n    H --&gt; I\n    I --&gt; J[Gate Weights]\n\n    E --&gt; K[Weighted Combination]\n    H --&gt; K\n    J --&gt; K\n    K --&gt; L[Final Embeddings]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style L fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/advanced-numerical-embedding/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach AdvancedNumericalEmbedding's Solution Feature Representation Simple dense layers or one-hot encoding \ud83c\udfaf Dual-branch architecture combining continuous and discrete processing Numerical Features Treat all numerical features uniformly \u26a1 Specialized processing for different numerical characteristics Embedding Learning Separate embedding for categorical only \ud83e\udde0 Unified embedding for both continuous and discrete aspects Feature Interactions Limited interaction modeling \ud83d\udd17 Rich interactions through gating and residual connections"},{"location":"layers/advanced-numerical-embedding/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Mixed Data Types: Processing both continuous and discrete numerical features</li> <li>Feature Engineering: Creating rich embeddings for numerical features</li> <li>Representation Learning: Learning sophisticated feature representations</li> <li>Tabular Deep Learning: Advanced preprocessing for tabular neural networks</li> <li>Transfer Learning: Creating reusable feature embeddings</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/advanced-numerical-embedding/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedNumericalEmbedding\n\n# Create sample input data\nbatch_size, num_features = 32, 5\nx = keras.random.normal((batch_size, num_features))\n\n# Apply advanced numerical embedding\nembedding = AdvancedNumericalEmbedding(\n    embedding_dim=8,\n    mlp_hidden_units=16,\n    num_bins=10\n)\nembedded = embedding(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 5)\nprint(f\"Output shape: {embedded.shape}\")   # (32, 5, 8)\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedNumericalEmbedding\n\nmodel = keras.Sequential([\n    AdvancedNumericalEmbedding(\n        embedding_dim=16,\n        mlp_hidden_units=32,\n        num_bins=20\n    ),\n    keras.layers.Flatten(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import AdvancedNumericalEmbedding\n\n# Define inputs\ninputs = keras.Input(shape=(10,))  # 10 numerical features\n\n# Apply advanced embedding\nx = AdvancedNumericalEmbedding(\n    embedding_dim=32,\n    mlp_hidden_units=64,\n    num_bins=15\n)(inputs)\n\n# Flatten and continue processing\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(128, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nembedding = AdvancedNumericalEmbedding(\n    embedding_dim=64,           # Higher embedding dimension\n    mlp_hidden_units=128,       # More hidden units\n    num_bins=50,                # More bins for finer discretization\n    init_min=-5.0,              # Custom initialization range\n    init_max=5.0,\n    dropout_rate=0.2,           # Higher dropout for regularization\n    use_batch_norm=True,        # Enable batch normalization\n    name=\"custom_advanced_embedding\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(20,))\nx = embedding(inputs)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(256, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\nx = keras.layers.Dense(64, activation='relu')(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/advanced-numerical-embedding/#kmr.layers.AdvancedNumericalEmbedding","title":"kmr.layers.AdvancedNumericalEmbedding","text":"<p>This module implements an AdvancedNumericalEmbedding layer that embeds continuous numerical features into a higher-dimensional space using a combination of continuous and discrete branches.</p>"},{"location":"layers/advanced-numerical-embedding/#kmr.layers.AdvancedNumericalEmbedding-classes","title":"Classes","text":""},{"location":"layers/advanced-numerical-embedding/#kmr.layers.AdvancedNumericalEmbedding.AdvancedNumericalEmbedding","title":"AdvancedNumericalEmbedding","text":"<pre><code>AdvancedNumericalEmbedding(\n    embedding_dim: int = 8,\n    mlp_hidden_units: int = 16,\n    num_bins: int = 10,\n    init_min: float | list[float] = -3.0,\n    init_max: float | list[float] = 3.0,\n    dropout_rate: float = 0.1,\n    use_batch_norm: bool = True,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Advanced numerical embedding layer for continuous features.</p> <p>This layer embeds each continuous numerical feature into a higher-dimensional space by combining two branches:</p> <ol> <li>Continuous Branch: Each feature is processed via a small MLP.</li> <li>Discrete Branch: Each feature is discretized into bins using learnable min/max boundaries      and then an embedding is looked up for its bin.</li> </ol> <p>A learnable gate combines the two branch outputs per feature and per embedding dimension. Additionally, the continuous branch uses a residual connection and optional batch normalization to improve training stability.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>Output embedding dimension per feature.</p> <code>8</code> <code>mlp_hidden_units</code> <code>int</code> <p>Hidden units for the continuous branch MLP.</p> <code>16</code> <code>num_bins</code> <code>int</code> <p>Number of bins for discretization.</p> <code>10</code> <code>init_min</code> <code>float or list</code> <p>Initial minimum values for discretization boundaries. If a scalar is provided, it is applied to all features.</p> <code>-3.0</code> <code>init_max</code> <code>float or list</code> <p>Initial maximum values for discretization boundaries.</p> <code>3.0</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied to the continuous branch.</p> <code>0.1</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to apply batch normalization to the continuous branch.</p> <code>True</code> <code>name</code> <code>str</code> <p>Name for the layer.</p> <code>None</code> Input shape <p>Tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>Tensor with shape: <code>(batch_size, num_features, embedding_dim)</code> or <code>(batch_size, embedding_dim)</code> if num_features=1</p> Example <pre><code>import keras\nfrom kmr.layers import AdvancedNumericalEmbedding\n\n# Create sample input data\nx = keras.random.normal((32, 5))  # 32 samples, 5 features\n\n# Create the layer\nembedding = AdvancedNumericalEmbedding(\n    embedding_dim=8,\n    mlp_hidden_units=16,\n    num_bins=10\n)\ny = embedding(x)\nprint(\"Output shape:\", y.shape)  # (32, 5, 8)\n</code></pre> <p>Initialize the AdvancedNumericalEmbedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>8</code> <code>mlp_hidden_units</code> <code>int</code> <p>Hidden units in MLP.</p> <code>16</code> <code>num_bins</code> <code>int</code> <p>Number of bins for discretization.</p> <code>10</code> <code>init_min</code> <code>float | list[float]</code> <p>Minimum initialization value.</p> <code>-3.0</code> <code>init_max</code> <code>float | list[float]</code> <p>Maximum initialization value.</p> <code>3.0</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization.</p> <code>True</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/advanced-numerical-embedding/#kmr.layers.AdvancedNumericalEmbedding.AdvancedNumericalEmbedding-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape of the output tensor.</p>"},{"location":"layers/advanced-numerical-embedding/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/advanced-numerical-embedding/#embedding_dim-int","title":"<code>embedding_dim</code> (int)","text":"<ul> <li>Purpose: Output embedding dimension per feature</li> <li>Range: 4 to 128+ (typically 8-64)</li> <li>Impact: Higher values = richer representations but more parameters</li> <li>Recommendation: Start with 8-16, scale based on data complexity</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#mlp_hidden_units-int","title":"<code>mlp_hidden_units</code> (int)","text":"<ul> <li>Purpose: Hidden units for the continuous branch MLP</li> <li>Range: 8 to 256+ (typically 16-128)</li> <li>Impact: Larger values = more complex continuous processing</li> <li>Recommendation: Start with 16-32, adjust based on feature complexity</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#num_bins-int","title":"<code>num_bins</code> (int)","text":"<ul> <li>Purpose: Number of bins for discretization</li> <li>Range: 5 to 100+ (typically 10-50)</li> <li>Impact: More bins = finer discretization but more parameters</li> <li>Recommendation: Start with 10-20, increase for high-precision features</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#init_min-init_max-float-or-list","title":"<code>init_min</code> / <code>init_max</code> (float or list)","text":"<ul> <li>Purpose: Initial minimum/maximum values for discretization boundaries</li> <li>Range: -10.0 to 10.0 (typically -3.0 to 3.0)</li> <li>Impact: Affects initial bin boundaries and training stability</li> <li>Recommendation: Use -3.0 to 3.0 for normalized data, adjust based on data range</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium feature counts, scales with embedding_dim</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to dual-branch architecture</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex numerical feature processing</li> <li>Best For: Tabular data with numerical features requiring rich representations</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/advanced-numerical-embedding/#example-1-mixed-data-type-processing","title":"Example 1: Mixed Data Type Processing","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import AdvancedNumericalEmbedding\n\n# Simulate mixed numerical data\nbatch_size = 1000\n\n# Continuous features (age, income, etc.)\ncontinuous_features = np.random.normal(0, 1, (batch_size, 5))\n\n# Discrete-like features (counts, ratings, etc.)\ndiscrete_features = np.random.randint(0, 10, (batch_size, 3))\n\n# Combine features\nnumerical_data = np.concatenate([continuous_features, discrete_features], axis=1)\n\n# Build model with advanced embedding\ninputs = keras.Input(shape=(8,))  # 8 numerical features\n\n# Apply advanced numerical embedding\nx = AdvancedNumericalEmbedding(\n    embedding_dim=16,\n    mlp_hidden_units=32,\n    num_bins=20,\n    init_min=-3.0,\n    init_max=3.0\n)(inputs)\n\n# Process embeddings\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#example-2-financial-data-embedding","title":"Example 2: Financial Data Embedding","text":"<pre><code># Process financial data with advanced numerical embedding\ndef create_financial_model():\n    inputs = keras.Input(shape=(15,))  # 15 financial features\n\n    # Advanced numerical embedding\n    x = AdvancedNumericalEmbedding(\n        embedding_dim=32,\n        mlp_hidden_units=64,\n        num_bins=25,\n        init_min=-5.0,\n        init_max=5.0,\n        dropout_rate=0.1\n    )(inputs)\n\n    # Process embeddings\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Multiple outputs\n    risk_score = keras.layers.Dense(1, activation='sigmoid', name='risk')(x)\n    category = keras.layers.Dense(5, activation='softmax', name='category')(x)\n\n    return keras.Model(inputs, [risk_score, category])\n\nmodel = create_financial_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'risk': 'binary_crossentropy', 'category': 'categorical_crossentropy'},\n    loss_weights={'risk': 1.0, 'category': 0.5}\n)\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#example-3-multi-scale-feature-processing","title":"Example 3: Multi-Scale Feature Processing","text":"<pre><code># Process features at different scales with advanced embedding\ndef create_multi_scale_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Different embedding configurations for different feature groups\n    # Group 1: High-precision features (0-5)\n    high_precision = inputs[:, :5]\n    high_precision_emb = AdvancedNumericalEmbedding(\n        embedding_dim=16,\n        mlp_hidden_units=32,\n        num_bins=50,  # More bins for high precision\n        init_min=0.0,\n        init_max=5.0\n    )(high_precision)\n\n    # Group 2: General features (5-15)\n    general_features = inputs[:, 5:15]\n    general_emb = AdvancedNumericalEmbedding(\n        embedding_dim=24,\n        mlp_hidden_units=48,\n        num_bins=20,\n        init_min=-3.0,\n        init_max=3.0\n    )(general_features)\n\n    # Group 3: Categorical-like features (15-20)\n    categorical_like = inputs[:, 15:20]\n    categorical_emb = AdvancedNumericalEmbedding(\n        embedding_dim=12,\n        mlp_hidden_units=24,\n        num_bins=10,\n        init_min=0.0,\n        init_max=10.0\n    )(categorical_like)\n\n    # Combine all embeddings\n    all_embeddings = keras.layers.Concatenate()([\n        keras.layers.Flatten()(high_precision_emb),\n        keras.layers.Flatten()(general_emb),\n        keras.layers.Flatten()(categorical_emb)\n    ])\n\n    # Final processing\n    x = keras.layers.Dense(128, activation='relu')(all_embeddings)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    output = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, output)\n\nmodel = create_multi_scale_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/advanced-numerical-embedding/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Feature Preprocessing: Ensure numerical features are properly normalized</li> <li>Bin Count: Use more bins for high-precision features, fewer for general features</li> <li>Embedding Dimension: Start with 8-16, scale based on data complexity</li> <li>Initialization Range: Set init_min/max based on your data's actual range</li> <li>Batch Normalization: Enable for better training stability</li> <li>Regularization: Use appropriate dropout to prevent overfitting</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, num_features)</li> <li>Memory Usage: Scales with embedding_dim and num_bins</li> <li>Initialization: Poor init_min/max can hurt training - match your data range</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Feature Count: Works best with moderate number of features (5-50)</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DistributionAwareEncoder - Distribution-aware feature encoding</li> <li>DistributionTransformLayer - Distribution transformation</li> <li>GatedFeatureFusion - Feature fusion mechanism</li> <li>TabularAttention - Attention-based feature processing</li> </ul>"},{"location":"layers/advanced-numerical-embedding/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Deep Learning for Tabular Data - Tabular deep learning approaches</li> <li>Feature Embedding in Neural Networks - Feature learning concepts</li> <li>Numerical Feature Processing - Feature engineering techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/boosting-block/","title":"\ud83d\ude80 BoostingBlock\ud83d\ude80 BoostingBlock","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/boosting-block/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>BoostingBlock</code> simulates gradient boosting behavior in a neural network by computing a correction term via a configurable MLP and adding a scaled version to the input. This layer implements a weak learner that can be stacked to mimic the iterative residual-correction process of gradient boosting.</p> <p>This layer is particularly powerful for tabular data where gradient boosting techniques are effective, allowing you to combine the benefits of neural networks with boosting algorithms.</p>"},{"location":"layers/boosting-block/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The BoostingBlock processes data through a boosting-inspired transformation:</p> <ol> <li>MLP Processing: Applies a configurable MLP to the input</li> <li>Correction Computation: Computes a correction term from the MLP output</li> <li>Scaling: Applies a learnable or fixed scaling factor (gamma)</li> <li>Residual Addition: Adds the scaled correction to the original input</li> <li>Output Generation: Produces the boosted output</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[MLP Processing]\n    B --&gt; C[Correction Term]\n    C --&gt; D[Gamma Scaling]\n    D --&gt; E[Scaled Correction]\n    A --&gt; F[Residual Addition]\n    E --&gt; F\n    F --&gt; G[Boosted Output]\n\n    H[Learnable Gamma] --&gt; D\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style F fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/boosting-block/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach BoostingBlock's Solution Gradient Boosting Separate boosting algorithms \ud83c\udfaf Neural network implementation of boosting Residual Learning Manual residual computation \u26a1 Automatic residual correction learning Weak Learners Separate weak learner models \ud83e\udde0 Integrated weak learners in neural networks Ensemble Learning External ensemble methods \ud83d\udd17 End-to-end ensemble learning"},{"location":"layers/boosting-block/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Tabular Data: Combining neural networks with boosting techniques</li> <li>Residual Learning: Learning residual corrections iteratively</li> <li>Ensemble Methods: Building ensemble models in neural networks</li> <li>Gradient Boosting: Implementing boosting algorithms in neural networks</li> <li>Weak Learners: Creating weak learners for ensemble methods</li> </ul>"},{"location":"layers/boosting-block/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/boosting-block/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import BoostingBlock\n\n# Create sample input data\nbatch_size, input_dim = 32, 16\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply boosting block\nboosting_block = BoostingBlock(hidden_units=64)\noutput = boosting_block(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 16)\nprint(f\"Output shape: {output.shape}\")     # (32, 16)\n</code></pre>"},{"location":"layers/boosting-block/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import BoostingBlock\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    BoostingBlock(hidden_units=64),\n    BoostingBlock(hidden_units=32),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/boosting-block/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import BoostingBlock\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply boosting blocks\nx = BoostingBlock(hidden_units=64)(inputs)\nx = BoostingBlock(hidden_units=32)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/boosting-block/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple boosting blocks\ndef create_boosting_network():\n    inputs = keras.Input(shape=(30,))\n\n    # Multiple boosting blocks with different configurations\n    x = BoostingBlock(\n        hidden_units=[64, 32],  # Two hidden layers\n        hidden_activation='selu',\n        dropout_rate=0.1,\n        gamma_trainable=True\n    )(inputs)\n\n    x = BoostingBlock(\n        hidden_units=32,\n        hidden_activation='relu',\n        dropout_rate=0.1,\n        gamma_trainable=True\n    )(x)\n\n    x = BoostingBlock(\n        hidden_units=16,\n        hidden_activation='tanh',\n        dropout_rate=0.05,\n        gamma_trainable=False\n    )(x)\n\n    # Final processing\n    x = keras.layers.Dense(8, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_boosting_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/boosting-block/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/boosting-block/#kmr.layers.BoostingBlock","title":"kmr.layers.BoostingBlock","text":"<p>This module implements a BoostingBlock layer that simulates gradient boosting behavior in a neural network. The layer computes a correction term via a configurable MLP and adds a scaled version to the input.</p>"},{"location":"layers/boosting-block/#kmr.layers.BoostingBlock-classes","title":"Classes","text":""},{"location":"layers/boosting-block/#kmr.layers.BoostingBlock.BoostingBlock","title":"BoostingBlock","text":"<pre><code>BoostingBlock(\n    hidden_units: int | list[int] = 64,\n    hidden_activation: str = \"relu\",\n    output_activation: str | None = None,\n    gamma_trainable: bool = True,\n    gamma_initializer: str\n    | initializers.Initializer = \"ones\",\n    use_bias: bool = True,\n    kernel_initializer: str\n    | initializers.Initializer = \"glorot_uniform\",\n    bias_initializer: str\n    | initializers.Initializer = \"zeros\",\n    dropout_rate: float | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>A neural network layer that simulates gradient boosting behavior.</p> <p>This layer implements a weak learner that computes a correction term via a configurable MLP and adds a scaled version of this correction to the input. Stacking several such blocks can mimic the iterative residual-correction process of gradient boosting.</p> The output is computed as <p>output = inputs + gamma * f(inputs)</p> <p>where:     - f is a configurable MLP (default: two-layer network)     - gamma is a learnable or fixed scaling factor</p> <p>Parameters:</p> Name Type Description Default <code>hidden_units</code> <code>int | list[int]</code> <p>Number of units in the hidden layer(s). Can be an int for single hidden layer or a list of ints for multiple hidden layers. Default is 64.</p> <code>64</code> <code>hidden_activation</code> <code>str</code> <p>Activation function for hidden layers. Default is 'relu'.</p> <code>'relu'</code> <code>output_activation</code> <code>str | None</code> <p>Activation function for the output layer. Default is None.</p> <code>None</code> <code>gamma_trainable</code> <code>bool</code> <p>Whether the scaling factor gamma is trainable. Default is True.</p> <code>True</code> <code>gamma_initializer</code> <code>str | Initializer</code> <p>Initializer for the gamma scaling factor. Default is 'ones'.</p> <code>'ones'</code> <code>use_bias</code> <code>bool</code> <p>Whether to include bias terms in the dense layers. Default is True.</p> <code>True</code> <code>kernel_initializer</code> <code>str | Initializer</code> <p>Initializer for the dense layer kernels. Default is 'glorot_uniform'.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <code>str | Initializer</code> <p>Initializer for the dense layer biases. Default is 'zeros'.</p> <code>'zeros'</code> <code>dropout_rate</code> <code>float | None</code> <p>Optional dropout rate to apply after hidden layers. Default is None.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>N-D tensor with shape: (batch_size, ..., input_dim)</p> Output shape <p>Same shape as input: (batch_size, ..., input_dim)</p> Example <pre><code>import tensorflow as tf\nfrom kmr.layers import BoostingBlock\n\n# Create sample input data\nx = tf.random.normal((32, 16))  # 32 samples, 16 features\n\n# Basic usage\nblock = BoostingBlock(hidden_units=64)\ny = block(x)\nprint(\"Output shape:\", y.shape)  # (32, 16)\n\n# Advanced configuration\nblock = BoostingBlock(\n    hidden_units=[32, 16],  # Two hidden layers\n    hidden_activation='selu',\n    dropout_rate=0.1,\n    gamma_trainable=False\n)\ny = block(x)\n</code></pre> <p>Initialize the BoostingBlock layer.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_units</code> <code>int | list[int]</code> <p>Number of hidden units or list of units per layer.</p> <code>64</code> <code>hidden_activation</code> <code>str</code> <p>Activation function for hidden layers.</p> <code>'relu'</code> <code>output_activation</code> <code>str | None</code> <p>Activation function for output layer.</p> <code>None</code> <code>gamma_trainable</code> <code>bool</code> <p>Whether gamma parameter is trainable.</p> <code>True</code> <code>gamma_initializer</code> <code>str | Initializer</code> <p>Initializer for gamma parameter.</p> <code>'ones'</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias.</p> <code>True</code> <code>kernel_initializer</code> <code>str | Initializer</code> <p>Initializer for kernel weights.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <code>str | Initializer</code> <p>Initializer for bias weights.</p> <code>'zeros'</code> <code>dropout_rate</code> <code>float | None</code> <p>Dropout rate.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/boosting-block/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/boosting-block/#hidden_units-int-or-list","title":"<code>hidden_units</code> (int or list)","text":"<ul> <li>Purpose: Number of hidden units in the MLP</li> <li>Range: 8 to 256+ (typically 32-128)</li> <li>Impact: Larger values = more complex corrections</li> <li>Recommendation: Start with 64, scale based on data complexity</li> </ul>"},{"location":"layers/boosting-block/#hidden_activation-str","title":"<code>hidden_activation</code> (str)","text":"<ul> <li>Purpose: Activation function for hidden layers</li> <li>Options: 'relu', 'selu', 'tanh', 'sigmoid', etc.</li> <li>Default: 'relu'</li> <li>Impact: Affects the correction term computation</li> <li>Recommendation: Use 'relu' for most cases, 'selu' for deeper networks</li> </ul>"},{"location":"layers/boosting-block/#gamma_trainable-bool","title":"<code>gamma_trainable</code> (bool)","text":"<ul> <li>Purpose: Whether the scaling factor is trainable</li> <li>Default: True</li> <li>Impact: Trainable gamma allows learning optimal scaling</li> <li>Recommendation: Use True for most cases, False for fixed scaling</li> </ul>"},{"location":"layers/boosting-block/#dropout_rate-float-optional","title":"<code>dropout_rate</code> (float, optional)","text":"<ul> <li>Purpose: Dropout rate for regularization</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Use 0.1-0.2 for regularization</li> </ul>"},{"location":"layers/boosting-block/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast - simple MLP computation</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Moderate memory usage due to MLP</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for residual learning</li> <li>Best For: Tabular data where boosting techniques are effective</li> </ul>"},{"location":"layers/boosting-block/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/boosting-block/#example-1-gradient-boosting-simulation","title":"Example 1: Gradient Boosting Simulation","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import BoostingBlock\n\n# Create a gradient boosting simulation\ndef create_gradient_boosting_simulation():\n    inputs = keras.Input(shape=(25,))\n\n    # Multiple boosting blocks to simulate gradient boosting\n    x = BoostingBlock(hidden_units=64, gamma_trainable=True)(inputs)\n    x = BoostingBlock(hidden_units=64, gamma_trainable=True)(x)\n    x = BoostingBlock(hidden_units=32, gamma_trainable=True)(x)\n    x = BoostingBlock(hidden_units=32, gamma_trainable=True)(x)\n    x = BoostingBlock(hidden_units=16, gamma_trainable=True)(x)\n\n    # Final processing\n    x = keras.layers.Dense(8, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_gradient_boosting_simulation()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Gradient boosting simulation predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/boosting-block/#example-2-residual-learning-analysis","title":"Example 2: Residual Learning Analysis","text":"<pre><code># Analyze residual learning in boosting blocks\ndef analyze_residual_learning():\n    # Create model with boosting blocks\n    inputs = keras.Input(shape=(15,))\n    x = BoostingBlock(hidden_units=32, gamma_trainable=True)(inputs)\n    x = BoostingBlock(hidden_units=16, gamma_trainable=True)(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Residual Learning Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze residual learning\n# model = analyze_residual_learning()\n</code></pre>"},{"location":"layers/boosting-block/#example-3-boosting-block-comparison","title":"Example 3: Boosting Block Comparison","text":"<pre><code># Compare different boosting block configurations\ndef compare_boosting_configurations():\n    inputs = keras.Input(shape=(20,))\n\n    # Configuration 1: Single hidden layer\n    x1 = BoostingBlock(hidden_units=64, gamma_trainable=True)(inputs)\n    x1 = keras.layers.Dense(1, activation='sigmoid')(x1)\n    model1 = keras.Model(inputs, x1)\n\n    # Configuration 2: Multiple hidden layers\n    x2 = BoostingBlock(hidden_units=[64, 32], gamma_trainable=True)(inputs)\n    x2 = keras.layers.Dense(1, activation='sigmoid')(x2)\n    model2 = keras.Model(inputs, x2)\n\n    # Configuration 3: Fixed gamma\n    x3 = BoostingBlock(hidden_units=64, gamma_trainable=False)(inputs)\n    x3 = keras.layers.Dense(1, activation='sigmoid')(x3)\n    model3 = keras.Model(inputs, x3)\n\n    # Test with sample data\n    test_data = keras.random.normal((50, 20))\n\n    print(\"Boosting Block Comparison:\")\n    print(\"=\" * 40)\n    print(f\"Single hidden layer: {model1.count_params()} parameters\")\n    print(f\"Multiple hidden layers: {model2.count_params()} parameters\")\n    print(f\"Fixed gamma: {model3.count_params()} parameters\")\n\n    return model1, model2, model3\n\n# Compare configurations\n# models = compare_boosting_configurations()\n</code></pre>"},{"location":"layers/boosting-block/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Hidden Units: Start with 64 units, scale based on data complexity</li> <li>Gamma Training: Use trainable gamma for most applications</li> <li>Activation Functions: Use 'relu' for most cases, 'selu' for deeper networks</li> <li>Dropout: Use 0.1-0.2 dropout rate for regularization</li> <li>Stacking: Stack multiple boosting blocks for better performance</li> <li>Residual Learning: The layer automatically handles residual learning</li> </ul>"},{"location":"layers/boosting-block/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Hidden Units: Must be positive integer or list of positive integers</li> <li>Gamma Training: Fixed gamma may limit learning capacity</li> <li>Overfitting: Monitor for overfitting with complex configurations</li> <li>Memory Usage: Scales with hidden units and number of layers</li> <li>Gradient Flow: Residual connections help but monitor training</li> </ul>"},{"location":"layers/boosting-block/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>BoostingEnsembleLayer - Ensemble of boosting blocks</li> <li>GatedResidualNetwork - Gated residual networks</li> <li>StochasticDepth - Stochastic depth regularization</li> <li>VariableSelection - Variable selection with GRN</li> </ul>"},{"location":"layers/boosting-block/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Gradient Boosting - Gradient boosting concepts</li> <li>Residual Learning - Residual learning paper</li> <li>Ensemble Methods - Ensemble learning concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/boosting-ensemble-layer/","title":"\ud83c\udfaf BoostingEnsembleLayer\ud83c\udfaf BoostingEnsembleLayer","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/boosting-ensemble-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>BoostingEnsembleLayer</code> aggregates multiple BoostingBlocks in parallel, combining their outputs via learnable weights to form an ensemble prediction. This layer implements ensemble learning in a differentiable, end-to-end manner, allowing multiple weak learners to work together.</p> <p>This layer is particularly powerful for tabular data where ensemble methods are effective, providing a neural network implementation of boosting ensemble techniques.</p>"},{"location":"layers/boosting-ensemble-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The BoostingEnsembleLayer processes data through parallel boosting blocks:</p> <ol> <li>Parallel Processing: Creates multiple boosting blocks that process input independently</li> <li>Correction Computation: Each block computes its own correction term</li> <li>Gating Mechanism: Learns weights for combining block outputs</li> <li>Weighted Aggregation: Combines block outputs using learned weights</li> <li>Output Generation: Produces ensemble prediction</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B1[Boosting Block 1]\n    A --&gt; B2[Boosting Block 2]\n    A --&gt; B3[Boosting Block N]\n\n    B1 --&gt; C1[Correction 1]\n    B2 --&gt; C2[Correction 2]\n    B3 --&gt; C3[Correction N]\n\n    C1 --&gt; D[Gating Mechanism]\n    C2 --&gt; D\n    C3 --&gt; D\n\n    D --&gt; E[Learnable Weights]\n    E --&gt; F[Weighted Aggregation]\n    F --&gt; G[Ensemble Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B1 fill:#fff9e6,stroke:#ffb74d\n    style B2 fill:#fff9e6,stroke:#ffb74d\n    style B3 fill:#fff9e6,stroke:#ffb74d\n    style D fill:#f3e5f5,stroke:#9c27b0\n    style F fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/boosting-ensemble-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach BoostingEnsembleLayer's Solution Ensemble Learning Separate ensemble models \ud83c\udfaf Integrated ensemble in neural networks Parallel Processing Sequential boosting \u26a1 Parallel boosting blocks Weight Learning Fixed ensemble weights \ud83e\udde0 Learnable weights for optimal combination End-to-End Learning Separate training phases \ud83d\udd17 End-to-end ensemble learning"},{"location":"layers/boosting-ensemble-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Ensemble Learning: Building ensemble models in neural networks</li> <li>Parallel Boosting: Implementing parallel boosting techniques</li> <li>Weak Learner Combination: Combining multiple weak learners</li> <li>Tabular Data: Effective for tabular data ensemble methods</li> <li>Robust Predictions: Creating more robust predictions through ensemble</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/boosting-ensemble-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import BoostingEnsembleLayer\n\n# Create sample input data\nbatch_size, input_dim = 32, 16\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply boosting ensemble\nensemble = BoostingEnsembleLayer(num_learners=3, learner_units=64)\noutput = ensemble(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 16)\nprint(f\"Output shape: {output.shape}\")     # (32, 16)\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import BoostingEnsembleLayer\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    BoostingEnsembleLayer(num_learners=3, learner_units=64),\n    keras.layers.Dense(16, activation='relu'),\n    BoostingEnsembleLayer(num_learners=2, learner_units=32),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import BoostingEnsembleLayer\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply boosting ensemble\nx = BoostingEnsembleLayer(num_learners=4, learner_units=64)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = BoostingEnsembleLayer(num_learners=2, learner_units=32)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple ensemble layers\ndef create_ensemble_network():\n    inputs = keras.Input(shape=(30,))\n\n    # Multiple ensemble layers with different configurations\n    x = BoostingEnsembleLayer(\n        num_learners=5,\n        learner_units=[64, 32],  # Two hidden layers in each learner\n        hidden_activation='selu',\n        dropout_rate=0.1\n    )(inputs)\n\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = BoostingEnsembleLayer(\n        num_learners=3,\n        learner_units=32,\n        hidden_activation='relu',\n        dropout_rate=0.1\n    )(x)\n\n    x = keras.layers.Dense(24, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_ensemble_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/boosting-ensemble-layer/#kmr.layers.BoostingEnsembleLayer","title":"kmr.layers.BoostingEnsembleLayer","text":"<p>This module implements a BoostingEnsembleLayer that aggregates multiple BoostingBlocks in parallel. Their outputs are combined via learnable weights to form an ensemble prediction. This is similar in spirit to boosting ensembles but implemented in a differentiable, end-to-end manner.</p>"},{"location":"layers/boosting-ensemble-layer/#kmr.layers.BoostingEnsembleLayer-classes","title":"Classes","text":""},{"location":"layers/boosting-ensemble-layer/#kmr.layers.BoostingEnsembleLayer.BoostingEnsembleLayer","title":"BoostingEnsembleLayer","text":"<pre><code>BoostingEnsembleLayer(\n    num_learners: int = 3,\n    learner_units: int | list[int] = 64,\n    hidden_activation: str = \"relu\",\n    output_activation: str | None = None,\n    gamma_trainable: bool = True,\n    dropout_rate: float | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Ensemble layer of boosting blocks for tabular data.</p> <p>This layer aggregates multiple boosting blocks (weak learners) in parallel. Each learner produces a correction to the input. A gating mechanism (via learnable weights) then computes a weighted sum of the learners' outputs.</p> <p>Parameters:</p> Name Type Description Default <code>num_learners</code> <code>int</code> <p>Number of boosting blocks in the ensemble. Default is 3.</p> <code>3</code> <code>learner_units</code> <code>int | list[int]</code> <p>Number of hidden units in each boosting block. Can be an int for single hidden layer or a list of ints for multiple hidden layers. Default is 64.</p> <code>64</code> <code>hidden_activation</code> <code>str</code> <p>Activation function for hidden layers in boosting blocks. Default is 'relu'.</p> <code>'relu'</code> <code>output_activation</code> <code>str | None</code> <p>Activation function for the output layer in boosting blocks. Default is None.</p> <code>None</code> <code>gamma_trainable</code> <code>bool</code> <p>Whether the scaling factor gamma in boosting blocks is trainable. Default is True.</p> <code>True</code> <code>dropout_rate</code> <code>float | None</code> <p>Optional dropout rate to apply in boosting blocks. Default is None.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>N-D tensor with shape: (batch_size, ..., input_dim)</p> Output shape <p>Same shape as input: (batch_size, ..., input_dim)</p> Example <pre><code>import keras\nfrom kmr.layers import BoostingEnsembleLayer\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\n\n# Basic usage\nensemble = BoostingEnsembleLayer(num_learners=3, learner_units=64)\ny = ensemble(x)\nprint(\"Ensemble output shape:\", y.shape)  # (32, 16)\n\n# Advanced configuration\nensemble = BoostingEnsembleLayer(\n    num_learners=5,\n    learner_units=[32, 16],  # Two hidden layers in each learner\n    hidden_activation='selu',\n    dropout_rate=0.1\n)\ny = ensemble(x)\n</code></pre> <p>Initialize the BoostingEnsembleLayer.</p> <p>Parameters:</p> Name Type Description Default <code>num_learners</code> <code>int</code> <p>Number of boosting learners.</p> <code>3</code> <code>learner_units</code> <code>int | list[int]</code> <p>Number of units per learner or list of units.</p> <code>64</code> <code>hidden_activation</code> <code>str</code> <p>Activation function for hidden layers.</p> <code>'relu'</code> <code>output_activation</code> <code>str | None</code> <p>Activation function for output layer.</p> <code>None</code> <code>gamma_trainable</code> <code>bool</code> <p>Whether gamma parameter is trainable.</p> <code>True</code> <code>dropout_rate</code> <code>float | None</code> <p>Dropout rate.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/boosting-ensemble-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/boosting-ensemble-layer/#num_learners-int","title":"<code>num_learners</code> (int)","text":"<ul> <li>Purpose: Number of boosting blocks in the ensemble</li> <li>Range: 2 to 20+ (typically 3-8)</li> <li>Impact: More learners = more ensemble diversity but more parameters</li> <li>Recommendation: Start with 3-5, scale based on data complexity</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#learner_units-int-or-list","title":"<code>learner_units</code> (int or list)","text":"<ul> <li>Purpose: Number of hidden units in each boosting block</li> <li>Range: 16 to 256+ (typically 32-128)</li> <li>Impact: Larger values = more complex individual learners</li> <li>Recommendation: Start with 64, scale based on data complexity</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#hidden_activation-str","title":"<code>hidden_activation</code> (str)","text":"<ul> <li>Purpose: Activation function for hidden layers in boosting blocks</li> <li>Options: 'relu', 'selu', 'tanh', 'sigmoid', etc.</li> <li>Default: 'relu'</li> <li>Impact: Affects individual learner behavior</li> <li>Recommendation: Use 'relu' for most cases, 'selu' for deeper networks</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#dropout_rate-float-optional","title":"<code>dropout_rate</code> (float, optional)","text":"<ul> <li>Purpose: Dropout rate for regularization in boosting blocks</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Use 0.1-0.2 for regularization</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium ensembles, scales with learners</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multiple learners</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for ensemble learning</li> <li>Best For: Tabular data where ensemble methods are effective</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/boosting-ensemble-layer/#example-1-ensemble-learning","title":"Example 1: Ensemble Learning","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import BoostingEnsembleLayer\n\n# Create an ensemble learning model\ndef create_ensemble_learning_model():\n    inputs = keras.Input(shape=(25,))\n\n    # Multiple ensemble layers\n    x = BoostingEnsembleLayer(\n        num_learners=6,\n        learner_units=64,\n        hidden_activation='relu',\n        dropout_rate=0.1\n    )(inputs)\n\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = BoostingEnsembleLayer(\n        num_learners=4,\n        learner_units=32,\n        hidden_activation='relu',\n        dropout_rate=0.1\n    )(x)\n\n    x = keras.layers.Dense(24, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_ensemble_learning_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Ensemble learning predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#example-2-ensemble-analysis","title":"Example 2: Ensemble Analysis","text":"<pre><code># Analyze ensemble behavior\ndef analyze_ensemble_behavior():\n    # Create model with ensemble\n    inputs = keras.Input(shape=(15,))\n    x = BoostingEnsembleLayer(num_learners=4, learner_units=32)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Ensemble Behavior Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze ensemble behavior\n# model = analyze_ensemble_behavior()\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#example-3-ensemble-comparison","title":"Example 3: Ensemble Comparison","text":"<pre><code># Compare different ensemble configurations\ndef compare_ensemble_configurations():\n    inputs = keras.Input(shape=(20,))\n\n    # Configuration 1: Few learners, large units\n    x1 = BoostingEnsembleLayer(num_learners=3, learner_units=64)(inputs)\n    x1 = keras.layers.Dense(1, activation='sigmoid')(x1)\n    model1 = keras.Model(inputs, x1)\n\n    # Configuration 2: Many learners, small units\n    x2 = BoostingEnsembleLayer(num_learners=8, learner_units=32)(inputs)\n    x2 = keras.layers.Dense(1, activation='sigmoid')(x2)\n    model2 = keras.Model(inputs, x2)\n\n    # Configuration 3: Balanced configuration\n    x3 = BoostingEnsembleLayer(num_learners=5, learner_units=48)(inputs)\n    x3 = keras.layers.Dense(1, activation='sigmoid')(x3)\n    model3 = keras.Model(inputs, x3)\n\n    # Test with sample data\n    test_data = keras.random.normal((50, 20))\n\n    print(\"Ensemble Configuration Comparison:\")\n    print(\"=\" * 50)\n    print(f\"Few learners, large units: {model1.count_params()} parameters\")\n    print(f\"Many learners, small units: {model2.count_params()} parameters\")\n    print(f\"Balanced configuration: {model3.count_params()} parameters\")\n\n    return model1, model2, model3\n\n# Compare configurations\n# models = compare_ensemble_configurations()\n</code></pre>"},{"location":"layers/boosting-ensemble-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Number of Learners: Start with 3-5 learners, scale based on data complexity</li> <li>Learner Units: Use 32-64 units per learner for most applications</li> <li>Activation Functions: Use 'relu' for most cases, 'selu' for deeper networks</li> <li>Dropout: Use 0.1-0.2 dropout rate for regularization</li> <li>Ensemble Diversity: Different learners will specialize in different patterns</li> <li>Weight Learning: The layer automatically learns optimal combination weights</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Number of Learners: Must be positive integer</li> <li>Learner Units: Must be positive integer or list of positive integers</li> <li>Memory Usage: Scales with number of learners and units</li> <li>Overfitting: Can overfit with too many learners on small datasets</li> <li>Learner Utilization: Some learners may not be used effectively</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>BoostingBlock - Individual boosting block</li> <li>SparseAttentionWeighting - Sparse attention weighting</li> <li>TabularMoELayer - Mixture of experts</li> <li>VariableSelection - Variable selection</li> </ul>"},{"location":"layers/boosting-ensemble-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Ensemble Learning - Ensemble learning concepts</li> <li>Boosting Methods - Boosting techniques</li> <li>Parallel Processing - Parallel processing concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/business-rules-layer/","title":"\ud83d\udccb BusinessRulesLayer\ud83d\udccb BusinessRulesLayer","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/business-rules-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>BusinessRulesLayer</code> applies configurable business rules to neural network outputs, enabling the combination of learned patterns with explicit domain knowledge. This layer is particularly useful for anomaly detection and data validation where business rules can provide additional constraints.</p> <p>This layer supports both numerical and categorical features with various comparison operators, making it flexible for different types of business rule validation.</p>"},{"location":"layers/business-rules-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The BusinessRulesLayer processes data through configurable business rules:</p> <ol> <li>Rule Definition: Defines business rules for numerical or categorical features</li> <li>Rule Evaluation: Evaluates each rule against the input data</li> <li>Anomaly Detection: Identifies data that violates business rules</li> <li>Weight Learning: Optionally learns weights for soft rule enforcement</li> <li>Output Generation: Produces anomaly detection results</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Rule Evaluation]\n    B --&gt; C[Numerical Rules]\n    B --&gt; D[Categorical Rules]\n\n    C --&gt; E[Comparison Operators: &gt;, &lt;]\n    D --&gt; F[Set Operators: ==, in, !=, not in]\n\n    E --&gt; G[Rule Violations]\n    F --&gt; G\n    G --&gt; H[Anomaly Detection]\n    H --&gt; I[Business Anomaly Output]\n\n    J[Learnable Weights] --&gt; G\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style I fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#f3e5f5,stroke:#9c27b0\n    style G fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/business-rules-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach BusinessRulesLayer's Solution Domain Knowledge Separate rule validation \ud83c\udfaf Integrated business rules in neural networks Anomaly Detection Statistical methods only \u26a1 Rule-based anomaly detection Data Validation Manual validation \ud83e\udde0 Automatic validation with business rules Interpretability Black box models \ud83d\udd17 Interpretable rule-based validation"},{"location":"layers/business-rules-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Anomaly Detection: Detecting data that violates business rules</li> <li>Data Validation: Validating data against business constraints</li> <li>Domain Knowledge: Incorporating domain expertise into models</li> <li>Quality Control: Ensuring data quality with business rules</li> <li>Compliance: Enforcing business compliance rules</li> </ul>"},{"location":"layers/business-rules-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/business-rules-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import BusinessRulesLayer\n\n# Create sample input data\nbatch_size, input_dim = 32, 1\nx = keras.random.normal((batch_size, input_dim)) * 50  # Values around 0-50\n\n# Apply business rules for numerical data\nrules_layer = BusinessRulesLayer(\n    rules=[(\"&gt;\", 0), (\"&lt;\", 100)],  # Values must be between 0 and 100\n    feature_type=\"numerical\"\n)\noutput = rules_layer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 1)\nprint(f\"Output keys: {output.keys()}\")     # ['business_anomaly']\nprint(f\"Anomaly shape: {output['business_anomaly'].shape}\")  # (32, 1)\n</code></pre>"},{"location":"layers/business-rules-layer/#categorical-rules","title":"Categorical Rules","text":"<pre><code>import keras\nfrom kmr.layers import BusinessRulesLayer\n\n# Create sample categorical data\ncategorical_data = keras.ops.convert_to_tensor([\n    [\"red\"], [\"green\"], [\"blue\"], [\"yellow\"]\n])\n\n# Apply business rules for categorical data\ncategorical_rules = BusinessRulesLayer(\n    rules=[(\"in\", [\"red\", \"green\", \"blue\"])],  # Only allow red, green, blue\n    feature_type=\"categorical\"\n)\noutput = categorical_rules(categorical_data)\n\nprint(f\"Anomaly detection: {output['business_anomaly']}\")\n# Output: [[False], [False], [False], [True]]  # yellow is anomalous\n</code></pre>"},{"location":"layers/business-rules-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import BusinessRulesLayer\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid'),\n    BusinessRulesLayer(\n        rules=[(\"&gt;\", 0), (\"&lt;\", 1)],  # Ensure output is between 0 and 1\n        feature_type=\"numerical\"\n    )\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/business-rules-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import BusinessRulesLayer\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Process features\nx = keras.layers.Dense(32, activation='relu')(inputs)\nx = keras.layers.Dense(16, activation='relu')(x)\nx = keras.layers.Dense(1, activation='sigmoid')(x)\n\n# Apply business rules\nrules_output = BusinessRulesLayer(\n    rules=[(\"&gt;\", 0), (\"&lt;\", 1)],\n    feature_type=\"numerical\"\n)(x)\n\n# Combine with original output\ncombined = keras.layers.Concatenate()([x, rules_output['business_anomaly']])\nfinal_output = keras.layers.Dense(1, activation='sigmoid')(combined)\n\nmodel = keras.Model(inputs, final_output)\n</code></pre>"},{"location":"layers/business-rules-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with trainable weights\ndef create_business_rules_model():\n    inputs = keras.Input(shape=(25,))\n\n    # Process features\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Apply business rules with trainable weights\n    rules_output = BusinessRulesLayer(\n        rules=[(\"&gt;\", 0), (\"&lt;\", 100), (\"!=\", 50)],  # Multiple rules\n        feature_type=\"numerical\",\n        trainable_weights=True,  # Learn rule weights\n        weight_initializer=\"ones\"\n    )(x)\n\n    # Combine with anomaly information\n    anomaly_info = rules_output['business_anomaly']\n    combined = keras.layers.Concatenate()([x, anomaly_info])\n\n    # Final processing\n    x = keras.layers.Dense(8, activation='relu')(combined)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [classification, regression, anomaly])\n\nmodel = create_business_rules_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'classification': 1.0, 'regression': 0.5, 'anomaly': 0.3}\n)\n</code></pre>"},{"location":"layers/business-rules-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/business-rules-layer/#kmr.layers.BusinessRulesLayer","title":"kmr.layers.BusinessRulesLayer","text":"<p>This module implements a BusinessRulesLayer that allows applying configurable business rules to neural network outputs. This enables combining learned patterns with explicit domain knowledge.</p>"},{"location":"layers/business-rules-layer/#kmr.layers.BusinessRulesLayer-classes","title":"Classes","text":""},{"location":"layers/business-rules-layer/#kmr.layers.BusinessRulesLayer.BusinessRulesLayer","title":"BusinessRulesLayer","text":"<pre><code>BusinessRulesLayer(\n    rules: list[Rule],\n    feature_type: str,\n    trainable_weights: bool = True,\n    weight_initializer: str\n    | initializers.Initializer = \"ones\",\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Evaluates business-defined rules for anomaly detection.</p> <p>This layer applies user-defined business rules to detect anomalies. Rules can be defined for both numerical and categorical features.</p> For numerical features <ul> <li>Comparison operators: '&gt;' and '&lt;'</li> <li>Example: [(\"&gt;\", 0), (\"&lt;\", 100)] for range validation</li> </ul> For categorical features <ul> <li>Set operators: '==', 'in', '!=', 'not in'</li> <li>Example: [(\"in\", [\"red\", \"green\", \"blue\"])] for valid categories</li> </ul> <p>Attributes:</p> Name Type Description <code>rules</code> <p>List of rule tuples (operator, value).</p> <code>feature_type</code> <p>Type of feature ('numerical' or 'categorical').</p> Example <pre><code># Numerical rules\nlayer = BusinessRulesLayer(rules=[(\"&gt;\", 0), (\"&lt;\", 100)], feature_type=\"numerical\")\noutputs = layer(tf.constant([[50.0], [-10.0]]))\nprint(outputs['business_anomaly'])  # [[False], [True]]\n\n# Categorical rules\nlayer = BusinessRulesLayer(\n    rules=[(\"in\", [\"red\", \"green\"])],\n    feature_type=\"categorical\"\n)\noutputs = layer(tf.constant([[\"red\"], [\"blue\"]]))\nprint(outputs['business_anomaly'])  # [[False], [True]]\n</code></pre> <p>Initializes the layer.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[Rule]</code> <p>List of rule tuples (operator, value).</p> required <code>feature_type</code> <code>str</code> <p>Type of feature ('numerical' or 'categorical').</p> required <code>trainable_weights</code> <code>bool</code> <p>Whether to use trainable weights for soft rule enforcement. Default is True.</p> <code>True</code> <code>weight_initializer</code> <code>str | Initializer</code> <p>Initializer for rule weights. Default is 'ones'.</p> <code>'ones'</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional layer arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature_type is invalid or rules have invalid operators.</p>"},{"location":"layers/business-rules-layer/#kmr.layers.BusinessRulesLayer.BusinessRulesLayer-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int | None, int]\n) -&gt; dict[str, tuple[int | None, int]]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int | None, int]</code> <p>Input shape tuple.</p> required <p>Returns:</p> Type Description <code>dict[str, tuple[int | None, int]]</code> <p>Dictionary mapping output names to their shapes.</p>"},{"location":"layers/business-rules-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/business-rules-layer/#rules-list","title":"<code>rules</code> (list)","text":"<ul> <li>Purpose: List of business rules to apply</li> <li>Format: List of tuples (operator, value)</li> <li>Examples: [(\"&gt;\", 0), (\"&lt;\", 100)] for numerical, [(\"in\", [\"red\", \"green\"])] for categorical</li> <li>Impact: Defines the business constraints to enforce</li> <li>Recommendation: Define rules based on domain knowledge</li> </ul>"},{"location":"layers/business-rules-layer/#feature_type-str","title":"<code>feature_type</code> (str)","text":"<ul> <li>Purpose: Type of feature being validated</li> <li>Options: \"numerical\" or \"categorical\"</li> <li>Impact: Determines which operators are available</li> <li>Recommendation: Use \"numerical\" for continuous data, \"categorical\" for discrete data</li> </ul>"},{"location":"layers/business-rules-layer/#trainable_weights-bool","title":"<code>trainable_weights</code> (bool)","text":"<ul> <li>Purpose: Whether to use trainable weights for soft rule enforcement</li> <li>Default: True</li> <li>Impact: Allows learning optimal rule weights</li> <li>Recommendation: Use True for most applications</li> </ul>"},{"location":"layers/business-rules-layer/#weight_initializer-str-or-initializer","title":"<code>weight_initializer</code> (str or initializer)","text":"<ul> <li>Purpose: Initializer for rule weights</li> <li>Default: \"ones\"</li> <li>Impact: Affects initial rule importance</li> <li>Recommendation: Use \"ones\" for equal initial importance</li> </ul>"},{"location":"layers/business-rules-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple rule evaluation</li> <li>Memory: \ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for rule-based validation</li> <li>Best For: Data validation and anomaly detection with business rules</li> </ul>"},{"location":"layers/business-rules-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/business-rules-layer/#example-1-financial-data-validation","title":"Example 1: Financial Data Validation","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import BusinessRulesLayer\n\n# Create financial data validation model\ndef create_financial_validation_model():\n    inputs = keras.Input(shape=(10,))  # 10 financial features\n\n    # Process financial features\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    # Apply financial business rules\n    rules_output = BusinessRulesLayer(\n        rules=[(\"&gt;\", 0), (\"&lt;\", 1)],  # Probability must be between 0 and 1\n        feature_type=\"numerical\",\n        trainable_weights=True\n    )(x)\n\n    # Combine with anomaly information\n    anomaly_info = rules_output['business_anomaly']\n    combined = keras.layers.Concatenate()([x, anomaly_info])\n\n    # Final output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(combined)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_financial_validation_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 10))\npredictions = model(sample_data)\nprint(f\"Financial validation predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/business-rules-layer/#example-2-categorical-data-validation","title":"Example 2: Categorical Data Validation","text":"<pre><code># Create categorical data validation model\ndef create_categorical_validation_model():\n    inputs = keras.Input(shape=(5,))  # 5 categorical features\n\n    # Process categorical features\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    # Apply categorical business rules\n    rules_output = BusinessRulesLayer(\n        rules=[(\"&gt;\", 0), (\"&lt;\", 1)],  # Probability must be between 0 and 1\n        feature_type=\"numerical\",\n        trainable_weights=True\n    )(x)\n\n    # Combine with anomaly information\n    anomaly_info = rules_output['business_anomaly']\n    combined = keras.layers.Concatenate()([x, anomaly_info])\n\n    # Final output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(combined)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_categorical_validation_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/business-rules-layer/#example-3-rule-analysis","title":"Example 3: Rule Analysis","text":"<pre><code># Analyze rule violations\ndef analyze_rule_violations():\n    # Create model with business rules\n    inputs = keras.Input(shape=(15,))\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    rules_output = BusinessRulesLayer(\n        rules=[(\"&gt;\", 0), (\"&lt;\", 1)],\n        feature_type=\"numerical\"\n    )(x)\n\n    model = keras.Model(inputs, [x, rules_output['business_anomaly']])\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Rule Violation Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction, anomaly = model(test_input)\n        print(f\"Test {i+1}: Anomaly rate = {keras.ops.mean(anomaly):.4f}\")\n\n    return model\n\n# Analyze rule violations\n# model = analyze_rule_violations()\n</code></pre>"},{"location":"layers/business-rules-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Rule Definition: Define rules based on domain knowledge</li> <li>Feature Types: Use appropriate feature types (numerical vs categorical)</li> <li>Trainable Weights: Use trainable weights for soft rule enforcement</li> <li>Rule Complexity: Start with simple rules, add complexity as needed</li> <li>Validation: Test rules on known good and bad data</li> <li>Integration: Combine with other layers for comprehensive validation</li> </ul>"},{"location":"layers/business-rules-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Feature Types: Must match the actual data type</li> <li>Rule Operators: Use correct operators for each feature type</li> <li>Rule Values: Ensure rule values are appropriate for the data</li> <li>Memory Usage: Rules are evaluated for each sample</li> <li>Gradient Flow: Rules may not be differentiable</li> </ul>"},{"location":"layers/business-rules-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>NumericalAnomalyDetection - Numerical anomaly detection</li> <li>CategoricalAnomalyDetectionLayer - Categorical anomaly detection</li> <li>FeatureCutout - Feature regularization</li> <li>StochasticDepth - Stochastic depth regularization</li> </ul>"},{"location":"layers/business-rules-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Business Rules - Business rule concepts</li> <li>Anomaly Detection - Anomaly detection techniques</li> <li>Data Validation - Data validation concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/cast-to-float32-layer/","title":"\ud83d\udd04 CastToFloat32Layer\ud83d\udd04 CastToFloat32Layer","text":"\ud83d\udfe2 Beginner \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/cast-to-float32-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>CastToFloat32Layer</code> casts input tensors to float32 data type, ensuring consistent data types in a model. This layer is particularly useful when working with mixed precision or when receiving inputs of various data types.</p> <p>This layer is essential for data preprocessing pipelines where data type consistency is crucial for neural network training and inference.</p>"},{"location":"layers/cast-to-float32-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The CastToFloat32Layer processes tensors through simple type casting:</p> <ol> <li>Input Validation: Accepts tensors of any numeric data type</li> <li>Type Casting: Converts input tensor to float32 data type</li> <li>Shape Preservation: Maintains the original tensor shape</li> <li>Output Generation: Produces float32 tensor with same shape</li> </ol> <pre><code>graph TD\n    A[Input Tensor: Any Numeric Type] --&gt; B[Type Casting]\n    B --&gt; C[Convert to float32]\n    C --&gt; D[Output Tensor: float32]\n\n    E[Shape Preservation] --&gt; D\n    F[Data Type Consistency] --&gt; D\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style D fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/cast-to-float32-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach CastToFloat32Layer's Solution Data Type Inconsistency Manual type conversion \ud83c\udfaf Automatic casting to float32 Mixed Precision Complex type handling \u26a1 Simplified type management Model Compatibility Manual type checking \ud83e\udde0 Ensures compatibility with neural networks Data Preprocessing Separate conversion steps \ud83d\udd17 Integrated type casting in pipelines"},{"location":"layers/cast-to-float32-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Data Type Standardization: Ensuring consistent float32 data types</li> <li>Mixed Precision Training: Converting inputs to float32 for training</li> <li>Data Preprocessing: Type casting in preprocessing pipelines</li> <li>Model Compatibility: Ensuring inputs are compatible with neural networks</li> <li>Data Loading: Converting loaded data to appropriate types</li> </ul>"},{"location":"layers/cast-to-float32-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/cast-to-float32-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import CastToFloat32Layer\n\n# Create sample input data with different types\nint_data = keras.ops.convert_to_tensor(np.array([1, 2, 3], dtype=np.int64))\nfloat64_data = keras.ops.convert_to_tensor(np.array([1.0, 2.0, 3.0], dtype=np.float64))\n\n# Apply type casting\ncast_layer = CastToFloat32Layer()\nint_float32 = cast_layer(int_data)\nfloat64_float32 = cast_layer(float64_data)\n\nprint(f\"Input types: {int_data.dtype}, {float64_data.dtype}\")\nprint(f\"Output types: {int_float32.dtype}, {float64_float32.dtype}\")\n# Output: Input types: int64, float64\n#         Output types: float32, float32\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import CastToFloat32Layer\n\nmodel = keras.Sequential([\n    CastToFloat32Layer(),  # Cast to float32 first\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import CastToFloat32Layer\n\n# Define inputs\ninputs = keras.Input(shape=(10,))  # 10 features\n\n# Apply type casting\nx = CastToFloat32Layer()(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom naming\ndef create_typed_model():\n    # Input for mixed data types\n    inputs = keras.Input(shape=(20,))\n\n    # Apply type casting with custom name\n    x = CastToFloat32Layer(name=\"input_type_casting\")(inputs)\n\n    # Process with different branches\n    branch1 = keras.layers.Dense(32, activation='relu')(x)\n    branch1 = keras.layers.Dense(16, activation='relu')(branch1)\n\n    branch2 = keras.layers.Dense(32, activation='tanh')(x)\n    branch2 = keras.layers.Dense(16, activation='tanh')(branch2)\n\n    # Combine branches\n    x = keras.layers.Concatenate()([branch1, branch2])\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_typed_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/cast-to-float32-layer/#kmr.layers.CastToFloat32Layer","title":"kmr.layers.CastToFloat32Layer","text":"<p>This module implements a CastToFloat32Layer that casts input tensors to float32 data type.</p>"},{"location":"layers/cast-to-float32-layer/#kmr.layers.CastToFloat32Layer-classes","title":"Classes","text":""},{"location":"layers/cast-to-float32-layer/#kmr.layers.CastToFloat32Layer.CastToFloat32Layer","title":"CastToFloat32Layer","text":"<pre><code>CastToFloat32Layer(name: str | None = None, **kwargs: Any)\n</code></pre> <p>Layer that casts input tensors to float32 data type.</p> <p>This layer is useful for ensuring consistent data types in a model, especially when working with mixed precision or when receiving inputs of various data types.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>Tensor of any shape and numeric data type.</p> Output shape <p>Same as input shape, but with float32 data type.</p> Example <pre><code>import keras\nimport numpy as np\nfrom kmr.layers import CastToFloat32Layer\n\n# Create sample input data with int64 type\nx = keras.ops.convert_to_tensor(np.array([1, 2, 3], dtype=np.int64))\n\n# Apply casting layer\ncast_layer = CastToFloat32Layer()\ny = cast_layer(x)\n\nprint(y.dtype)  # float32\n</code></pre> <p>Initialize the CastToFloat32Layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/cast-to-float32-layer/#kmr.layers.CastToFloat32Layer.CastToFloat32Layer-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Same shape as input.</p>"},{"location":"layers/cast-to-float32-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/cast-to-float32-layer/#no-parameters","title":"No Parameters","text":"<ul> <li>Purpose: This layer has no configurable parameters</li> <li>Behavior: Automatically casts input to float32</li> <li>Output: Always produces float32 tensor with same shape</li> </ul>"},{"location":"layers/cast-to-float32-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple type casting operation</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Perfect for type conversion</li> <li>Best For: Data type standardization and mixed precision handling</li> </ul>"},{"location":"layers/cast-to-float32-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/cast-to-float32-layer/#example-1-mixed-data-type-handling","title":"Example 1: Mixed Data Type Handling","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import CastToFloat32Layer\n\n# Handle mixed data types in a preprocessing pipeline\ndef create_mixed_type_pipeline():\n    # Input for mixed data types\n    inputs = keras.Input(shape=(15,))\n\n    # Apply type casting\n    x = CastToFloat32Layer()(inputs)\n\n    # Process with different preprocessing\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_mixed_type_pipeline()\n\n# Test with different data types\nint_data = keras.ops.convert_to_tensor(np.random.randint(0, 10, (10, 15)), dtype=\"int32\")\nfloat64_data = keras.ops.convert_to_tensor(np.random.randn(10, 15), dtype=\"float64\")\n\n# Both should work with the model\nint_pred = model(int_data)\nfloat64_pred = model(float64_data)\n\nprint(f\"Int32 input prediction shape: {int_pred.shape}\")\nprint(f\"Float64 input prediction shape: {float64_pred.shape}\")\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#example-2-data-loading-pipeline","title":"Example 2: Data Loading Pipeline","text":"<pre><code># Create a data loading pipeline with type casting\ndef create_data_loading_pipeline():\n    # Input for loaded data\n    inputs = keras.Input(shape=(25,))\n\n    # Apply type casting\n    x = CastToFloat32Layer()(inputs)\n\n    # Data preprocessing\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Feature processing\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(5, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_data_loading_pipeline()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#example-3-type-safety-validation","title":"Example 3: Type Safety Validation","text":"<pre><code># Validate type safety in a model\ndef validate_type_safety():\n    # Create model with type casting\n    inputs = keras.Input(shape=(10,))\n    x = CastToFloat32Layer()(inputs)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different data types\n    test_cases = [\n        (\"int32\", keras.ops.convert_to_tensor(np.random.randint(0, 10, (5, 10)), dtype=\"int32\")),\n        (\"int64\", keras.ops.convert_to_tensor(np.random.randint(0, 10, (5, 10)), dtype=\"int64\")),\n        (\"float32\", keras.ops.convert_to_tensor(np.random.randn(5, 10), dtype=\"float32\")),\n        (\"float64\", keras.ops.convert_to_tensor(np.random.randn(5, 10), dtype=\"float64\")),\n    ]\n\n    print(\"Type Safety Validation:\")\n    print(\"=\" * 40)\n\n    for dtype_name, data in test_cases:\n        try:\n            prediction = model(data)\n            output_dtype = prediction.dtype\n            print(f\"{dtype_name:&gt;8} -&gt; {output_dtype:&gt;8} \u2713\")\n        except Exception as e:\n            print(f\"{dtype_name:&gt;8} -&gt; Error: {e}\")\n\n    return model\n\n# Validate type safety\n# model = validate_type_safety()\n</code></pre>"},{"location":"layers/cast-to-float32-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Input Types: Accepts any numeric data type</li> <li>Output Type: Always produces float32 tensor</li> <li>Shape Preservation: Maintains original tensor shape</li> <li>Performance: Very fast with minimal overhead</li> <li>Integration: Works seamlessly with other Keras layers</li> <li>Memory: No additional memory overhead</li> </ul>"},{"location":"layers/cast-to-float32-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Non-Numeric Types: Doesn't handle string or boolean types</li> <li>Shape Changes: Doesn't change tensor shape, only data type</li> <li>Precision Loss: May lose precision when converting from higher precision types</li> <li>Memory Usage: Creates new tensor, doesn't modify in-place</li> <li>Gradient Flow: Maintains gradient flow through type casting</li> </ul>"},{"location":"layers/cast-to-float32-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> <li>DifferentialPreprocessingLayer - Advanced preprocessing</li> <li>DateParsingLayer - Date string parsing</li> <li>FeatureCutout - Feature regularization</li> </ul>"},{"location":"layers/cast-to-float32-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Data Type Conversion - Type conversion concepts</li> <li>Mixed Precision Training - Mixed precision techniques</li> <li>Neural Network Data Types - Floating point representation</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/","title":"\ud83d\udd0d CategoricalAnomalyDetectionLayer\ud83d\udd0d CategoricalAnomalyDetectionLayer","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/categorical-anomaly-detection-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>CategoricalAnomalyDetectionLayer</code> identifies outliers in categorical features by learning the distribution of categorical values and detecting rare or unusual combinations. It uses embedding-based approaches and frequency analysis to detect anomalies in categorical data.</p> <p>This layer is particularly powerful for identifying outliers in categorical data, providing a specialized approach for non-numerical features that traditional statistical methods may not handle well.</p>"},{"location":"layers/categorical-anomaly-detection-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The CategoricalAnomalyDetectionLayer processes data through categorical anomaly detection:</p> <ol> <li>Categorical Encoding: Encodes categorical features into embeddings</li> <li>Frequency Analysis: Analyzes frequency of categorical values</li> <li>Rarity Detection: Identifies rare or unusual categorical combinations</li> <li>Embedding Learning: Learns embeddings for categorical values</li> <li>Anomaly Scoring: Computes anomaly scores based on rarity and embeddings</li> <li>Output Generation: Produces anomaly scores for each categorical feature</li> </ol> <pre><code>graph TD\n    A[Categorical Features] --&gt; B[Categorical Encoding]\n    B --&gt; C[Frequency Analysis]\n    C --&gt; D[Rarity Detection]\n\n    B --&gt; E[Embedding Learning]\n    E --&gt; F[Embedding Analysis]\n    F --&gt; G[Anomaly Scoring]\n\n    D --&gt; G\n    G --&gt; H[Anomaly Scores]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style H fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style E fill:#e1f5fe,stroke:#03a9f4\n    style G fill:#fff3e0,stroke:#ff9800</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach CategoricalAnomalyDetectionLayer's Solution Categorical Outliers Limited methods \ud83c\udfaf Specialized approach for categorical data Rarity Detection Manual frequency analysis \u26a1 Automatic rarity detection Embedding Learning No embedding learning \ud83e\udde0 Embedding-based anomaly detection Frequency Analysis Static frequency analysis \ud83d\udd17 Dynamic frequency analysis"},{"location":"layers/categorical-anomaly-detection-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Categorical Outlier Detection: Identifying outliers in categorical features</li> <li>Data Quality: Ensuring data quality through categorical anomaly detection</li> <li>Rarity Analysis: Analyzing rare categorical combinations</li> <li>Embedding Learning: Learning embeddings for categorical values</li> <li>Frequency Analysis: Analyzing frequency of categorical values</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/categorical-anomaly-detection-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import CategoricalAnomalyDetectionLayer\n\n# Create sample categorical data\nbatch_size, num_features = 32, 5\nx = keras.ops.convert_to_tensor([\n    [\"red\", \"small\", \"A\", \"high\", \"yes\"],\n    [\"blue\", \"large\", \"B\", \"low\", \"no\"],\n    [\"green\", \"medium\", \"C\", \"medium\", \"yes\"],\n    # ... more samples\n])\n\n# Apply categorical anomaly detection\nanomaly_layer = CategoricalAnomalyDetectionLayer()\nanomaly_scores = anomaly_layer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 5)\nprint(f\"Anomaly scores shape: {anomaly_scores.shape}\")  # (32, 5)\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import CategoricalAnomalyDetectionLayer\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    CategoricalAnomalyDetectionLayer(),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import CategoricalAnomalyDetectionLayer\n\n# Define inputs\ninputs = keras.Input(shape=(10,), dtype='string')  # 10 categorical features\n\n# Apply categorical anomaly detection\nanomaly_scores = CategoricalAnomalyDetectionLayer()(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(inputs)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, [outputs, anomaly_scores])\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple anomaly detection layers\ndef create_categorical_anomaly_network():\n    inputs = keras.Input(shape=(15,), dtype='string')  # 15 categorical features\n\n    # Multiple anomaly detection layers\n    anomaly_scores1 = CategoricalAnomalyDetectionLayer()(inputs)\n\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    anomaly_scores2 = CategoricalAnomalyDetectionLayer()(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [classification, regression, anomaly, anomaly_scores1, anomaly_scores2])\n\nmodel = create_categorical_anomaly_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'classification': 1.0, 'regression': 0.5, 'anomaly': 0.3}\n)\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/categorical-anomaly-detection-layer/#kmr.layers.CategoricalAnomalyDetectionLayer","title":"kmr.layers.CategoricalAnomalyDetectionLayer","text":""},{"location":"layers/categorical-anomaly-detection-layer/#kmr.layers.CategoricalAnomalyDetectionLayer-classes","title":"Classes","text":""},{"location":"layers/categorical-anomaly-detection-layer/#kmr.layers.CategoricalAnomalyDetectionLayer.CategoricalAnomalyDetectionLayer","title":"CategoricalAnomalyDetectionLayer","text":"<pre><code>CategoricalAnomalyDetectionLayer(\n    dtype: str = \"string\", **kwargs\n)\n</code></pre> <p>Backend-agnostic anomaly detection for categorical features.</p> <p>This layer detects anomalies in categorical features by checking if values belong to a predefined set of valid categories. Values not in this set are considered anomalous.</p> <p>The layer uses a Keras StringLookup or IntegerLookup layer internally to efficiently map input values to indices, which are then used to determine if a value is valid.</p> <p>Attributes:</p> Name Type Description <code>dtype</code> <code>Any</code> <p>The data type of input values ('string' or 'int32').</p> <code>lookup</code> <code>StringLookup | IntegerLookup | None</code> <p>A Keras lookup layer for mapping values to indices.</p> <code>vocabulary</code> <code>StringLookup | IntegerLookup | None</code> <p>list of valid categorical values.</p> Example <pre><code>layer = CategoricalAnomalyDetectionLayer(dtype='string')\nlayer.initialize_from_stats(vocabulary=['red', 'green', 'blue'])\noutputs = layer(tf.constant([['red'], ['purple']]))\nprint(outputs['anomaly'])  # [[False], [True]]\n</code></pre> <p>Initializes the layer.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <p>Data type of input values ('string' or 'int32'). Defaults to 'string'.</p> <code>'string'</code> <code>**kwargs</code> <p>Additional layer arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dtype is not 'string' or 'int32'.</p>"},{"location":"layers/categorical-anomaly-detection-layer/#kmr.layers.CategoricalAnomalyDetectionLayer.CategoricalAnomalyDetectionLayer-attributes","title":"Attributes","text":"dtype <code>property</code> <pre><code>dtype: Any\n</code></pre> <p>Get the dtype of the layer.</p>"},{"location":"layers/categorical-anomaly-detection-layer/#kmr.layers.CategoricalAnomalyDetectionLayer.CategoricalAnomalyDetectionLayer-functions","title":"Functions","text":"set_dtype <pre><code>set_dtype(value) -&gt; None\n</code></pre> <p>Set the dtype and initialize the appropriate lookup layer.</p> initialize_from_stats <pre><code>initialize_from_stats(vocabulary: list[str | int]) -&gt; None\n</code></pre> <p>Initializes the layer with a vocabulary of valid values.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>list[str | int]</code> <p>list of valid categorical values.</p> required compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int | None, int]\n) -&gt; dict[str, tuple[int | None, int]]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int | None, int]</code> <p>Input shape tuple.</p> required <p>Returns:</p> Type Description <code>dict[str, tuple[int | None, int]]</code> <p>Dictionary mapping output names to their shapes.</p> from_config <code>classmethod</code> <pre><code>from_config(config) -&gt; Any\n</code></pre> <p>Create layer from configuration.</p>"},{"location":"layers/categorical-anomaly-detection-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/categorical-anomaly-detection-layer/#embedding_dim-int-optional","title":"<code>embedding_dim</code> (int, optional)","text":"<ul> <li>Purpose: Dimension of categorical embeddings</li> <li>Range: 8 to 64+ (typically 16-32)</li> <li>Impact: Larger values = more expressive embeddings but more parameters</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#frequency_threshold-float-optional","title":"<code>frequency_threshold</code> (float, optional)","text":"<ul> <li>Purpose: Threshold for frequency-based anomaly detection</li> <li>Range: 0.0 to 1.0 (typically 0.01-0.1)</li> <li>Impact: Lower values = more sensitive to rare values</li> <li>Recommendation: Use 0.01-0.05 for most applications</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#embedding_weight-float-optional","title":"<code>embedding_weight</code> (float, optional)","text":"<ul> <li>Purpose: Weight for embedding-based anomaly detection</li> <li>Range: 0.0 to 1.0 (typically 0.3-0.7)</li> <li>Impact: Higher values = more emphasis on embedding-based detection</li> <li>Recommendation: Use 0.3-0.7 based on data characteristics</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with embedding dimension</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to embeddings</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for categorical anomaly detection</li> <li>Best For: Categorical data with potential outliers</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/categorical-anomaly-detection-layer/#example-1-categorical-outlier-detection","title":"Example 1: Categorical Outlier Detection","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import CategoricalAnomalyDetectionLayer\n\n# Create a model for categorical outlier detection\ndef create_categorical_outlier_model():\n    inputs = keras.Input(shape=(10,), dtype='string')  # 10 categorical features\n\n    # Anomaly detection layer\n    anomaly_scores = CategoricalAnomalyDetectionLayer()(inputs)\n\n    # Process features\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, [outputs, anomaly_scores])\n\nmodel = create_categorical_outlier_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.ops.convert_to_tensor([\n    [\"red\", \"small\", \"A\", \"high\", \"yes\", \"cat\", \"fast\", \"new\", \"good\", \"up\"],\n    [\"blue\", \"large\", \"B\", \"low\", \"no\", \"dog\", \"slow\", \"old\", \"bad\", \"down\"],\n    # ... more samples\n])\npredictions, anomaly_scores = model(sample_data)\nprint(f\"Categorical outlier predictions shape: {predictions.shape}\")\nprint(f\"Anomaly scores shape: {anomaly_scores.shape}\")\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#example-2-rarity-analysis","title":"Example 2: Rarity Analysis","text":"<pre><code># Analyze rarity in categorical data\ndef analyze_categorical_rarity():\n    # Create model with categorical anomaly detection\n    inputs = keras.Input(shape=(8,), dtype='string')\n    anomaly_scores = CategoricalAnomalyDetectionLayer()(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(inputs)\n\n    model = keras.Model(inputs, [outputs, anomaly_scores])\n\n    # Test with different categorical patterns\n    test_inputs = [\n        keras.ops.convert_to_tensor([[\"common\", \"frequent\", \"usual\", \"normal\", \"typical\", \"standard\", \"regular\", \"ordinary\"]]),\n        keras.ops.convert_to_tensor([[\"rare\", \"unusual\", \"strange\", \"abnormal\", \"atypical\", \"nonstandard\", \"irregular\", \"extraordinary\"]]),\n    ]\n\n    print(\"Categorical Rarity Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction, anomaly = model(test_input)\n        print(f\"Test {i+1}: Anomaly mean = {keras.ops.mean(anomaly):.4f}\")\n\n    return model\n\n# Analyze categorical rarity\n# model = analyze_categorical_rarity()\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#example-3-frequency-analysis","title":"Example 3: Frequency Analysis","text":"<pre><code># Analyze frequency patterns in categorical data\ndef analyze_categorical_frequency():\n    # Create model with categorical anomaly detection\n    inputs = keras.Input(shape=(6,), dtype='string')\n    anomaly_scores = CategoricalAnomalyDetectionLayer()(inputs)\n\n    model = keras.Model(inputs, anomaly_scores)\n\n    # Test with sample data\n    sample_data = keras.ops.convert_to_tensor([\n        [\"red\", \"small\", \"A\", \"high\", \"yes\", \"cat\"],\n        [\"blue\", \"large\", \"B\", \"low\", \"no\", \"dog\"],\n        # ... more samples\n    ])\n    anomaly_scores = model(sample_data)\n\n    print(\"Categorical Frequency Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {sample_data.shape}\")\n    print(f\"Anomaly scores shape: {anomaly_scores.shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze categorical frequency\n# model = analyze_categorical_frequency()\n</code></pre>"},{"location":"layers/categorical-anomaly-detection-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Embedding Dimension: Start with 16-32, scale based on data complexity</li> <li>Frequency Threshold: Use 0.01-0.05 for most applications</li> <li>Embedding Weight: Balance embedding and frequency-based detection</li> <li>Categorical Encoding: Ensure proper categorical encoding</li> <li>Rarity Analysis: Monitor rarity patterns for interpretability</li> <li>Frequency Analysis: Track frequency changes over time</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Embedding Dimension: Must be positive integer</li> <li>Frequency Threshold: Must be between 0 and 1</li> <li>Embedding Weight: Must be between 0 and 1</li> <li>Memory Usage: Scales with embedding dimension and vocabulary size</li> <li>Categorical Encoding: Ensure proper string tensor handling</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>NumericalAnomalyDetection - Numerical anomaly detection</li> <li>BusinessRulesLayer - Business rules validation</li> <li>FeatureCutout - Feature regularization</li> <li>DistributionAwareEncoder - Distribution-aware encoding</li> </ul>"},{"location":"layers/categorical-anomaly-detection-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Categorical Data - Categorical data concepts</li> <li>Anomaly Detection - Anomaly detection techniques</li> <li>Frequency Analysis - Frequency analysis concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/column-attention/","title":"\ud83d\udcca ColumnAttention\ud83d\udcca ColumnAttention","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/column-attention/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>ColumnAttention</code> layer implements a column-wise attention mechanism that dynamically weights features based on their importance and context. Unlike traditional attention mechanisms that focus on sequence relationships, this layer learns to assign attention weights to each feature (column) in tabular data, allowing the model to focus on the most relevant features for each prediction.</p> <p>This layer is particularly useful for feature selection, interpretability, and improving model performance by learning which features are most important for each sample.</p>"},{"location":"layers/column-attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The ColumnAttention layer processes tabular data through a feature-wise attention mechanism:</p> <ol> <li>Feature Analysis: Analyzes all input features to understand their importance</li> <li>Attention Weight Generation: Uses a neural network to compute attention weights for each feature</li> <li>Dynamic Weighting: Applies learned weights to scale feature importance</li> <li>Weighted Output: Returns the input features scaled by their attention weights</li> </ol> <pre><code>graph TD\n    A[Input: batch_size, num_features] --&gt; B[Feature Analysis]\n    B --&gt; C[Attention Network]\n    C --&gt; D[Softmax Activation]\n    D --&gt; E[Attention Weights]\n    A --&gt; F[Element-wise Multiplication]\n    E --&gt; F\n    F --&gt; G[Weighted Features Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style C fill:#fff9e6,stroke:#ffb74d\n    style E fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/column-attention/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach ColumnAttention's Solution Feature Importance Manual feature selection or uniform treatment \ud83c\udfaf Automatic learning of feature importance per sample Dynamic Weighting Static feature weights or simple normalization \u26a1 Context-aware feature weighting based on input Interpretability Black-box feature processing \ud83d\udc41\ufe0f Transparent attention weights show feature importance Noise Reduction All features treated equally \ud83d\udd07 Automatic filtering of less important features"},{"location":"layers/column-attention/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Feature Selection: Automatically identifying and emphasizing important features</li> <li>Noise Reduction: Down-weighting irrelevant or noisy features</li> <li>Interpretability: Understanding which features drive predictions</li> <li>Data Quality: Handling datasets with varying feature importance</li> <li>Model Regularization: Preventing overfitting by focusing on important features</li> </ul>"},{"location":"layers/column-attention/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/column-attention/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import ColumnAttention\n\n# Create sample tabular data\nbatch_size, num_features = 32, 10\nx = keras.random.normal((batch_size, num_features))\n\n# Apply column attention\nattention = ColumnAttention(input_dim=num_features)\nweighted_features = attention(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {weighted_features.shape}\")  # (32, 10)\n</code></pre>"},{"location":"layers/column-attention/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import ColumnAttention\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    ColumnAttention(input_dim=64),  # Apply attention to 64 features\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/column-attention/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import ColumnAttention\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Process features\nx = keras.layers.Dense(64, activation='relu')(inputs)\nx = ColumnAttention(input_dim=64)(x)  # Apply column attention\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/column-attention/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom hidden dimension\nattention = ColumnAttention(\n    input_dim=128,\n    hidden_dim=64,  # Custom hidden layer size\n    name=\"custom_column_attention\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50,))\nx = keras.layers.Dense(128, activation='relu')(inputs)\nx = attention(x)  # Apply column attention\nx = keras.layers.LayerNormalization()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/column-attention/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/column-attention/#kmr.layers.ColumnAttention","title":"kmr.layers.ColumnAttention","text":"<p>Column attention mechanism for weighting features dynamically.</p>"},{"location":"layers/column-attention/#kmr.layers.ColumnAttention-classes","title":"Classes","text":""},{"location":"layers/column-attention/#kmr.layers.ColumnAttention.ColumnAttention","title":"ColumnAttention","text":"<pre><code>ColumnAttention(\n    input_dim: int,\n    hidden_dim: int | None = None,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Column attention mechanism to weight features dynamically.</p> <p>This layer applies attention weights to each feature (column) in the input tensor. The attention weights are computed using a two-layer neural network that takes the input features and outputs attention weights for each feature.</p> Example <pre><code>import tensorflow as tf\nfrom kmr.layers import ColumnAttention\n\n# Create sample data\nbatch_size = 32\ninput_dim = 10\ninputs = tf.random.normal((batch_size, input_dim))\n\n# Apply column attention\nattention = ColumnAttention(input_dim=input_dim)\nweighted_outputs = attention(inputs)\n</code></pre> <p>Initialize column attention.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension</p> required <code>hidden_dim</code> <code>int | None</code> <p>Hidden layer dimension. If None, uses input_dim // 2</p> <code>None</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments</p> <code>{}</code>"},{"location":"layers/column-attention/#kmr.layers.ColumnAttention.ColumnAttention-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; ColumnAttention\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>ColumnAttention</code> <p>ColumnAttention instance</p>"},{"location":"layers/column-attention/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/column-attention/#input_dim-int","title":"<code>input_dim</code> (int)","text":"<ul> <li>Purpose: Number of input features to apply attention to</li> <li>Range: 1 to 1000+ (typically 10-100)</li> <li>Impact: Must match the number of features in your input</li> <li>Recommendation: Set to the output dimension of your previous layer</li> </ul>"},{"location":"layers/column-attention/#hidden_dim-int-optional","title":"<code>hidden_dim</code> (int, optional)","text":"<ul> <li>Purpose: Size of the hidden layer in the attention network</li> <li>Range: 1 to input_dim (default: input_dim // 2)</li> <li>Impact: Larger values = more complex attention patterns but more parameters</li> <li>Recommendation: Start with default, increase for complex feature interactions</li> </ul>"},{"location":"layers/column-attention/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple neural network computation</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Good for feature importance and noise reduction</li> <li>Best For: Tabular data where feature importance varies by sample</li> </ul>"},{"location":"layers/column-attention/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/column-attention/#example-1-feature-importance-analysis","title":"Example 1: Feature Importance Analysis","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import ColumnAttention\n\n# Create data with varying feature importance\nnp.random.seed(42)\nbatch_size, num_features = 100, 8\n\n# Features 0, 2, 5 are important, others are noise\nimportant_features = np.random.normal(0, 1, (batch_size, 3))\nnoise_features = np.random.normal(0, 0.1, (batch_size, 5))\nx = np.concatenate([important_features[:, [0]], noise_features[:, [0]], \n                   important_features[:, [1]], noise_features[:, [1]], \n                   noise_features[:, [2]], important_features[:, [2]], \n                   noise_features[:, [3]], noise_features[:, [4]]], axis=1)\n\n# Build model with column attention\ninputs = keras.Input(shape=(num_features,))\nx = keras.layers.Dense(16, activation='relu')(inputs)\nx = ColumnAttention(input_dim=16)(x)  # Learn feature importance\nx = keras.layers.Dense(8, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train and analyze attention weights\nmodel.fit(x, np.random.randint(0, 2, (batch_size, 1)), epochs=10, verbose=0)\n\n# Get attention weights for interpretability\nattention_layer = model.layers[2]  # ColumnAttention layer\nattention_weights = attention_layer.attention_net(x[:5])  # Get weights for first 5 samples\nprint(\"Attention weights shape:\", attention_weights.shape)\nprint(\"Sample attention weights:\", attention_weights[0])\n</code></pre>"},{"location":"layers/column-attention/#example-2-multi-task-learning-with-feature-attention","title":"Example 2: Multi-Task Learning with Feature Attention","text":"<pre><code># Multi-task model where different tasks need different features\ndef create_multi_task_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Shared feature processing with attention\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = ColumnAttention(input_dim=64)(x)\n\n    # Task 1: Classification (needs different features)\n    task1 = keras.layers.Dense(32, activation='relu')(x)\n    task1 = keras.layers.Dropout(0.2)(task1)\n    task1_output = keras.layers.Dense(3, activation='softmax', name='classification')(task1)\n\n    # Task 2: Regression (needs different features)\n    task2 = keras.layers.Dense(32, activation='relu')(x)\n    task2 = keras.layers.Dropout(0.2)(task2)\n    task2_output = keras.layers.Dense(1, name='regression')(task2)\n\n    return keras.Model(inputs, [task1_output, task2_output])\n\nmodel = create_multi_task_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/column-attention/#example-3-noisy-data-handling","title":"Example 3: Noisy Data Handling","text":"<pre><code># Handle noisy tabular data with column attention\ndef create_robust_model():\n    inputs = keras.Input(shape=(30,))\n\n    # Initial feature processing\n    x = keras.layers.Dense(128, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Column attention to focus on important features\n    x = ColumnAttention(input_dim=128, hidden_dim=64)(x)\n\n    # Additional processing\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Use with noisy data\nmodel = create_robust_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# The column attention will automatically learn to down-weight noisy features\n</code></pre>"},{"location":"layers/column-attention/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Placement: Use after initial feature processing but before final predictions</li> <li>Hidden Dimension: Start with input_dim // 2, adjust based on complexity</li> <li>Regularization: Combine with dropout and batch normalization for better generalization</li> <li>Interpretability: Access attention weights to understand feature importance</li> <li>Data Quality: Particularly effective with noisy or high-dimensional data</li> <li>Monitoring: Track attention weight distributions during training</li> </ul>"},{"location":"layers/column-attention/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, num_features)</li> <li>Dimension Mismatch: input_dim must match the number of features</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Memory: Hidden dimension affects memory usage - keep reasonable</li> <li>Interpretation: Attention weights are relative, not absolute importance</li> </ul>"},{"location":"layers/column-attention/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>RowAttention - Row-wise attention for sample relationships</li> <li>TabularAttention - General tabular attention mechanism</li> <li>VariableSelection - Feature selection layer</li> <li>SparseAttentionWeighting - Sparse attention weights</li> </ul>"},{"location":"layers/column-attention/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Mechanisms in Deep Learning - Understanding attention mechanisms</li> <li>Feature Selection in Machine Learning - Feature selection concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/date-encoding-layer/","title":"\ud83d\udd04 DateEncodingLayer\ud83d\udd04 DateEncodingLayer","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/date-encoding-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DateEncodingLayer</code> takes date components (year, month, day, day of week) and encodes them into cyclical features using sine and cosine transformations. This approach preserves the cyclical nature of temporal data, which is crucial for neural networks to understand patterns like seasonality and periodicity.</p> <p>This layer is particularly powerful for time series analysis where the cyclical nature of dates is important, such as seasonal patterns, weekly cycles, and daily rhythms.</p>"},{"location":"layers/date-encoding-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DateEncodingLayer processes date components through cyclical encoding:</p> <ol> <li>Component Extraction: Extracts year, month, day, and day of week</li> <li>Year Normalization: Normalizes year to [0, 1] range based on min/max years</li> <li>Cyclical Encoding: Applies sine and cosine transformations to each component</li> <li>Feature Combination: Combines all cyclical encodings into a single tensor</li> <li>Output Generation: Produces 8-dimensional cyclical feature vector</li> </ol> <pre><code>graph TD\n    A[Date Components: year, month, day, day_of_week] --&gt; B[Year Normalization]\n    B --&gt; C[Cyclical Encoding]\n\n    C --&gt; D[Year: sin(2\u03c0 * year_norm), cos(2\u03c0 * year_norm)]\n    C --&gt; E[Month: sin(2\u03c0 * month/12), cos(2\u03c0 * month/12)]\n    C --&gt; F[Day: sin(2\u03c0 * day/31), cos(2\u03c0 * day/31)]\n    C --&gt; G[Day of Week: sin(2\u03c0 * dow/7), cos(2\u03c0 * dow/7)]\n\n    D --&gt; H[Combine All Encodings]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Cyclical Features: 8 dimensions]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style I fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style H fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/date-encoding-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DateEncodingLayer's Solution Cyclical Nature Treats dates as linear values \ud83c\udfaf Preserves cyclicality with sine/cosine encoding Seasonal Patterns Misses seasonal relationships \u26a1 Captures seasonality through cyclical encoding Neural Network Understanding Linear encoding confuses networks \ud83e\udde0 Neural-friendly cyclical representation Temporal Relationships Loses temporal proximity \ud83d\udd17 Maintains temporal relationships through encoding"},{"location":"layers/date-encoding-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Time Series Analysis: Encoding temporal features for neural networks</li> <li>Seasonal Pattern Recognition: Capturing seasonal and cyclical patterns</li> <li>Event Prediction: Predicting events based on temporal patterns</li> <li>Financial Analysis: Analyzing financial data with temporal components</li> <li>Weather Forecasting: Processing weather data with seasonal patterns</li> </ul>"},{"location":"layers/date-encoding-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/date-encoding-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import DateEncodingLayer\n\n# Create sample date components [year, month, day, day_of_week]\ndate_components = keras.ops.convert_to_tensor([\n    [2023, 1, 15, 6],   # Sunday, January 15, 2023\n    [2023, 6, 21, 2],   # Wednesday, June 21, 2023\n    [2023, 12, 25, 0]   # Sunday, December 25, 2023\n], dtype=\"float32\")\n\n# Apply cyclical encoding\nencoder = DateEncodingLayer(min_year=1900, max_year=2100)\nencoded = encoder(date_components)\n\nprint(f\"Input shape: {date_components.shape}\")    # (3, 4)\nprint(f\"Output shape: {encoded.shape}\")          # (3, 8)\nprint(f\"Encoded features: {encoded}\")\n# Output: [year_sin, year_cos, month_sin, month_cos, day_sin, day_cos, dow_sin, dow_cos]\n</code></pre>"},{"location":"layers/date-encoding-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DateEncodingLayer\n\nmodel = keras.Sequential([\n    DateEncodingLayer(min_year=1900, max_year=2100),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/date-encoding-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DateEncodingLayer\n\n# Define inputs\ninputs = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n# Apply cyclical encoding\nx = DateEncodingLayer(min_year=1900, max_year=2100)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/date-encoding-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom year range\ndef create_temporal_analysis_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply cyclical encoding\n    cyclical_features = DateEncodingLayer(\n        min_year=2000,  # Custom year range\n        max_year=2030\n    )(date_input)\n\n    # Process cyclical features\n    x = keras.layers.Dense(64, activation='relu')(cyclical_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Multi-task output\n    season = keras.layers.Dense(4, activation='softmax', name='season')(x)\n    weekday = keras.layers.Dense(7, activation='softmax', name='weekday')(x)\n    is_weekend = keras.layers.Dense(1, activation='sigmoid', name='is_weekend')(x)\n\n    return keras.Model(date_input, [season, weekday, is_weekend])\n\nmodel = create_temporal_analysis_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'season': 'categorical_crossentropy', 'weekday': 'categorical_crossentropy', 'is_weekend': 'binary_crossentropy'},\n    loss_weights={'season': 1.0, 'weekday': 0.5, 'is_weekend': 0.3}\n)\n</code></pre>"},{"location":"layers/date-encoding-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/date-encoding-layer/#kmr.layers.DateEncodingLayer","title":"kmr.layers.DateEncodingLayer","text":"<p>DateEncodingLayer for encoding date components into cyclical features.</p> <p>This layer takes date components (year, month, day, day of week) and encodes them into cyclical features using sine and cosine transformations.</p>"},{"location":"layers/date-encoding-layer/#kmr.layers.DateEncodingLayer-classes","title":"Classes","text":""},{"location":"layers/date-encoding-layer/#kmr.layers.DateEncodingLayer.DateEncodingLayer","title":"DateEncodingLayer","text":"<pre><code>DateEncodingLayer(\n    min_year: int = 1900, max_year: int = 2100, **kwargs\n)\n</code></pre> <p>Layer for encoding date components into cyclical features.</p> <p>This layer takes date components (year, month, day, day of week) and encodes them into cyclical features using sine and cosine transformations. The year is normalized to a range between 0 and 1 based on min_year and max_year.</p> <p>Parameters:</p> Name Type Description Default <code>min_year</code> <code>int</code> <p>Minimum year for normalization (default: 1900)</p> <code>1900</code> <code>max_year</code> <code>int</code> <p>Maximum year for normalization (default: 2100)</p> <code>2100</code> <code>**kwargs</code> <p>Additional layer arguments</p> <code>{}</code> Input shape <p>Tensor with shape: <code>(..., 4)</code> containing [year, month, day, day_of_week]</p> Output shape <p>Tensor with shape: <code>(..., 8)</code> containing cyclical encodings: [year_sin, year_cos, month_sin, month_cos, day_sin, day_cos, dow_sin, dow_cos]</p> <p>Initialize the layer.</p>"},{"location":"layers/date-encoding-layer/#kmr.layers.DateEncodingLayer.DateEncodingLayer-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(input_shape) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>Shape of the input tensor</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Output shape</p>"},{"location":"layers/date-encoding-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/date-encoding-layer/#min_year-int","title":"<code>min_year</code> (int)","text":"<ul> <li>Purpose: Minimum year for normalization</li> <li>Default: 1900</li> <li>Impact: Affects year normalization range</li> <li>Recommendation: Set based on your data's year range</li> </ul>"},{"location":"layers/date-encoding-layer/#max_year-int","title":"<code>max_year</code> (int)","text":"<ul> <li>Purpose: Maximum year for normalization</li> <li>Default: 2100</li> <li>Impact: Affects year normalization range</li> <li>Recommendation: Set based on your data's year range</li> </ul>"},{"location":"layers/date-encoding-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple mathematical operations</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for cyclical temporal features</li> <li>Best For: Time series data requiring cyclical encoding</li> </ul>"},{"location":"layers/date-encoding-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/date-encoding-layer/#example-1-seasonal-pattern-analysis","title":"Example 1: Seasonal Pattern Analysis","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DateEncodingLayer\n\n# Create seasonal analysis model\ndef create_seasonal_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply cyclical encoding\n    cyclical_features = DateEncodingLayer(min_year=2000, max_year=2030)(date_input)\n\n    # Process cyclical features\n    x = keras.layers.Dense(64, activation='relu')(cyclical_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Predictions\n    temperature = keras.layers.Dense(1, name='temperature')(x)\n    humidity = keras.layers.Dense(1, name='humidity')(x)\n    season = keras.layers.Dense(4, activation='softmax', name='season')(x)\n\n    return keras.Model(date_input, [temperature, humidity, season])\n\nmodel = create_seasonal_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'temperature': 'mse', 'humidity': 'mse', 'season': 'categorical_crossentropy'},\n    loss_weights={'temperature': 1.0, 'humidity': 0.5, 'season': 0.3}\n)\n\n# Test with sample data\nsample_dates = keras.ops.convert_to_tensor([\n    [2023, 1, 15, 6],   # Winter Sunday\n    [2023, 4, 15, 5],   # Spring Saturday\n    [2023, 7, 15, 5],   # Summer Saturday\n    [2023, 10, 15, 6]   # Fall Sunday\n], dtype=\"float32\")\n\npredictions = model(sample_dates)\nprint(f\"Predictions shape: {[p.shape for p in predictions]}\")\n</code></pre>"},{"location":"layers/date-encoding-layer/#example-2-business-cycle-analysis","title":"Example 2: Business Cycle Analysis","text":"<pre><code># Analyze business cycles with cyclical encoding\ndef create_business_cycle_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply cyclical encoding\n    cyclical_features = DateEncodingLayer(min_year=2020, max_year=2030)(date_input)\n\n    # Process cyclical features\n    x = keras.layers.Dense(128, activation='relu')(cyclical_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Business predictions\n    sales_volume = keras.layers.Dense(1, name='sales_volume')(x)\n    customer_traffic = keras.layers.Dense(1, name='customer_traffic')(x)\n    is_peak_season = keras.layers.Dense(1, activation='sigmoid', name='is_peak_season')(x)\n\n    return keras.Model(date_input, [sales_volume, customer_traffic, is_peak_season])\n\nmodel = create_business_cycle_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'sales_volume': 'mse', 'customer_traffic': 'mse', 'is_peak_season': 'binary_crossentropy'},\n    loss_weights={'sales_volume': 1.0, 'customer_traffic': 0.5, 'is_peak_season': 0.3}\n)\n</code></pre>"},{"location":"layers/date-encoding-layer/#example-3-cyclical-feature-analysis","title":"Example 3: Cyclical Feature Analysis","text":"<pre><code># Analyze the cyclical features produced by the encoding\ndef analyze_cyclical_features():\n    # Create sample date components\n    dates = keras.ops.convert_to_tensor([\n        [2023, 1, 1, 0],    # New Year's Day (Sunday)\n        [2023, 3, 20, 0],   # Spring Equinox (Sunday)\n        [2023, 6, 21, 2],   # Summer Solstice (Wednesday)\n        [2023, 9, 22, 4],   # Fall Equinox (Friday)\n        [2023, 12, 21, 3]   # Winter Solstice (Thursday)\n    ], dtype=\"float32\")\n\n    # Apply cyclical encoding\n    encoder = DateEncodingLayer(min_year=2000, max_year=2030)\n    encoded = encoder(dates)\n\n    # Analyze cyclical patterns\n    print(\"Cyclical Feature Analysis:\")\n    print(\"=\" * 50)\n    print(\"Date\\t\\tYear\\tMonth\\tDay\\tDOW\\tYear_Sin\\tYear_Cos\\tMonth_Sin\\tMonth_Cos\")\n    print(\"-\" * 80)\n\n    for i, date in enumerate(dates):\n        year, month, day, dow = date.numpy()\n        year_sin, year_cos, month_sin, month_cos, day_sin, day_cos, dow_sin, dow_cos = encoded[i].numpy()\n\n        print(f\"{int(year)}-{int(month):02d}-{int(day):02d}\\t{int(year)}\\t{int(month)}\\t{int(day)}\\t{int(dow)}\\t\"\n              f\"{year_sin:.3f}\\t\\t{year_cos:.3f}\\t\\t{month_sin:.3f}\\t\\t{month_cos:.3f}\")\n\n    return encoded\n\n# Analyze cyclical features\n# cyclical_data = analyze_cyclical_features()\n</code></pre>"},{"location":"layers/date-encoding-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Year Range: Set min_year and max_year based on your data's actual range</li> <li>Input Format: Input must be [year, month, day, day_of_week] format</li> <li>Cyclical Nature: The encoding preserves cyclical relationships</li> <li>Neural Networks: Works well with neural networks for temporal patterns</li> <li>Seasonality: Excellent for capturing seasonal and cyclical patterns</li> <li>Integration: Combines well with other temporal processing layers</li> </ul>"},{"location":"layers/date-encoding-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be (..., 4) tensor with date components</li> <li>Year Range: min_year must be less than max_year</li> <li>Component Order: Must be [year, month, day, day_of_week] in that order</li> <li>Data Type: Input should be float32 tensor</li> <li>Missing Values: Doesn't handle missing values - preprocess first</li> </ul>"},{"location":"layers/date-encoding-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DateParsingLayer - Date string parsing</li> <li>SeasonLayer - Seasonal information extraction</li> <li>CastToFloat32Layer - Type casting utility</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> </ul>"},{"location":"layers/date-encoding-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Cyclical Encoding in Time Series - Cyclical encoding concepts</li> <li>Sine and Cosine Transformations - Trigonometric functions</li> <li>Time Series Feature Engineering - Feature engineering techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/date-parsing-layer/","title":"\ud83d\udcc5 DateParsingLayer\ud83d\udcc5 DateParsingLayer","text":"\ud83d\udfe2 Beginner \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/date-parsing-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DateParsingLayer</code> takes date strings in a specified format and returns a tensor containing the year, month, day of the month, and day of the week. This layer is essential for processing temporal data and converting date strings into numerical features that can be used by neural networks.</p> <p>This layer supports multiple date formats and automatically calculates the day of the week using Zeller's congruence algorithm, making it perfect for time series analysis and temporal feature engineering.</p>"},{"location":"layers/date-parsing-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DateParsingLayer processes date strings through intelligent parsing:</p> <ol> <li>Format Validation: Validates the input date format</li> <li>String Parsing: Extracts year, month, and day components</li> <li>Day of Week Calculation: Uses Zeller's congruence to calculate day of week</li> <li>Component Extraction: Returns [year, month, day, day_of_week] as integers</li> <li>Output Generation: Produces numerical date components</li> </ol> <pre><code>graph TD\n    A[Date String Input] --&gt; B[Format Validation]\n    B --&gt; C[String Parsing]\n    C --&gt; D[Extract Year, Month, Day]\n    D --&gt; E[Calculate Day of Week]\n    E --&gt; F[Zeller's Congruence Algorithm]\n    F --&gt; G[Date Components Output]\n\n    H[Supported Formats] --&gt; B\n    I[YYYY-MM-DD] --&gt; H\n    J[YYYY/MM/DD] --&gt; H\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style F fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/date-parsing-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DateParsingLayer's Solution Date String Processing Manual parsing with pandas/datetime \ud83c\udfaf Automatic parsing with format validation Day of Week Calculation Separate calculation step \u26a1 Integrated calculation using Zeller's algorithm Format Consistency Multiple parsing functions \ud83e\udde0 Unified interface for different date formats Neural Network Integration Separate preprocessing step \ud83d\udd17 Seamless integration with Keras models"},{"location":"layers/date-parsing-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Time Series Analysis: Converting date strings to numerical features</li> <li>Temporal Feature Engineering: Creating date-based features</li> <li>Event Analysis: Processing event timestamps</li> <li>Seasonal Analysis: Extracting seasonal information from dates</li> <li>Financial Data: Processing financial timestamps and dates</li> </ul>"},{"location":"layers/date-parsing-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/date-parsing-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import DateParsingLayer\n\n# Create sample date strings\ndate_strings = [\n    \"2023-01-15\",\n    \"2023-06-21\", \n    \"2023-12-25\"\n]\n\n# Apply date parsing\nparser = DateParsingLayer(date_format=\"YYYY-MM-DD\")\nparsed = parser(date_strings)\n\nprint(f\"Input: {date_strings}\")\nprint(f\"Output shape: {parsed.shape}\")  # (3, 4)\nprint(f\"Parsed dates: {parsed}\")\n# Output: [[2023, 1, 15, 6], [2023, 6, 21, 2], [2023, 12, 25, 0]]\n# Format: [year, month, day, day_of_week] where 0=Sunday, 6=Saturday\n</code></pre>"},{"location":"layers/date-parsing-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DateParsingLayer\n\nmodel = keras.Sequential([\n    DateParsingLayer(date_format=\"YYYY-MM-DD\"),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/date-parsing-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DateParsingLayer\n\n# Define inputs\ninputs = keras.Input(shape=(), dtype=\"string\")  # String input for dates\n\n# Apply date parsing\nx = DateParsingLayer(date_format=\"YYYY-MM-DD\")(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/date-parsing-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with different date formats\ndef create_date_processing_model():\n    # Input for date strings\n    date_input = keras.Input(shape=(), dtype=\"string\")\n\n    # Parse dates\n    date_components = DateParsingLayer(date_format=\"YYYY/MM/DD\")(date_input)\n\n    # Process date components\n    x = keras.layers.Dense(64, activation='relu')(date_components)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    season = keras.layers.Dense(4, activation='softmax', name='season')(x)\n    weekday = keras.layers.Dense(7, activation='softmax', name='weekday')(x)\n    is_weekend = keras.layers.Dense(1, activation='sigmoid', name='is_weekend')(x)\n\n    return keras.Model(date_input, [season, weekday, is_weekend])\n\nmodel = create_date_processing_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'season': 'categorical_crossentropy', 'weekday': 'categorical_crossentropy', 'is_weekend': 'binary_crossentropy'},\n    loss_weights={'season': 1.0, 'weekday': 0.5, 'is_weekend': 0.3}\n)\n</code></pre>"},{"location":"layers/date-parsing-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/date-parsing-layer/#kmr.layers.DateParsingLayer","title":"kmr.layers.DateParsingLayer","text":"<p>Date Parsing Layer for Keras 3.</p> <p>This module provides a layer for parsing date strings into numerical components.</p>"},{"location":"layers/date-parsing-layer/#kmr.layers.DateParsingLayer-classes","title":"Classes","text":""},{"location":"layers/date-parsing-layer/#kmr.layers.DateParsingLayer.DateParsingLayer","title":"DateParsingLayer","text":"<pre><code>DateParsingLayer(date_format: str = 'YYYY-MM-DD', **kwargs)\n</code></pre> <p>Layer for parsing date strings into numerical components.</p> <p>This layer takes date strings in a specified format and returns a tensor containing the year, month, day of the month, and day of the week.</p> <p>Parameters:</p> Name Type Description Default <code>date_format</code> <code>str</code> <p>Format of the date strings. Currently supports 'YYYY-MM-DD' and 'YYYY/MM/DD'. Default is 'YYYY-MM-DD'.</p> <code>'YYYY-MM-DD'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the base layer.</p> <code>{}</code> Input shape <p>String tensor of any shape.</p> Output shape <p>Same as input shape with an additional dimension of size 4 appended. For example, if input shape is [batch_size], output shape will be [batch_size, 4].</p> <p>Initialize the layer.</p>"},{"location":"layers/date-parsing-layer/#kmr.layers.DateParsingLayer.DateParsingLayer-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(input_shape) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>Shape of the input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape of the output tensor.</p>"},{"location":"layers/date-parsing-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/date-parsing-layer/#date_format-str","title":"<code>date_format</code> (str)","text":"<ul> <li>Purpose: Format of the date strings</li> <li>Options: \"YYYY-MM-DD\", \"YYYY/MM/DD\"</li> <li>Default: \"YYYY-MM-DD\"</li> <li>Impact: Determines how date strings are parsed</li> <li>Recommendation: Use \"YYYY-MM-DD\" for ISO format, \"YYYY/MM/DD\" for alternative format</li> </ul>"},{"location":"layers/date-parsing-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple string parsing</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for date parsing and day calculation</li> <li>Best For: Date string processing and temporal feature extraction</li> </ul>"},{"location":"layers/date-parsing-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/date-parsing-layer/#example-1-time-series-feature-engineering","title":"Example 1: Time Series Feature Engineering","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DateParsingLayer\n\n# Create time series data with dates\ndef create_time_series_features():\n    # Sample date strings\n    dates = [\n        \"2023-01-01\", \"2023-01-02\", \"2023-01-03\",\n        \"2023-02-14\", \"2023-03-15\", \"2023-04-01\",\n        \"2023-05-01\", \"2023-06-21\", \"2023-07-04\",\n        \"2023-08-15\", \"2023-09-01\", \"2023-10-31\",\n        \"2023-11-11\", \"2023-12-25\"\n    ]\n\n    # Parse dates\n    parser = DateParsingLayer(date_format=\"YYYY-MM-DD\")\n    date_components = parser(dates)\n\n    # Create additional features\n    year = date_components[:, 0:1]\n    month = date_components[:, 1:2]\n    day = date_components[:, 2:3]\n    day_of_week = date_components[:, 3:4]\n\n    # Create derived features\n    is_weekend = keras.ops.cast(day_of_week &gt;= 5, \"float32\")  # Saturday=5, Sunday=6\n    is_month_start = keras.ops.cast(day == 1, \"float32\")\n    is_quarter_start = keras.ops.cast(\n        keras.ops.logical_or(\n            keras.ops.equal(month, 1),\n            keras.ops.logical_or(\n                keras.ops.equal(month, 4),\n                keras.ops.logical_or(\n                    keras.ops.equal(month, 7),\n                    keras.ops.equal(month, 10)\n                )\n            )\n        ), \"float32\"\n    )\n\n    # Combine all features\n    features = keras.ops.concatenate([\n        year, month, day, day_of_week,\n        is_weekend, is_month_start, is_quarter_start\n    ], axis=1)\n\n    return features\n\n# Create features\ntime_series_features = create_time_series_features()\nprint(f\"Time series features shape: {time_series_features.shape}\")\nprint(f\"Features: [year, month, day, day_of_week, is_weekend, is_month_start, is_quarter_start]\")\n</code></pre>"},{"location":"layers/date-parsing-layer/#example-2-seasonal-analysis","title":"Example 2: Seasonal Analysis","text":"<pre><code># Analyze seasonal patterns in dates\ndef create_seasonal_analysis_model():\n    # Input for date strings\n    date_input = keras.Input(shape=(), dtype=\"string\")\n\n    # Parse dates\n    date_components = DateParsingLayer(date_format=\"YYYY-MM-DD\")(date_input)\n\n    # Extract components\n    year = date_components[:, 0:1]\n    month = date_components[:, 1:2]\n    day = date_components[:, 2:3]\n    day_of_week = date_components[:, 3:4]\n\n    # Create seasonal features\n    # Spring: March (3), April (4), May (5)\n    is_spring = keras.ops.cast(\n        keras.ops.logical_and(month &gt;= 3, month &lt;= 5), \"float32\"\n    )\n\n    # Summer: June (6), July (7), August (8)\n    is_summer = keras.ops.cast(\n        keras.ops.logical_and(month &gt;= 6, month &lt;= 8), \"float32\"\n    )\n\n    # Fall: September (9), October (10), November (11)\n    is_fall = keras.ops.cast(\n        keras.ops.logical_and(month &gt;= 9, month &lt;= 11), \"float32\"\n    )\n\n    # Winter: December (12), January (1), February (2)\n    is_winter = keras.ops.cast(\n        keras.ops.logical_or(\n            keras.ops.equal(month, 12),\n            keras.ops.logical_or(\n                keras.ops.equal(month, 1),\n                keras.ops.equal(month, 2)\n            )\n        ), \"float32\"\n    )\n\n    # Combine features\n    seasonal_features = keras.ops.concatenate([\n        year, month, day, day_of_week,\n        is_spring, is_summer, is_fall, is_winter\n    ], axis=1)\n\n    # Process seasonal features\n    x = keras.layers.Dense(32, activation='relu')(seasonal_features)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Predictions\n    season_pred = keras.layers.Dense(4, activation='softmax', name='season')(x)\n    temperature_pred = keras.layers.Dense(1, name='temperature')(x)\n\n    return keras.Model(date_input, [season_pred, temperature_pred])\n\nmodel = create_seasonal_analysis_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'season': 'categorical_crossentropy', 'temperature': 'mse'},\n    loss_weights={'season': 1.0, 'temperature': 0.5}\n)\n</code></pre>"},{"location":"layers/date-parsing-layer/#example-3-business-day-analysis","title":"Example 3: Business Day Analysis","text":"<pre><code># Analyze business day patterns\ndef create_business_day_model():\n    # Input for date strings\n    date_input = keras.Input(shape=(), dtype=\"string\")\n\n    # Parse dates\n    date_components = DateParsingLayer(date_format=\"YYYY-MM-DD\")(date_input)\n\n    # Extract components\n    year = date_components[:, 0:1]\n    month = date_components[:, 1:2]\n    day = date_components[:, 2:3]\n    day_of_week = date_components[:, 3:4]\n\n    # Create business day features\n    is_weekday = keras.ops.cast(day_of_week &lt; 5, \"float32\")  # Monday=0 to Friday=4\n    is_weekend = keras.ops.cast(day_of_week &gt;= 5, \"float32\")  # Saturday=5, Sunday=6\n    is_monday = keras.ops.cast(day_of_week == 0, \"float32\")\n    is_friday = keras.ops.cast(day_of_week == 4, \"float32\")\n\n    # Month-end and month-start features\n    is_month_start = keras.ops.cast(day == 1, \"float32\")\n    is_month_end = keras.ops.cast(day &gt;= 28, \"float32\")  # Approximate month-end\n\n    # Combine features\n    business_features = keras.ops.concatenate([\n        year, month, day, day_of_week,\n        is_weekday, is_weekend, is_monday, is_friday,\n        is_month_start, is_month_end\n    ], axis=1)\n\n    # Process business features\n    x = keras.layers.Dense(64, activation='relu')(business_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Predictions\n    is_business_day = keras.layers.Dense(1, activation='sigmoid', name='is_business_day')(x)\n    activity_level = keras.layers.Dense(1, name='activity_level')(x)\n\n    return keras.Model(date_input, [is_business_day, activity_level])\n\nmodel = create_business_day_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'is_business_day': 'binary_crossentropy', 'activity_level': 'mse'},\n    loss_weights={'is_business_day': 1.0, 'activity_level': 0.5}\n)\n</code></pre>"},{"location":"layers/date-parsing-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Date Format: Use consistent date format across your dataset</li> <li>String Input: Ensure input is string tensor, not numerical</li> <li>Day of Week: Remember 0=Sunday, 6=Saturday</li> <li>Validation: Layer validates date format automatically</li> <li>Integration: Works seamlessly with other Keras layers</li> <li>Performance: Very fast for batch processing of dates</li> </ul>"},{"location":"layers/date-parsing-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Type: Must be string tensor, not numerical</li> <li>Date Format: Must match one of the supported formats</li> <li>Invalid Dates: Layer doesn't validate date validity (e.g., 2023-02-30)</li> <li>Timezone: Doesn't handle timezone information</li> <li>Leap Years: Day of week calculation handles leap years correctly</li> </ul>"},{"location":"layers/date-parsing-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DateEncodingLayer - Cyclical date encoding</li> <li>SeasonLayer - Seasonal information extraction</li> <li>CastToFloat32Layer - Type casting utility</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> </ul>"},{"location":"layers/date-parsing-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Date and Time Processing - Date format standards</li> <li>Zeller's Congruence - Day of week calculation algorithm</li> <li>Time Series Analysis - Time series concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/","title":"\ud83d\udd27 DifferentiableTabularPreprocessor\ud83d\udd27 DifferentiableTabularPreprocessor","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/differentiable-tabular-preprocessor/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DifferentiableTabularPreprocessor</code> integrates preprocessing into the model so that optimal imputation and normalization parameters are learned end-to-end. This approach is particularly useful for tabular data with missing values and features that need normalization.</p> <p>This layer replaces missing values with learnable imputation vectors and applies learned affine transformations (scaling and shifting) to each feature, making the entire preprocessing pipeline differentiable.</p>"},{"location":"layers/differentiable-tabular-preprocessor/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DifferentiableTabularPreprocessor processes tabular data through learnable preprocessing:</p> <ol> <li>Missing Value Detection: Identifies NaN values in input data</li> <li>Learnable Imputation: Replaces missing values with learned imputation vectors</li> <li>Affine Transformation: Applies learned scaling (gamma) and shifting (beta) to each feature</li> <li>End-to-End Learning: All parameters are learned jointly with the model</li> <li>Output Generation: Produces preprocessed features ready for downstream processing</li> </ol> <pre><code>graph TD\n    A[Input Features with NaNs] --&gt; B[Missing Value Detection]\n    B --&gt; C[Learnable Imputation]\n    C --&gt; D[Affine Transformation]\n    D --&gt; E[Gamma Scaling]\n    E --&gt; F[Beta Shifting]\n    F --&gt; G[Preprocessed Features]\n\n    H[Learnable Imputation Vector] --&gt; C\n    I[Learnable Gamma Parameters] --&gt; E\n    J[Learnable Beta Parameters] --&gt; F\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DifferentiableTabularPreprocessor's Solution Missing Values Separate imputation step (mean, median, etc.) \ud83c\udfaf Learnable imputation optimized for the task Feature Scaling Static normalization (z-score, min-max) \u26a1 Learned scaling adapted to data and task End-to-End Learning Separate preprocessing and modeling \ud83e\udde0 Integrated preprocessing learned jointly Data Quality Fixed preprocessing strategies \ud83d\udd17 Adaptive preprocessing that improves with training"},{"location":"layers/differentiable-tabular-preprocessor/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Missing Data Handling: Intelligent imputation of missing values</li> <li>Feature Normalization: Learned scaling and shifting of features</li> <li>End-to-End Learning: Integrated preprocessing and modeling</li> <li>Tabular Deep Learning: Advanced preprocessing for tabular neural networks</li> <li>Data Quality: Adaptive preprocessing that improves with training</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/differentiable-tabular-preprocessor/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\n# Create sample data with missing values\nx = keras.ops.convert_to_tensor([\n    [1.0, np.nan, 3.0, 4.0, 5.0],\n    [2.0, 2.0, np.nan, 4.0, 5.0],\n    [np.nan, 2.0, 3.0, 4.0, np.nan]\n], dtype=\"float32\")\n\n# Apply differentiable preprocessing\npreprocessor = DifferentiableTabularPreprocessor(num_features=5)\npreprocessed = preprocessor(x)\n\nprint(f\"Input shape: {x.shape}\")           # (3, 5)\nprint(f\"Output shape: {preprocessed.shape}\")  # (3, 5)\nprint(f\"Has NaNs: {keras.ops.any(keras.ops.isnan(preprocessed))}\")  # False\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\nmodel = keras.Sequential([\n    DifferentiableTabularPreprocessor(num_features=10),  # Preprocess first\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\n# Define inputs\ninputs = keras.Input(shape=(15,))  # 15 features\n\n# Apply differentiable preprocessing\nx = DifferentiableTabularPreprocessor(num_features=15)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom preprocessing\ndef create_advanced_preprocessing_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Apply differentiable preprocessing\n    x = DifferentiableTabularPreprocessor(\n        num_features=20,\n        name=\"learnable_preprocessing\"\n    )(inputs)\n\n    # Multi-branch processing\n    branch1 = keras.layers.Dense(32, activation='relu')(x)\n    branch1 = keras.layers.Dense(16, activation='relu')(branch1)\n\n    branch2 = keras.layers.Dense(32, activation='tanh')(x)\n    branch2 = keras.layers.Dense(16, activation='tanh')(branch2)\n\n    # Combine branches\n    x = keras.layers.Concatenate()([branch1, branch2])\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_advanced_preprocessing_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/differentiable-tabular-preprocessor/#kmr.layers.DifferentiableTabularPreprocessor","title":"kmr.layers.DifferentiableTabularPreprocessor","text":"<p>This module implements a DifferentiableTabularPreprocessor layer that integrates preprocessing into the model so that the optimal imputation and normalization parameters are learned end-to-end. This approach is useful for tabular data with missing values and features that need normalization.</p>"},{"location":"layers/differentiable-tabular-preprocessor/#kmr.layers.DifferentiableTabularPreprocessor-classes","title":"Classes","text":""},{"location":"layers/differentiable-tabular-preprocessor/#kmr.layers.DifferentiableTabularPreprocessor.DifferentiableTabularPreprocessor","title":"DifferentiableTabularPreprocessor","text":"<pre><code>DifferentiableTabularPreprocessor(\n    num_features: int,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>A differentiable preprocessing layer for numeric tabular data.</p> This layer <ul> <li>Replaces missing values (NaNs) with a learnable imputation vector.</li> <li>Applies a learned affine transformation (scaling and shifting) to each feature.</li> </ul> <p>The idea is to integrate preprocessing into the model so that the optimal imputation and normalization parameters are learned end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of numeric features in the input.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, num_features)</code> (same as input)</p> Example <pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\n# Suppose we have tabular data with 5 numeric features\nx = keras.ops.convert_to_tensor([\n    [1.0, np.nan, 3.0, 4.0, 5.0],\n    [2.0, 2.0, np.nan, 4.0, 5.0]\n], dtype=\"float32\")\n\npreproc = DifferentiableTabularPreprocessor(num_features=5)\ny = preproc(x)\nprint(y)\n</code></pre> <p>Initialize the DifferentiableTabularPreprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/differentiable-tabular-preprocessor/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/differentiable-tabular-preprocessor/#num_features-int","title":"<code>num_features</code> (int)","text":"<ul> <li>Purpose: Number of numeric features in the input</li> <li>Range: 1 to 1000+ (typically 5-100)</li> <li>Impact: Must match the last dimension of your input tensor</li> <li>Recommendation: Set to the number of features in your dataset</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast - simple mathematical operations</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for handling missing values and normalization</li> <li>Best For: Tabular data with missing values requiring end-to-end learning</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/differentiable-tabular-preprocessor/#example-1-missing-data-handling","title":"Example 1: Missing Data Handling","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\n# Create data with different missing patterns\ndef create_missing_data():\n    np.random.seed(42)\n    n_samples, n_features = 1000, 8\n\n    # Create base data\n    data = np.random.normal(0, 1, (n_samples, n_features))\n\n    # Introduce missing values with different patterns\n    # Random missing\n    random_mask = np.random.random((n_samples, n_features)) &lt; 0.1\n    data[random_mask] = np.nan\n\n    # Column-specific missing (some columns have more missing values)\n    data[:, 2][np.random.random(n_samples) &lt; 0.3] = np.nan  # 30% missing in column 2\n    data[:, 5][np.random.random(n_samples) &lt; 0.2] = np.nan  # 20% missing in column 5\n\n    return data\n\n# Create and preprocess data\nmissing_data = create_missing_data()\nprint(f\"Missing data shape: {missing_data.shape}\")\nprint(f\"Missing values per column: {np.isnan(missing_data).sum(axis=0)}\")\n\n# Build model with differentiable preprocessing\ninputs = keras.Input(shape=(8,))\nx = DifferentiableTabularPreprocessor(num_features=8)(inputs)\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test preprocessing\ntest_data = keras.ops.convert_to_tensor(missing_data[:10], dtype=\"float32\")\npreprocessed = model.layers[0](test_data)\nprint(f\"Preprocessed shape: {preprocessed.shape}\")\nprint(f\"Has NaNs after preprocessing: {keras.ops.any(keras.ops.isnan(preprocessed))}\")\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#example-2-feature-specific-preprocessing","title":"Example 2: Feature-Specific Preprocessing","text":"<pre><code># Analyze learned preprocessing parameters\ndef analyze_preprocessing_parameters(model):\n    \"\"\"Analyze the learned preprocessing parameters.\"\"\"\n    preprocessor = model.layers[0]  # First layer is the preprocessor\n\n    # Get learned parameters\n    imputation_values = preprocessor.impute.numpy()\n    gamma_values = preprocessor.gamma.numpy()\n    beta_values = preprocessor.beta.numpy()\n\n    print(\"Learned Preprocessing Parameters:\")\n    print(\"=\" * 50)\n\n    for i in range(len(imputation_values)):\n        print(f\"Feature {i+1}:\")\n        print(f\"  Imputation value: {imputation_values[i]:.4f}\")\n        print(f\"  Gamma (scaling): {gamma_values[i]:.4f}\")\n        print(f\"  Beta (shifting): {beta_values[i]:.4f}\")\n        print()\n\n    return {\n        'imputation': imputation_values,\n        'gamma': gamma_values,\n        'beta': beta_values\n    }\n\n# Use with your trained model\n# params = analyze_preprocessing_parameters(model)\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#example-3-comparison-with-traditional-preprocessing","title":"Example 3: Comparison with Traditional Preprocessing","text":"<pre><code># Compare with traditional preprocessing methods\ndef compare_preprocessing_methods():\n    # Create data with missing values\n    data = np.random.normal(0, 1, (100, 5))\n    data[data &lt; -1] = np.nan  # Introduce missing values\n\n    # Traditional preprocessing\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import StandardScaler\n\n    # Impute missing values\n    imputer = SimpleImputer(strategy='mean')\n    data_imputed = imputer.fit_transform(data)\n\n    # Standardize\n    scaler = StandardScaler()\n    data_traditional = scaler.fit_transform(data_imputed)\n\n    # Differentiable preprocessing\n    inputs = keras.Input(shape=(5,))\n    x = DifferentiableTabularPreprocessor(num_features=5)(inputs)\n    model = keras.Model(inputs, x)\n\n    # Apply differentiable preprocessing\n    data_differentiable = model(keras.ops.convert_to_tensor(data, dtype=\"float32\"))\n\n    print(\"Traditional Preprocessing:\")\n    print(f\"  Mean: {np.mean(data_traditional, axis=0)}\")\n    print(f\"  Std: {np.std(data_traditional, axis=0)}\")\n\n    print(\"\\nDifferentiable Preprocessing:\")\n    print(f\"  Mean: {keras.ops.mean(data_differentiable, axis=0).numpy()}\")\n    print(f\"  Std: {keras.ops.std(data_differentiable, axis=0).numpy()}\")\n\n    return data_traditional, data_differentiable.numpy()\n\n# Compare methods\n# traditional, differentiable = compare_preprocessing_methods()\n</code></pre>"},{"location":"layers/differentiable-tabular-preprocessor/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Feature Count: Must match the number of features in your dataset</li> <li>Missing Values: Works best with moderate amounts of missing data</li> <li>Initialization: Parameters are initialized to reasonable defaults</li> <li>End-to-End Learning: Let the model learn optimal preprocessing parameters</li> <li>Monitoring: Track learned parameters to understand preprocessing behavior</li> <li>Combination: Use with other preprocessing layers for complex pipelines</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, num_features)</li> <li>Feature Mismatch: num_features must match input dimension</li> <li>NaN Handling: Only handles NaN values, not other missing value representations</li> <li>Memory Usage: Creates learnable parameters for each feature</li> <li>Overfitting: Can overfit on small datasets with many features</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DifferentialPreprocessingLayer - Advanced differential preprocessing</li> <li>DistributionTransformLayer - Distribution transformation</li> <li>CastToFloat32Layer - Type casting utility</li> <li>FeatureCutout - Feature regularization</li> </ul>"},{"location":"layers/differentiable-tabular-preprocessor/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>End-to-End Learning in Deep Learning - End-to-end learning concepts</li> <li>Missing Data Handling - Missing data techniques</li> <li>Feature Normalization - Feature scaling methods</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/differential-preprocessing-layer/","title":"\ud83d\udd04 DifferentialPreprocessingLayer\ud83d\udd04 DifferentialPreprocessingLayer","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/differential-preprocessing-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DifferentialPreprocessingLayer</code> applies multiple candidate transformations to tabular data and learns to combine them optimally. It handles missing values with learnable imputation and provides a differentiable preprocessing pipeline where the optimal preprocessing strategy is learned end-to-end.</p> <p>This layer is particularly powerful for tabular data where the optimal preprocessing strategy is not known in advance, allowing the model to learn the best combination of transformations.</p>"},{"location":"layers/differential-preprocessing-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DifferentialPreprocessingLayer processes data through multiple transformation candidates:</p> <ol> <li>Missing Value Imputation: Replaces missing values with learnable imputation vectors</li> <li>Multiple Transformations: Applies several candidate transformations:</li> <li>Identity (pass-through)</li> <li>Affine transformation (learnable scaling and bias)</li> <li>Nonlinear transformation via MLP</li> <li>Log transformation (using softplus for positivity)</li> <li>Learnable Combination: Uses softmax weights to combine transformation outputs</li> <li>End-to-End Learning: All parameters are learned jointly with the model</li> <li>Output Generation: Produces optimally preprocessed features</li> </ol> <pre><code>graph TD\n    A[Input Features with NaNs] --&gt; B[Missing Value Imputation]\n    B --&gt; C[Multiple Transformation Candidates]\n\n    C --&gt; D[Identity Transform]\n    C --&gt; E[Affine Transform]\n    C --&gt; F[Nonlinear MLP Transform]\n    C --&gt; G[Log Transform]\n\n    D --&gt; H[Softmax Combination Weights]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    H --&gt; I[Weighted Combination]\n    I --&gt; J[Preprocessed Features]\n\n    K[Learnable Imputation Vector] --&gt; B\n    L[Learnable Alpha Weights] --&gt; H\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style H fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/differential-preprocessing-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DifferentialPreprocessingLayer's Solution Unknown Preprocessing Manual preprocessing strategy selection \ud83c\udfaf Automatic learning of optimal preprocessing Multiple Transformations Single transformation approach \u26a1 Multiple candidates with learned combination Missing Values Separate imputation step \ud83e\udde0 Integrated imputation learned end-to-end Adaptive Preprocessing Fixed preprocessing pipeline \ud83d\udd17 Adaptive preprocessing that improves with training"},{"location":"layers/differential-preprocessing-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Unknown Data Characteristics: When optimal preprocessing strategy is unknown</li> <li>Multiple Transformation Needs: Data requiring different preprocessing approaches</li> <li>End-to-End Learning: Integrated preprocessing and modeling</li> <li>Adaptive Preprocessing: Preprocessing that adapts to data patterns</li> <li>Complex Tabular Data: Sophisticated preprocessing for complex datasets</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/differential-preprocessing-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentialPreprocessingLayer\n\n# Create sample data with missing values\nx = keras.ops.convert_to_tensor([\n    [1.0, 2.0, float('nan'), 4.0],\n    [2.0, float('nan'), 3.0, 4.0],\n    [float('nan'), 2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0, float('nan')],\n    [1.0, 2.0, 3.0, 4.0],\n    [2.0, 3.0, 4.0, 5.0],\n], dtype=\"float32\")\n\n# Apply differential preprocessing\npreprocessor = DifferentialPreprocessingLayer(\n    num_features=4,\n    mlp_hidden_units=8\n)\npreprocessed = preprocessor(x)\n\nprint(f\"Input shape: {x.shape}\")           # (6, 4)\nprint(f\"Output shape: {preprocessed.shape}\")  # (6, 4)\nprint(f\"Has NaNs: {keras.ops.any(keras.ops.isnan(preprocessed))}\")  # False\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DifferentialPreprocessingLayer\n\nmodel = keras.Sequential([\n    DifferentialPreprocessingLayer(\n        num_features=10,\n        mlp_hidden_units=16\n    ),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DifferentialPreprocessingLayer\n\n# Define inputs\ninputs = keras.Input(shape=(15,))  # 15 features\n\n# Apply differential preprocessing\nx = DifferentialPreprocessingLayer(\n    num_features=15,\n    mlp_hidden_units=32\n)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom MLP size\ndef create_advanced_preprocessing_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Apply differential preprocessing with larger MLP\n    x = DifferentialPreprocessingLayer(\n        num_features=20,\n        mlp_hidden_units=64,  # Larger MLP for more complex transformations\n        name=\"advanced_preprocessing\"\n    )(inputs)\n\n    # Multi-branch processing\n    branch1 = keras.layers.Dense(32, activation='relu')(x)\n    branch1 = keras.layers.Dense(16, activation='relu')(branch1)\n\n    branch2 = keras.layers.Dense(32, activation='tanh')(x)\n    branch2 = keras.layers.Dense(16, activation='tanh')(branch2)\n\n    # Combine branches\n    x = keras.layers.Concatenate()([branch1, branch2])\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_advanced_preprocessing_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/differential-preprocessing-layer/#kmr.layers.DifferentialPreprocessingLayer","title":"kmr.layers.DifferentialPreprocessingLayer","text":"<p>This module implements a DifferentialPreprocessingLayer that applies multiple candidate transformations to tabular data and learns to combine them optimally. It also handles missing values with learnable imputation. This approach is useful for tabular data where the optimal preprocessing strategy is not known in advance.</p>"},{"location":"layers/differential-preprocessing-layer/#kmr.layers.DifferentialPreprocessingLayer-classes","title":"Classes","text":""},{"location":"layers/differential-preprocessing-layer/#kmr.layers.DifferentialPreprocessingLayer.DifferentialPreprocessingLayer","title":"DifferentialPreprocessingLayer","text":"<pre><code>DifferentialPreprocessingLayer(\n    num_features: int,\n    mlp_hidden_units: int = 4,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Differentiable preprocessing layer for numeric tabular data with multiple candidate transformations.</p> This layer <ol> <li>Imputes missing values using a learnable imputation vector.</li> <li>Applies several candidate transformations:</li> <li>Identity (pass-through)</li> <li>Affine transformation (learnable scaling and bias)</li> <li>Nonlinear transformation via a small MLP</li> <li>Log transformation (using a softplus to ensure positivity)</li> <li>Learns softmax combination weights to aggregate the candidates.</li> </ol> <p>The entire preprocessing pipeline is differentiable, so the network learns the optimal imputation and transformation jointly with downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of numeric features in the input.</p> required <code>mlp_hidden_units</code> <code>int</code> <p>Number of hidden units in the nonlinear branch. Default is 4.</p> <code>4</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, num_features)</code> (same as input)</p> Example <pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentialPreprocessingLayer\n\n# Create dummy data: 6 samples, 4 features (with some missing values)\nx = keras.ops.convert_to_tensor([\n    [1.0, 2.0, float('nan'), 4.0],\n    [2.0, float('nan'), 3.0, 4.0],\n    [float('nan'), 2.0, 3.0, 4.0],\n    [1.0, 2.0, 3.0, float('nan')],\n    [1.0, 2.0, 3.0, 4.0],\n    [2.0, 3.0, 4.0, 5.0],\n], dtype=\"float32\")\n\n# Instantiate the layer for 4 features.\npreproc_layer = DifferentialPreprocessingLayer(num_features=4, mlp_hidden_units=8)\ny = preproc_layer(x)\nprint(y)\n</code></pre> <p>Initialize the DifferentialPreprocessingLayer.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of input features.</p> required <code>mlp_hidden_units</code> <code>int</code> <p>Number of hidden units in MLP.</p> <code>4</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/differential-preprocessing-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/differential-preprocessing-layer/#num_features-int","title":"<code>num_features</code> (int)","text":"<ul> <li>Purpose: Number of numeric features in the input</li> <li>Range: 1 to 1000+ (typically 5-100)</li> <li>Impact: Must match the last dimension of your input tensor</li> <li>Recommendation: Set to the number of features in your dataset</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#mlp_hidden_units-int","title":"<code>mlp_hidden_units</code> (int)","text":"<ul> <li>Purpose: Number of hidden units in the nonlinear transformation MLP</li> <li>Range: 2 to 128+ (typically 4-32)</li> <li>Impact: Larger values = more complex nonlinear transformations</li> <li>Recommendation: Start with 4-8, increase for more complex data</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast - simple mathematical operations</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multiple transformations</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for adaptive preprocessing</li> <li>Best For: Tabular data requiring sophisticated preprocessing strategies</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/differential-preprocessing-layer/#example-1-adaptive-preprocessing-analysis","title":"Example 1: Adaptive Preprocessing Analysis","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DifferentialPreprocessingLayer\n\n# Analyze which transformations are being used\ndef analyze_transformation_usage(model):\n    \"\"\"Analyze which transformations are being used most.\"\"\"\n    preprocessor = model.layers[0]  # First layer is the preprocessor\n\n    # Get learned combination weights\n    alpha_weights = preprocessor.alpha.numpy()\n\n    # Apply softmax to get probabilities\n    transformation_probs = keras.ops.softmax(alpha_weights, axis=0).numpy()\n\n    transformation_names = [\n        \"Identity\",\n        \"Affine\",\n        \"Nonlinear MLP\",\n        \"Log Transform\"\n    ]\n\n    print(\"Transformation Usage Probabilities:\")\n    print(\"=\" * 40)\n    for i, (name, prob) in enumerate(zip(transformation_names, transformation_probs)):\n        print(f\"{name}: {prob:.4f}\")\n\n    # Find most used transformation\n    most_used = np.argmax(transformation_probs)\n    print(f\"\\nMost used transformation: {transformation_names[most_used]}\")\n\n    return transformation_probs\n\n# Use with your trained model\n# probs = analyze_transformation_usage(model)\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#example-2-comparison-with-single-transformations","title":"Example 2: Comparison with Single Transformations","text":"<pre><code># Compare with single transformation approaches\ndef compare_preprocessing_approaches():\n    # Create data with missing values\n    data = np.random.normal(0, 1, (100, 5))\n    data[data &lt; -1] = np.nan  # Introduce missing values\n\n    # Single transformation approaches\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n    # Approach 1: Mean imputation + Standard scaling\n    imputer1 = SimpleImputer(strategy='mean')\n    scaler1 = StandardScaler()\n    data1 = scaler1.fit_transform(imputer1.fit_transform(data))\n\n    # Approach 2: Mean imputation + MinMax scaling\n    imputer2 = SimpleImputer(strategy='mean')\n    scaler2 = MinMaxScaler()\n    data2 = scaler2.fit_transform(imputer2.fit_transform(data))\n\n    # Approach 3: Differential preprocessing\n    inputs = keras.Input(shape=(5,))\n    x = DifferentialPreprocessingLayer(num_features=5, mlp_hidden_units=8)(inputs)\n    model = keras.Model(inputs, x)\n    data3 = model(keras.ops.convert_to_tensor(data, dtype=\"float32\")).numpy()\n\n    print(\"Preprocessing Comparison:\")\n    print(\"=\" * 50)\n    print(f\"Standard + Mean: Mean={np.mean(data1):.4f}, Std={np.std(data1):.4f}\")\n    print(f\"MinMax + Mean: Mean={np.mean(data2):.4f}, Std={np.std(data2):.4f}\")\n    print(f\"Differential: Mean={np.mean(data3):.4f}, Std={np.std(data3):.4f}\")\n\n    return data1, data2, data3\n\n# Compare approaches\n# std_data, minmax_data, diff_data = compare_preprocessing_approaches()\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#example-3-feature-specific-preprocessing","title":"Example 3: Feature-Specific Preprocessing","text":"<pre><code># Apply different preprocessing to different feature groups\ndef create_feature_specific_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Split features into groups\n    numerical_features = inputs[:, :10]    # First 10 features (numerical)\n    categorical_features = inputs[:, 10:15] # Next 5 features (categorical-like)\n    mixed_features = inputs[:, 15:20]      # Last 5 features (mixed)\n\n    # Apply different preprocessing to each group\n    numerical_preprocessed = DifferentialPreprocessingLayer(\n        num_features=10,\n        mlp_hidden_units=16\n    )(numerical_features)\n\n    categorical_preprocessed = DifferentialPreprocessingLayer(\n        num_features=5,\n        mlp_hidden_units=8\n    )(categorical_features)\n\n    mixed_preprocessed = DifferentialPreprocessingLayer(\n        num_features=5,\n        mlp_hidden_units=12\n    )(mixed_features)\n\n    # Combine preprocessed features\n    x = keras.layers.Concatenate()([\n        numerical_preprocessed,\n        categorical_preprocessed,\n        mixed_preprocessed\n    ])\n\n    # Process combined features\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_feature_specific_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/differential-preprocessing-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>MLP Size: Start with 4-8 hidden units, increase for complex data</li> <li>Feature Count: Must match the number of features in your dataset</li> <li>Missing Data: Works best with moderate amounts of missing data</li> <li>End-to-End Learning: Let the model learn optimal preprocessing</li> <li>Monitoring: Track transformation usage to understand preprocessing behavior</li> <li>Combination: Use with other preprocessing layers for complex pipelines</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, num_features)</li> <li>Feature Mismatch: num_features must match input dimension</li> <li>NaN Handling: Only handles NaN values, not other missing value representations</li> <li>Memory Usage: Creates multiple transformation branches</li> <li>Overfitting: Can overfit on small datasets with many features</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DifferentiableTabularPreprocessor - Simple differentiable preprocessing</li> <li>DistributionTransformLayer - Distribution transformation</li> <li>CastToFloat32Layer - Type casting utility</li> <li>FeatureCutout - Feature regularization</li> </ul>"},{"location":"layers/differential-preprocessing-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>End-to-End Learning in Deep Learning - End-to-end learning concepts</li> <li>Missing Data Handling - Missing data techniques</li> <li>Feature Transformation - Feature transformation methods</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/distribution-aware-encoder/","title":"\ud83d\udcca DistributionAwareEncoder\ud83d\udcca DistributionAwareEncoder","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/distribution-aware-encoder/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DistributionAwareEncoder</code> automatically detects the distribution type of input data and applies appropriate transformations and encodings. It builds upon the DistributionTransformLayer but adds sophisticated distribution detection and specialized encoding for different distribution types.</p> <p>This layer is particularly powerful for preprocessing data where the distribution characteristics are unknown or vary across features, providing intelligent adaptation to different data patterns.</p>"},{"location":"layers/distribution-aware-encoder/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DistributionAwareEncoder processes data through intelligent distribution-aware encoding:</p> <ol> <li>Distribution Detection: Analyzes input data to identify distribution type</li> <li>Transformation Selection: Chooses optimal transformation based on detected distribution</li> <li>Specialized Encoding: Applies distribution-specific encoding strategies</li> <li>Embedding Generation: Creates rich embeddings with optional distribution information</li> <li>Output Generation: Produces encoded features optimized for the detected distribution</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Distribution Detection]\n    B --&gt; C{Distribution Type}\n\n    C --&gt;|Normal| D[Normal Encoding]\n    C --&gt;|Exponential| E[Exponential Encoding]\n    C --&gt;|LogNormal| F[LogNormal Encoding]\n    C --&gt;|Uniform| G[Uniform Encoding]\n    C --&gt;|Beta| H[Beta Encoding]\n    C --&gt;|Bimodal| I[Bimodal Encoding]\n    C --&gt;|Heavy Tailed| J[Heavy Tailed Encoding]\n    C --&gt;|Mixed| K[Mixed Encoding]\n    C --&gt;|Unknown| L[Generic Encoding]\n\n    D --&gt; M[Transformation Layer]\n    E --&gt; M\n    F --&gt; M\n    G --&gt; M\n    H --&gt; M\n    I --&gt; M\n    J --&gt; M\n    K --&gt; M\n    L --&gt; M\n\n    M --&gt; N[Distribution Embedding]\n    N --&gt; O[Final Encoded Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style O fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/distribution-aware-encoder/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DistributionAwareEncoder's Solution Unknown Distributions One-size-fits-all preprocessing \ud83c\udfaf Automatic detection and adaptation to distribution type Mixed Data Types Uniform processing for all features \u26a1 Specialized encoding for different distribution types Distribution Changes Static preprocessing strategies \ud83e\udde0 Adaptive encoding that adjusts to data characteristics Feature Engineering Manual distribution analysis \ud83d\udd17 Automated preprocessing with learned distribution awareness"},{"location":"layers/distribution-aware-encoder/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Mixed Distribution Data: Datasets with features following different distributions</li> <li>Unknown Data Characteristics: When distribution types are not known in advance</li> <li>Adaptive Preprocessing: Systems that need to adapt to changing data patterns</li> <li>Feature Engineering: Automated creation of distribution-aware features</li> <li>Data Quality: Handling datasets with varying distribution quality</li> </ul>"},{"location":"layers/distribution-aware-encoder/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/distribution-aware-encoder/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DistributionAwareEncoder\n\n# Create sample data with different distributions\nbatch_size = 1000\n\n# Normal distribution\nnormal_data = np.random.normal(0, 1, (batch_size, 5))\n\n# Exponential distribution\nexp_data = np.random.exponential(1, (batch_size, 5))\n\n# Combine features\nmixed_data = np.concatenate([normal_data, exp_data], axis=1)\n\n# Apply distribution-aware encoding\nencoder = DistributionAwareEncoder(\n    embedding_dim=16,\n    add_distribution_embedding=True\n)\nencoded = encoder(mixed_data)\n\nprint(f\"Input shape: {mixed_data.shape}\")    # (1000, 10)\nprint(f\"Output shape: {encoded.shape}\")     # (1000, 16)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#automatic-detection","title":"Automatic Detection","text":"<pre><code># Let the layer automatically detect distributions\nauto_encoder = DistributionAwareEncoder(\n    embedding_dim=32,\n    auto_detect=True,  # Enable automatic detection\n    add_distribution_embedding=True\n)\n\n# Apply to unknown data\nunknown_data = keras.random.normal((100, 20))\nencoded = auto_encoder(unknown_data)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#manual-distribution-type","title":"Manual Distribution Type","text":"<pre><code># Specify distribution type manually\nmanual_encoder = DistributionAwareEncoder(\n    embedding_dim=24,\n    auto_detect=False,\n    distribution_type=\"exponential\",  # Specify distribution type\n    transform_type=\"log\"  # Specify transformation\n)\n\n# Apply to exponential data\nexp_data = keras.random.exponential(1, (100, 15))\nencoded = manual_encoder(exp_data)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DistributionAwareEncoder\n\nmodel = keras.Sequential([\n    DistributionAwareEncoder(\n        embedding_dim=32,\n        add_distribution_embedding=True\n    ),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DistributionAwareEncoder\n\n# Define inputs\ninputs = keras.Input(shape=(25,))  # 25 features\n\n# Apply distribution-aware encoding\nx = DistributionAwareEncoder(\n    embedding_dim=48,\n    auto_detect=True,\n    add_distribution_embedding=True\n)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(128, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.3)(x)\nx = keras.layers.Dense(64, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nencoder = DistributionAwareEncoder(\n    embedding_dim=64,                    # Higher embedding dimension\n    auto_detect=True,                    # Enable automatic detection\n    transform_type=\"auto\",               # Automatic transformation selection\n    add_distribution_embedding=True,     # Include distribution information\n    name=\"custom_distribution_encoder\"\n)\n\n# Use in a complex preprocessing pipeline\ninputs = keras.Input(shape=(50,))\n\n# Apply distribution-aware encoding\nx = encoder(inputs)\n\n# Multi-task processing\ntask1 = keras.layers.Dense(32, activation='relu')(x)\ntask1 = keras.layers.Dropout(0.2)(task1)\nclassification = keras.layers.Dense(5, activation='softmax', name='classification')(task1)\n\ntask2 = keras.layers.Dense(16, activation='relu')(x)\ntask2 = keras.layers.Dropout(0.1)(task2)\nregression = keras.layers.Dense(1, name='regression')(task2)\n\nmodel = keras.Model(inputs, [classification, regression])\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/distribution-aware-encoder/#kmr.layers.DistributionAwareEncoder","title":"kmr.layers.DistributionAwareEncoder","text":"<p>This module implements a DistributionAwareEncoder layer that automatically detects the distribution type of input data and applies appropriate transformations and encodings. It builds upon the DistributionTransformLayer but adds more sophisticated distribution detection and specialized encoding for different distribution types.</p>"},{"location":"layers/distribution-aware-encoder/#kmr.layers.DistributionAwareEncoder-classes","title":"Classes","text":""},{"location":"layers/distribution-aware-encoder/#kmr.layers.DistributionAwareEncoder.DistributionAwareEncoder","title":"DistributionAwareEncoder","text":"<pre><code>DistributionAwareEncoder(\n    embedding_dim: int | None = None,\n    auto_detect: bool = True,\n    distribution_type: str = \"unknown\",\n    transform_type: str = \"auto\",\n    add_distribution_embedding: bool = False,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Layer that automatically detects and encodes data based on its distribution.</p> <p>This layer first detects the distribution type of the input data and then applies appropriate transformations and encodings. It builds upon the DistributionTransformLayer but adds more sophisticated distribution detection and specialized encoding for different distribution types.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int | None</code> <p>Dimension of the output embedding. If None, the output will have the same dimension as the input. Default is None.</p> <code>None</code> <code>auto_detect</code> <code>bool</code> <p>Whether to automatically detect the distribution type. If False, the layer will use the specified distribution_type. Default is True.</p> <code>True</code> <code>distribution_type</code> <code>str</code> <p>The distribution type to use if auto_detect is False. Options are \"normal\", \"exponential\", \"lognormal\", \"uniform\", \"beta\", \"bimodal\", \"heavy_tailed\", \"mixed\", \"bounded\", \"unknown\". Default is \"unknown\".</p> <code>'unknown'</code> <code>transform_type</code> <code>str</code> <p>The transformation type to use. If \"auto\", the layer will automatically select the best transformation based on the detected distribution. See DistributionTransformLayer for available options. Default is \"auto\".</p> <code>'auto'</code> <code>add_distribution_embedding</code> <code>bool</code> <p>Whether to add a learned embedding of the distribution type to the output. Default is False.</p> <code>False</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>N-D tensor with shape: <code>(batch_size, ..., features)</code>.</p> Output shape <p>If embedding_dim is None, same shape as input: <code>(batch_size, ..., features)</code>. If embedding_dim is specified: <code>(batch_size, ..., embedding_dim)</code>. If add_distribution_embedding is True, the output will have an additional dimension for the distribution embedding.</p> Example <pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DistributionAwareEncoder\n\n# Create sample input data with different distributions\n# Normal distribution\nnormal_data = keras.ops.convert_to_tensor(\n    np.random.normal(0, 1, (100, 10)), dtype=\"float32\"\n)\n\n# Exponential distribution\nexp_data = keras.ops.convert_to_tensor(\n    np.random.exponential(1, (100, 10)), dtype=\"float32\"\n)\n\n# Create the encoder\nencoder = DistributionAwareEncoder(embedding_dim=16, add_distribution_embedding=True)\n\n# Apply to normal data\nnormal_encoded = encoder(normal_data)\nprint(\"Normal encoded shape:\", normal_encoded.shape)  # (100, 16)\n\n# Apply to exponential data\nexp_encoded = encoder(exp_data)\nprint(\"Exponential encoded shape:\", exp_encoded.shape)  # (100, 16)\n</code></pre> <p>Initialize the DistributionAwareEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int | None</code> <p>Embedding dimension.</p> <code>None</code> <code>auto_detect</code> <code>bool</code> <p>Whether to auto-detect distribution type.</p> <code>True</code> <code>distribution_type</code> <code>str</code> <p>Type of distribution.</p> <code>'unknown'</code> <code>transform_type</code> <code>str</code> <p>Type of transformation to apply.</p> <code>'auto'</code> <code>add_distribution_embedding</code> <code>bool</code> <p>Whether to add distribution embedding.</p> <code>False</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/distribution-aware-encoder/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/distribution-aware-encoder/#embedding_dim-int-optional","title":"<code>embedding_dim</code> (int, optional)","text":"<ul> <li>Purpose: Dimension of the output embedding</li> <li>Range: 8 to 256+ (typically 16-64)</li> <li>Impact: Higher values = richer representations but more parameters</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/distribution-aware-encoder/#auto_detect-bool","title":"<code>auto_detect</code> (bool)","text":"<ul> <li>Purpose: Whether to automatically detect distribution type</li> <li>Default: True</li> <li>Impact: Enables intelligent distribution detection</li> <li>Recommendation: Use True for unknown data, False for known distributions</li> </ul>"},{"location":"layers/distribution-aware-encoder/#distribution_type-str","title":"<code>distribution_type</code> (str)","text":"<ul> <li>Purpose: Distribution type to use if auto_detect is False</li> <li>Options: \"normal\", \"exponential\", \"lognormal\", \"uniform\", \"beta\", \"bimodal\", \"heavy_tailed\", \"mixed\", \"bounded\", \"unknown\"</li> <li>Default: \"unknown\"</li> <li>Impact: Determines encoding strategy</li> <li>Recommendation: Use specific type when you know the distribution</li> </ul>"},{"location":"layers/distribution-aware-encoder/#add_distribution_embedding-bool","title":"<code>add_distribution_embedding</code> (bool)","text":"<ul> <li>Purpose: Whether to add learned distribution type embedding</li> <li>Default: False</li> <li>Impact: Includes distribution information in output</li> <li>Recommendation: Use True for complex models that benefit from distribution awareness</li> </ul>"},{"location":"layers/distribution-aware-encoder/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium datasets, scales with embedding_dim</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to distribution detection and encoding</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for mixed-distribution data</li> <li>Best For: Tabular data with unknown or mixed distribution types</li> </ul>"},{"location":"layers/distribution-aware-encoder/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/distribution-aware-encoder/#example-1-mixed-distribution-data","title":"Example 1: Mixed Distribution Data","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DistributionAwareEncoder\n\n# Create data with different distributions\nbatch_size = 2000\n\n# Different distribution types\nnormal_features = np.random.normal(0, 1, (batch_size, 5))\nexponential_features = np.random.exponential(1, (batch_size, 5))\nuniform_features = np.random.uniform(-2, 2, (batch_size, 5))\nbeta_features = np.random.beta(2, 5, (batch_size, 5))\n\n# Combine all features\nmixed_data = np.concatenate([\n    normal_features, exponential_features, \n    uniform_features, beta_features\n], axis=1)\n\n# Build model with distribution-aware encoding\ninputs = keras.Input(shape=(20,))  # 20 mixed features\n\n# Apply distribution-aware encoding\nx = DistributionAwareEncoder(\n    embedding_dim=32,\n    auto_detect=True,\n    add_distribution_embedding=True\n)(inputs)\n\n# Process encoded features\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#example-2-time-series-with-varying-distributions","title":"Example 2: Time Series with Varying Distributions","text":"<pre><code># Process time series data with varying distributions over time\ndef create_time_series_model():\n    inputs = keras.Input(shape=(24, 10))  # 24 time steps, 10 features\n\n    # Apply distribution-aware encoding to each time step\n    x = keras.layers.TimeDistributed(\n        DistributionAwareEncoder(\n            embedding_dim=16,\n            auto_detect=True,\n            add_distribution_embedding=True\n        )\n    )(inputs)\n\n    # Process time series\n    x = keras.layers.LSTM(64, return_sequences=True)(x)\n    x = keras.layers.LSTM(32)(x)\n\n    # Multiple outputs\n    trend = keras.layers.Dense(1, name='trend')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [trend, anomaly])\n\nmodel = create_time_series_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'trend': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'trend': 1.0, 'anomaly': 0.5}\n)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#example-3-multi-modal-data-processing","title":"Example 3: Multi-Modal Data Processing","text":"<pre><code># Process different data modalities with distribution-aware encoding\ndef create_multi_modal_model():\n    # Different input modalities\n    numerical_input = keras.Input(shape=(15,), name='numerical')\n    sensor_input = keras.Input(shape=(10,), name='sensor')\n\n    # Apply distribution-aware encoding to each modality\n    numerical_encoded = DistributionAwareEncoder(\n        embedding_dim=24,\n        auto_detect=True,\n        add_distribution_embedding=True\n    )(numerical_input)\n\n    sensor_encoded = DistributionAwareEncoder(\n        embedding_dim=16,\n        auto_detect=True,\n        add_distribution_embedding=True\n    )(sensor_input)\n\n    # Combine modalities\n    combined = keras.layers.Concatenate()([numerical_encoded, sensor_encoded])\n\n    # Multi-task processing\n    x = keras.layers.Dense(64, activation='relu')(combined)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Different tasks\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model([numerical_input, sensor_input], [classification, regression])\n\nmodel = create_multi_modal_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/distribution-aware-encoder/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Auto Detection: Use auto_detect=True for unknown data distributions</li> <li>Distribution Embedding: Enable add_distribution_embedding for complex models</li> <li>Feature Preprocessing: Ensure features are properly scaled before encoding</li> <li>Embedding Dimension: Start with 16-32, scale based on data complexity</li> <li>Monitoring: Track distribution detection accuracy during training</li> <li>Data Quality: Works best with clean, well-preprocessed data</li> </ul>"},{"location":"layers/distribution-aware-encoder/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, num_features)</li> <li>Distribution Detection: May not work well with very small datasets</li> <li>Memory Usage: Scales with embedding_dim and distribution complexity</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Distribution Changes: May need retraining if data distribution changes significantly</li> </ul>"},{"location":"layers/distribution-aware-encoder/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DistributionTransformLayer - Distribution transformation</li> <li>AdvancedNumericalEmbedding - Advanced numerical embeddings</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> <li>CastToFloat32Layer - Type casting utility</li> </ul>"},{"location":"layers/distribution-aware-encoder/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Distribution Detection in Machine Learning - Distribution testing concepts</li> <li>Feature Encoding Techniques - Feature encoding approaches</li> <li>Adaptive Preprocessing - Adaptive data preprocessing</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/distribution-transform-layer/","title":"\ud83d\udcca DistributionTransformLayer\ud83d\udcca DistributionTransformLayer","text":"\ud83d\udd25 Popular \u2705 Stable \ud83d\udfe2 Beginner"},{"location":"layers/distribution-transform-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>DistributionTransformLayer</code> automatically transforms numerical features to improve their distribution characteristics, making them more suitable for neural network processing. This layer supports multiple transformation types including log, square root, Box-Cox, Yeo-Johnson, and more, with an intelligent 'auto' mode that selects the best transformation based on data characteristics.</p> <p>This layer is particularly valuable for preprocessing numerical data where the original distribution may not be optimal for neural network training, such as skewed distributions, heavy-tailed data, or features with varying scales.</p>"},{"location":"layers/distribution-transform-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The DistributionTransformLayer processes numerical features through intelligent transformation:</p> <ol> <li>Distribution Analysis: Analyzes input data characteristics (skewness, kurtosis, etc.)</li> <li>Transformation Selection: Chooses optimal transformation based on data properties</li> <li>Parameter Learning: Learns transformation parameters during training</li> <li>Data Transformation: Applies the selected transformation to normalize the data</li> <li>Output Generation: Returns transformed features with improved distribution</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Distribution Analysis]\n    B --&gt; C{Transform Type}\n\n    C --&gt;|Auto| D[Best Fit Selection]\n    C --&gt;|Manual| E[Specified Transform]\n\n    D --&gt; F[Log Transform]\n    D --&gt; G[Box-Cox Transform]\n    D --&gt; H[Yeo-Johnson Transform]\n    D --&gt; I[Other Transforms]\n\n    E --&gt; F\n    E --&gt; G\n    E --&gt; H\n    E --&gt; I\n\n    F --&gt; J[Transformed Features]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style D fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/distribution-transform-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach DistributionTransformLayer's Solution Skewed Data Manual transformation or ignore \ud83c\udfaf Automatic detection and transformation of skewed distributions Scale Differences Manual normalization \u26a1 Intelligent scaling based on data characteristics Distribution Types One-size-fits-all approach \ud83e\udde0 Adaptive transformation for different distribution types Preprocessing Complexity Manual feature engineering \ud83d\udd17 Automated preprocessing with learned parameters"},{"location":"layers/distribution-transform-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Financial Data: Transforming skewed financial metrics and ratios</li> <li>Medical Data: Normalizing lab values and health measurements</li> <li>Sensor Data: Preprocessing IoT and sensor readings</li> <li>Survey Data: Transforming rating scales and response distributions</li> <li>Time Series: Preprocessing numerical time series features</li> </ul>"},{"location":"layers/distribution-transform-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/distribution-transform-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import DistributionTransformLayer\n\n# Create sample data with skewed distribution\nbatch_size, num_features = 32, 10\nx = keras.random.exponential((batch_size, num_features))  # Exponential distribution\n\n# Apply automatic transformation\ntransformer = DistributionTransformLayer(transform_type='auto')\ntransformed = transformer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {transformed.shape}\")  # (32, 10)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#manual-transformation","title":"Manual Transformation","text":"<pre><code># Apply specific transformation\nlog_transformer = DistributionTransformLayer(transform_type='log')\nlog_transformed = log_transformer(x)\n\n# Box-Cox transformation\nbox_cox_transformer = DistributionTransformLayer(\n    transform_type='box-cox',\n    lambda_param=0.5\n)\nbox_cox_transformed = box_cox_transformer(x)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import DistributionTransformLayer\n\nmodel = keras.Sequential([\n    DistributionTransformLayer(transform_type='auto'),  # Preprocess data\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/distribution-transform-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import DistributionTransformLayer\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 numerical features\n\n# Apply distribution transformation\nx = DistributionTransformLayer(transform_type='yeo-johnson')(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\ntransformer = DistributionTransformLayer(\n    transform_type='auto',\n    epsilon=1e-8,                    # Custom epsilon for numerical stability\n    auto_candidates=['log', 'sqrt', 'box-cox', 'yeo-johnson'],  # Limited candidates\n    name=\"custom_distribution_transform\"\n)\n\n# Use in a complex preprocessing pipeline\ninputs = keras.Input(shape=(50,))\n\n# Multiple transformation strategies\nx1 = DistributionTransformLayer(transform_type='log')(inputs)\nx2 = DistributionTransformLayer(transform_type='yeo-johnson')(inputs)\n\n# Combine different transformations\nx = keras.layers.Concatenate()([x1, x2])\nx = keras.layers.Dense(128, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/distribution-transform-layer/#kmr.layers.DistributionTransformLayer","title":"kmr.layers.DistributionTransformLayer","text":"<p>This module implements a DistributionTransformLayer that applies various transformations to make data more normally distributed or to handle specific distribution types better. It's particularly useful for preprocessing data before anomaly detection or other statistical analyses.</p>"},{"location":"layers/distribution-transform-layer/#kmr.layers.DistributionTransformLayer-classes","title":"Classes","text":""},{"location":"layers/distribution-transform-layer/#kmr.layers.DistributionTransformLayer.DistributionTransformLayer","title":"DistributionTransformLayer","text":"<pre><code>DistributionTransformLayer(\n    transform_type: str = \"none\",\n    lambda_param: float = 0.0,\n    epsilon: float = 1e-10,\n    min_value: float = 0.0,\n    max_value: float = 1.0,\n    clip_values: bool = True,\n    auto_candidates: list[str] | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Layer for transforming data distributions to improve anomaly detection.</p> <p>This layer applies various transformations to make data more normally distributed or to handle specific distribution types better. Supported transformations include log, square root, Box-Cox, Yeo-Johnson, arcsinh, cube-root, logit, quantile, robust-scale, and min-max.</p> <p>When transform_type is set to 'auto', the layer automatically selects the most appropriate transformation based on the data characteristics during training.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>str</code> <p>Type of transformation to apply. Options are 'none', 'log', 'sqrt', 'box-cox', 'yeo-johnson', 'arcsinh', 'cube-root', 'logit', 'quantile', 'robust-scale', 'min-max', or 'auto'. Default is 'none'.</p> <code>'none'</code> <code>lambda_param</code> <code>float</code> <p>Parameter for parameterized transformations like Box-Cox and Yeo-Johnson. Default is 0.0.</p> <code>0.0</code> <code>epsilon</code> <code>float</code> <p>Small value added to prevent numerical issues like log(0). Default is 1e-10.</p> <code>1e-10</code> <code>min_value</code> <code>float</code> <p>Minimum value for min-max scaling. Default is 0.0.</p> <code>0.0</code> <code>max_value</code> <code>float</code> <p>Maximum value for min-max scaling. Default is 1.0.</p> <code>1.0</code> <code>clip_values</code> <code>bool</code> <p>Whether to clip values to the specified range in min-max scaling. Default is True.</p> <code>True</code> <code>auto_candidates</code> <code>list[str] | None</code> <p>list of transformation types to consider when transform_type is 'auto'. If None, all available transformations will be considered. Default is None.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>N-D tensor with shape: (batch_size, ..., features)</p> Output shape <p>Same shape as input: (batch_size, ..., features)</p> Example <pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DistributionTransformLayer\n\n# Create sample input data with skewed distribution\nx = keras.random.exponential((32, 10))  # 32 samples, 10 features\n\n# Apply log transformation\nlog_transform = DistributionTransformLayer(transform_type=\"log\")\ny = log_transform(x)\nprint(\"Transformed output shape:\", y.shape)  # (32, 10)\n\n# Apply Box-Cox transformation with lambda=0.5\nbox_cox = DistributionTransformLayer(transform_type=\"box-cox\", lambda_param=0.5)\nz = box_cox(x)\n\n# Apply arcsinh transformation (handles both positive and negative values)\narcsinh_transform = DistributionTransformLayer(transform_type=\"arcsinh\")\na = arcsinh_transform(x)\n\n# Apply min-max scaling to range [0, 1]\nmin_max = DistributionTransformLayer(transform_type=\"min-max\", min_value=0.0, max_value=1.0)\nb = min_max(x)\n\n# Use automatic transformation selection\nauto_transform = DistributionTransformLayer(transform_type=\"auto\")\nc = auto_transform(x)  # Will select the best transformation during training\n</code></pre> <p>Initialize the DistributionTransformLayer.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>str</code> <p>Type of transformation to apply.</p> <code>'none'</code> <code>lambda_param</code> <code>float</code> <p>Lambda parameter for Box-Cox transformation.</p> <code>0.0</code> <code>epsilon</code> <code>float</code> <p>Small value to avoid division by zero.</p> <code>1e-10</code> <code>min_value</code> <code>float</code> <p>Minimum value for clipping.</p> <code>0.0</code> <code>max_value</code> <code>float</code> <p>Maximum value for clipping.</p> <code>1.0</code> <code>clip_values</code> <code>bool</code> <p>Whether to clip values.</p> <code>True</code> <code>auto_candidates</code> <code>list[str] | None</code> <p>List of candidate transformations for auto mode.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/distribution-transform-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/distribution-transform-layer/#transform_type-str","title":"<code>transform_type</code> (str)","text":"<ul> <li>Purpose: Type of transformation to apply</li> <li>Options: 'none', 'log', 'sqrt', 'box-cox', 'yeo-johnson', 'arcsinh', 'cube-root', 'logit', 'quantile', 'robust-scale', 'min-max', 'auto'</li> <li>Default: 'none'</li> <li>Impact: Determines how the data is transformed</li> <li>Recommendation: Use 'auto' for automatic selection, specific types for known distributions</li> </ul>"},{"location":"layers/distribution-transform-layer/#lambda_param-float","title":"<code>lambda_param</code> (float)","text":"<ul> <li>Purpose: Parameter for Box-Cox and Yeo-Johnson transformations</li> <li>Range: -2.0 to 2.0 (typically 0.0 to 1.0)</li> <li>Impact: Controls the strength of the transformation</li> <li>Recommendation: Use 0.5 for moderate transformation, 0.0 for log-like behavior</li> </ul>"},{"location":"layers/distribution-transform-layer/#epsilon-float","title":"<code>epsilon</code> (float)","text":"<ul> <li>Purpose: Small value to prevent numerical issues</li> <li>Range: 1e-10 to 1e-6</li> <li>Impact: Prevents log(0) and division by zero errors</li> <li>Recommendation: Use 1e-8 for most cases, 1e-10 for very small values</li> </ul>"},{"location":"layers/distribution-transform-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple mathematical transformations</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for improving data distribution characteristics</li> <li>Best For: Numerical data with skewed or non-normal distributions</li> </ul>"},{"location":"layers/distribution-transform-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/distribution-transform-layer/#example-1-financial-data-preprocessing","title":"Example 1: Financial Data Preprocessing","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import DistributionTransformLayer\n\n# Simulate financial data with different distributions\nbatch_size = 1000\n\n# Income data (log-normal distribution)\nincome = np.random.lognormal(mean=10, sigma=1, size=(batch_size, 1))\n\n# Age data (normal distribution)\nage = np.random.normal(50, 15, size=(batch_size, 1))\n\n# Debt ratio (beta distribution)\ndebt_ratio = np.random.beta(2, 5, size=(batch_size, 1))\n\n# Combine features\nfinancial_data = np.concatenate([income, age, debt_ratio], axis=1)\n\n# Build preprocessing model\ninputs = keras.Input(shape=(3,))\n\n# Apply different transformations for different features\nincome_transformed = DistributionTransformLayer(transform_type='log')(inputs[:, :1])\nage_transformed = DistributionTransformLayer(transform_type='none')(inputs[:, 1:2])\ndebt_transformed = DistributionTransformLayer(transform_type='logit')(inputs[:, 2:3])\n\n# Combine transformed features\nx = keras.layers.Concatenate()([income_transformed, age_transformed, debt_transformed])\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/distribution-transform-layer/#example-2-sensor-data-preprocessing","title":"Example 2: Sensor Data Preprocessing","text":"<pre><code># Preprocess IoT sensor data with automatic transformation\ndef create_sensor_model():\n    inputs = keras.Input(shape=(10,))  # 10 sensor readings\n\n    # Automatic transformation selection\n    x = DistributionTransformLayer(transform_type='auto')(inputs)\n\n    # Additional preprocessing\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Multiple outputs\n    anomaly_score = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n    sensor_health = keras.layers.Dense(3, activation='softmax', name='health')(x)\n\n    return keras.Model(inputs, [anomaly_score, sensor_health])\n\nmodel = create_sensor_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'anomaly': 'binary_crossentropy', 'health': 'categorical_crossentropy'},\n    loss_weights={'anomaly': 1.0, 'health': 0.5}\n)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#example-3-survey-data-analysis","title":"Example 3: Survey Data Analysis","text":"<pre><code># Process survey data with different response scales\ndef create_survey_model():\n    inputs = keras.Input(shape=(15,))  # 15 survey questions\n\n    # Different transformations for different question types\n    # Likert scale (1-5) - no transformation needed\n    likert_questions = inputs[:, :5]\n\n    # Rating scale (0-10) - min-max scaling\n    rating_questions = DistributionTransformLayer(transform_type='min-max')(inputs[:, 5:10])\n\n    # Open-ended numerical - log transformation\n    numerical_questions = DistributionTransformLayer(transform_type='log')(inputs[:, 10:15])\n\n    # Combine all features\n    x = keras.layers.Concatenate()([likert_questions, rating_questions, numerical_questions])\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Survey analysis outputs\n    satisfaction = keras.layers.Dense(1, activation='sigmoid', name='satisfaction')(x)\n    category = keras.layers.Dense(5, activation='softmax', name='category')(x)\n\n    return keras.Model(inputs, [satisfaction, category])\n\nmodel = create_survey_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'satisfaction': 'binary_crossentropy', 'category': 'categorical_crossentropy'},\n    loss_weights={'satisfaction': 1.0, 'category': 0.3}\n)\n</code></pre>"},{"location":"layers/distribution-transform-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Auto Mode: Use 'auto' for unknown distributions, specific types for known patterns</li> <li>Data Validation: Check for negative values before applying log transformations</li> <li>Epsilon Tuning: Adjust epsilon based on your data's numerical precision</li> <li>Feature-Specific: Apply different transformations to different feature types</li> <li>Monitoring: Track transformation effects on model performance</li> <li>Inverse Transform: Consider if you need to inverse transform predictions</li> </ul>"},{"location":"layers/distribution-transform-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Negative Values: Log and sqrt transformations require non-negative values</li> <li>Zero Values: Use appropriate epsilon to handle zero values</li> <li>Overfitting: Don't over-transform - sometimes original distributions are fine</li> <li>Interpretability: Transformed features may be harder to interpret</li> <li>Inverse Transform: Remember to inverse transform if needed for predictions</li> </ul>"},{"location":"layers/distribution-transform-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DistributionAwareEncoder - Distribution-aware feature encoding</li> <li>AdvancedNumericalEmbedding - Advanced numerical embeddings</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> <li>CastToFloat32Layer - Type casting utility</li> </ul>"},{"location":"layers/distribution-transform-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Box-Cox Transformation - Box-Cox transformation details</li> <li>Yeo-Johnson Transformation - Yeo-Johnson transformation</li> <li>Data Preprocessing in Machine Learning - Data preprocessing concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/feature-cutout/","title":"\u2702\ufe0f FeatureCutout\u2702\ufe0f FeatureCutout","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/feature-cutout/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>FeatureCutout</code> layer randomly masks out (sets to zero) a specified fraction of features during training to improve model robustness and prevent overfitting. During inference, all features are kept intact, making it a powerful regularization technique.</p> <p>This layer is particularly effective for tabular data where feature interactions are complex and overfitting is a common concern. It forces the model to learn robust representations that don't rely on any single feature.</p>"},{"location":"layers/feature-cutout/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The FeatureCutout layer applies random masking during training:</p> <ol> <li>Training Mode Check: Only applies masking during training</li> <li>Random Mask Generation: Creates random mask based on cutout probability</li> <li>Feature Masking: Sets selected features to specified noise value</li> <li>Inference Passthrough: Returns original features during inference</li> <li>Output Generation: Produces masked or original features based on mode</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B{Training Mode?}\n    B --&gt;|Yes| C[Generate Random Mask]\n    B --&gt;|No| H[Return Original Features]\n\n    C --&gt; D[Apply Cutout Probability]\n    D --&gt; E[Create Binary Mask]\n    E --&gt; F[Apply Mask to Features]\n    F --&gt; G[Return Masked Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style H fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/feature-cutout/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach FeatureCutout's Solution Overfitting Dropout on hidden layers only \ud83c\udfaf Feature-level regularization prevents overfitting on input features Feature Dependencies Model may rely on specific features \u26a1 Forces robustness by randomly removing features Generalization Poor performance on unseen data \ud83e\udde0 Improves generalization through feature masking Data Augmentation Limited augmentation for tabular data \ud83d\udd17 Tabular data augmentation through feature masking"},{"location":"layers/feature-cutout/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Overfitting Prevention: Regularizing models prone to overfitting</li> <li>Feature Robustness: Ensuring models don't rely on specific features</li> <li>Data Augmentation: Augmenting tabular datasets during training</li> <li>Generalization: Improving model performance on unseen data</li> <li>Feature Importance: Understanding which features are truly important</li> </ul>"},{"location":"layers/feature-cutout/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/feature-cutout/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import FeatureCutout\n\n# Create sample input data\nbatch_size, feature_dim = 32, 10\nx = keras.random.normal((batch_size, feature_dim))\n\n# Apply feature cutout\ncutout = FeatureCutout(cutout_prob=0.2)\nmasked = cutout(x, training=True)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {masked.shape}\")     # (32, 10)\nprint(f\"Training mode: {cutout.training}\") # True\n</code></pre>"},{"location":"layers/feature-cutout/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import FeatureCutout\n\nmodel = keras.Sequential([\n    FeatureCutout(cutout_prob=0.15),  # Apply feature cutout first\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/feature-cutout/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import FeatureCutout\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply feature cutout\nx = FeatureCutout(cutout_prob=0.1)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/feature-cutout/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\ncutout = FeatureCutout(\n    cutout_prob=0.25,        # Higher cutout probability\n    noise_value=-1.0,        # Custom noise value instead of zero\n    seed=42,                 # Fixed seed for reproducibility\n    name=\"custom_feature_cutout\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50,))\n\n# Apply feature cutout\nx = cutout(inputs)\n\n# Multi-branch processing\nbranch1 = keras.layers.Dense(32, activation='relu')(x)\nbranch2 = keras.layers.Dense(32, activation='tanh')(x)\n\n# Combine branches\nx = keras.layers.Concatenate()([branch1, branch2])\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/feature-cutout/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/feature-cutout/#kmr.layers.FeatureCutout","title":"kmr.layers.FeatureCutout","text":"<p>Feature cutout regularization layer for neural networks.</p>"},{"location":"layers/feature-cutout/#kmr.layers.FeatureCutout-classes","title":"Classes","text":""},{"location":"layers/feature-cutout/#kmr.layers.FeatureCutout.FeatureCutout","title":"FeatureCutout","text":"<pre><code>FeatureCutout(\n    cutout_prob: float = 0.1,\n    noise_value: float = 0.0,\n    seed: int | None = None,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Feature cutout regularization layer.</p> <p>This layer randomly masks out (sets to zero) a specified fraction of features during training to improve model robustness and prevent overfitting. During inference, all features are kept intact.</p> Example <pre><code>from keras import random\nfrom kmr.layers import FeatureCutout\n\n# Create sample data\nbatch_size = 32\nfeature_dim = 10\ninputs = random.normal((batch_size, feature_dim))\n\n# Apply feature cutout\ncutout = FeatureCutout(cutout_prob=0.2)\nmasked_outputs = cutout(inputs, training=True)\n</code></pre> <p>Initialize feature cutout.</p> <p>Parameters:</p> Name Type Description Default <code>cutout_prob</code> <code>float</code> <p>Probability of masking each feature</p> <code>0.1</code> <code>noise_value</code> <code>float</code> <p>Value to use for masked features (default: 0.0)</p> <code>0.0</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If cutout_prob is not in [0, 1]</p>"},{"location":"layers/feature-cutout/#kmr.layers.FeatureCutout.FeatureCutout-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute output shape.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Input shape tuple</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Output shape tuple</p> from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; FeatureCutout\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>FeatureCutout</code> <p>FeatureCutout instance</p>"},{"location":"layers/feature-cutout/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/feature-cutout/#cutout_prob-float","title":"<code>cutout_prob</code> (float)","text":"<ul> <li>Purpose: Probability of masking each feature</li> <li>Range: 0.0 to 1.0 (typically 0.05-0.3)</li> <li>Impact: Higher values = more aggressive regularization</li> <li>Recommendation: Start with 0.1-0.2, adjust based on overfitting</li> </ul>"},{"location":"layers/feature-cutout/#noise_value-float","title":"<code>noise_value</code> (float)","text":"<ul> <li>Purpose: Value to use for masked features</li> <li>Default: 0.0</li> <li>Impact: Affects how masked features are represented</li> <li>Recommendation: Use 0.0 for most cases, -1.0 for normalized data</li> </ul>"},{"location":"layers/feature-cutout/#seed-int-optional","title":"<code>seed</code> (int, optional)","text":"<ul> <li>Purpose: Random seed for reproducibility</li> <li>Default: None (random)</li> <li>Impact: Controls randomness of masking</li> <li>Recommendation: Use fixed seed for reproducible experiments</li> </ul>"},{"location":"layers/feature-cutout/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple masking operation</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Good for preventing overfitting</li> <li>Best For: Tabular data where overfitting is a concern</li> </ul>"},{"location":"layers/feature-cutout/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/feature-cutout/#example-1-overfitting-prevention","title":"Example 1: Overfitting Prevention","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import FeatureCutout\n\n# Create a model prone to overfitting\ndef create_overfitting_model():\n    inputs = keras.Input(shape=(100,))  # High-dimensional input\n\n    # Apply feature cutout to prevent overfitting\n    x = FeatureCutout(cutout_prob=0.2)(inputs)\n\n    # Deep network\n    x = keras.layers.Dense(256, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_overfitting_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train with feature cutout\n# model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100)\n</code></pre>"},{"location":"layers/feature-cutout/#example-2-progressive-feature-cutout","title":"Example 2: Progressive Feature Cutout","text":"<pre><code># Apply different cutout probabilities to different feature groups\ndef create_progressive_cutout_model():\n    inputs = keras.Input(shape=(30,))\n\n    # Split features into groups\n    important_features = inputs[:, :10]    # First 10 features (important)\n    regular_features = inputs[:, 10:25]    # Next 15 features (regular)\n    noisy_features = inputs[:, 25:30]      # Last 5 features (potentially noisy)\n\n    # Apply different cutout probabilities\n    important_cutout = FeatureCutout(cutout_prob=0.05)(important_features)  # Low cutout\n    regular_cutout = FeatureCutout(cutout_prob=0.15)(regular_features)      # Medium cutout\n    noisy_cutout = FeatureCutout(cutout_prob=0.3)(noisy_features)           # High cutout\n\n    # Combine features\n    x = keras.layers.Concatenate()([important_cutout, regular_cutout, noisy_cutout])\n\n    # Process combined features\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_progressive_cutout_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/feature-cutout/#example-3-feature-importance-analysis","title":"Example 3: Feature Importance Analysis","text":"<pre><code># Analyze which features are most affected by cutout\ndef analyze_feature_importance(model, test_data, feature_names=None):\n    \"\"\"Analyze feature importance by measuring performance drop when features are cut out.\"\"\"\n    # Get baseline performance\n    baseline_pred = model(test_data)\n\n    # Test each feature individually\n    feature_importance = []\n\n    for i in range(test_data.shape[1]):\n        # Create copy of test data\n        test_copy = test_data.numpy().copy()\n\n        # Mask feature i\n        test_copy[:, i] = 0.0\n\n        # Get prediction with masked feature\n        masked_pred = model(keras.ops.convert_to_tensor(test_copy))\n\n        # Calculate importance as difference in prediction\n        importance = keras.ops.mean(keras.ops.abs(baseline_pred - masked_pred))\n        feature_importance.append(importance.numpy())\n\n    # Sort features by importance\n    sorted_indices = np.argsort(feature_importance)[::-1]\n\n    print(\"Feature importance (based on cutout sensitivity):\")\n    for i, idx in enumerate(sorted_indices[:10]):  # Top 10 features\n        feature_name = feature_names[idx] if feature_names else f\"Feature_{idx}\"\n        print(f\"{i+1}. {feature_name}: {feature_importance[idx]:.4f}\")\n\n    return feature_importance\n\n# Use with your model\n# importance_scores = analyze_feature_importance(model, test_data, feature_names)\n</code></pre>"},{"location":"layers/feature-cutout/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Cutout Probability: Start with 0.1-0.2, increase if overfitting persists</li> <li>Feature Groups: Apply different cutout probabilities to different feature types</li> <li>Seed Setting: Use fixed seed for reproducible experiments</li> <li>Noise Value: Choose noise value based on your data distribution</li> <li>Monitoring: Track validation performance to tune cutout probability</li> <li>Combination: Use with other regularization techniques (dropout, batch norm)</li> </ul>"},{"location":"layers/feature-cutout/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, feature_dim)</li> <li>Training Mode: Only applies masking during training</li> <li>Over-regularization: Too high cutout_prob can hurt performance</li> <li>Feature Dependencies: May not work well if features are highly correlated</li> <li>Memory Usage: Creates temporary masks during training</li> </ul>"},{"location":"layers/feature-cutout/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedFeatureSelection - Feature selection mechanism</li> <li>VariableSelection - Dynamic feature selection</li> <li>SparseAttentionWeighting - Sparse attention weighting</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> </ul>"},{"location":"layers/feature-cutout/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Dropout Regularization - Original dropout paper</li> <li>Data Augmentation Techniques - Data augmentation concepts</li> <li>Regularization in Deep Learning - Regularization techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/gated-feature-fusion/","title":"\ud83d\udd00 GatedFeatureFusion\ud83d\udd00 GatedFeatureFusion","text":"\ud83d\udd25 Popular \u2705 Stable \ud83d\udfe1 Intermediate"},{"location":"layers/gated-feature-fusion/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>GatedFeatureFusion</code> layer intelligently combines two feature representations using a learned gating mechanism. This is particularly useful when you have multiple representations of the same data (e.g., raw numerical features alongside their embeddings) and want to learn the optimal way to combine them.</p> <p>The layer uses a learned gate to balance the contributions of both representations, allowing the model to dynamically decide how much to rely on each representation based on the input context.</p>"},{"location":"layers/gated-feature-fusion/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The GatedFeatureFusion layer processes two feature representations through a sophisticated fusion mechanism:</p> <ol> <li>Input Concatenation: Combines both feature representations</li> <li>Gate Learning: Uses a dense layer to learn fusion weights</li> <li>Sigmoid Activation: Applies sigmoid to create gating values between 0 and 1</li> <li>Weighted Fusion: Combines representations using learned gates</li> <li>Output Generation: Produces the fused feature representation</li> </ol> <pre><code>graph TD\n    A[Feature Representation 1] --&gt; C[Concatenation]\n    B[Feature Representation 2] --&gt; C\n    C --&gt; D[Fusion Gate Network]\n    D --&gt; E[Sigmoid Activation]\n    E --&gt; F[Gate Weights]\n\n    A --&gt; G[Element-wise Multiplication]\n    F --&gt; G\n    B --&gt; H[Element-wise Multiplication]\n    F --&gt; H\n\n    G --&gt; I[Weighted Rep 1]\n    H --&gt; J[Weighted Rep 2]\n\n    I --&gt; K[Addition]\n    J --&gt; K\n    K --&gt; L[Fused Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style B fill:#fff9e6,stroke:#ffb74d\n    style L fill:#e8f5e9,stroke:#66bb6a\n    style D fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/gated-feature-fusion/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach GatedFeatureFusion's Solution Multiple Representations Simple concatenation or averaging \ud83c\udfaf Learned fusion that adapts to input context Feature Redundancy Treat all features equally \u26a1 Intelligent weighting to balance contributions Representation Quality No adaptation to representation quality \ud83e\udde0 Dynamic gating based on representation relevance Information Loss Fixed combination strategies \ud83d\udd17 Preserves information from both representations"},{"location":"layers/gated-feature-fusion/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Multi-Modal Data: Combining different data types (numerical + categorical embeddings)</li> <li>Feature Engineering: Fusing raw features with engineered features</li> <li>Ensemble Methods: Combining different model representations</li> <li>Transfer Learning: Fusing pre-trained features with task-specific features</li> <li>Data Augmentation: Combining original and augmented feature representations</li> </ul>"},{"location":"layers/gated-feature-fusion/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/gated-feature-fusion/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureFusion\n\n# Create two feature representations\nbatch_size, num_features = 32, 10\nfeatures1 = keras.random.normal((batch_size, num_features))  # Raw features\nfeatures2 = keras.random.normal((batch_size, num_features))  # Processed features\n\n# Apply gated fusion\nfusion = GatedFeatureFusion()\nfused_features = fusion([features1, features2])\n\nprint(f\"Fused features shape: {fused_features.shape}\")  # (32, 10)\n</code></pre>"},{"location":"layers/gated-feature-fusion/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureFusion\n\n# Create a model with feature fusion\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    # Split into two representations\n    keras.layers.Lambda(lambda x: [x, x]),  # For demo - normally you'd have different paths\n    GatedFeatureFusion(),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/gated-feature-fusion/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureFusion\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Create two different representations\nraw_features = keras.layers.Dense(32, activation='relu')(inputs)\nprocessed_features = keras.layers.Dense(32, activation='tanh')(inputs)\n\n# Apply gated fusion\nfused = GatedFeatureFusion()([raw_features, processed_features])\n\n# Continue processing\nx = keras.layers.Dense(16, activation='relu')(fused)\nx = keras.layers.Dropout(0.2)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-feature-fusion/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom activation\nfusion = GatedFeatureFusion(\n    activation='tanh',  # Custom activation for the gate\n    name=\"custom_fusion\"\n)\n\n# Use in a complex multi-representation model\ninputs = keras.Input(shape=(50,))\n\n# Multiple feature processing paths\npath1 = keras.layers.Dense(64, activation='relu')(inputs)\npath1 = keras.layers.Dense(32, activation='relu')(path1)\n\npath2 = keras.layers.Dense(64, activation='tanh')(inputs)\npath2 = keras.layers.Dense(32, activation='sigmoid')(path2)\n\n# Fuse the representations\nfused = fusion([path1, path2])\n\n# Final processing\nx = keras.layers.Dense(16, activation='relu')(fused)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-feature-fusion/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/gated-feature-fusion/#kmr.layers.GatedFeatureFusion","title":"kmr.layers.GatedFeatureFusion","text":"<p>This module implements a GatedFeatureFusion layer that combines two feature representations through a learned gating mechanism. It's particularly useful for tabular datasets with multiple representations (e.g., raw numeric features alongside embeddings).</p>"},{"location":"layers/gated-feature-fusion/#kmr.layers.GatedFeatureFusion-classes","title":"Classes","text":""},{"location":"layers/gated-feature-fusion/#kmr.layers.GatedFeatureFusion.GatedFeatureFusion","title":"GatedFeatureFusion","text":"<pre><code>GatedFeatureFusion(\n    activation: str = \"sigmoid\",\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Gated feature fusion layer for combining two feature representations.</p> <p>This layer takes two inputs (e.g., numerical features and their embeddings) and fuses them using a learned gate to balance their contributions. The gate is computed using a dense layer with sigmoid activation, applied to the concatenation of both inputs.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Activation function to use for the gate. Default is 'sigmoid'.</p> <code>'sigmoid'</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>A list of 2 tensors with shape: <code>[(batch_size, ..., features), (batch_size, ..., features)]</code> Both inputs must have the same shape.</p> Output shape <p>Tensor with shape: <code>(batch_size, ..., features)</code>, same as each input.</p> Example <pre><code>import keras\nfrom kmr.layers import GatedFeatureFusion\n\n# Two representations for the same 10 features\nfeat1 = keras.random.normal((32, 10))\nfeat2 = keras.random.normal((32, 10))\n\nfusion_layer = GatedFeatureFusion()\nfused = fusion_layer([feat1, feat2])\nprint(\"Fused output shape:\", fused.shape)  # Expected: (32, 10)\n</code></pre> <p>Initialize the GatedFeatureFusion layer.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Activation function for the gate.</p> <code>'sigmoid'</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/gated-feature-fusion/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/gated-feature-fusion/#activation-str","title":"<code>activation</code> (str)","text":"<ul> <li>Purpose: Activation function for the fusion gate</li> <li>Options: 'sigmoid', 'tanh', 'relu', 'softmax', etc.</li> <li>Default: 'sigmoid'</li> <li>Impact: Controls the gating behavior and output range</li> <li>Recommendation: Use 'sigmoid' for balanced fusion, 'tanh' for signed gating</li> </ul>"},{"location":"layers/gated-feature-fusion/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple dense layer computation</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for multi-representation fusion</li> <li>Best For: Tabular data with multiple feature representations</li> </ul>"},{"location":"layers/gated-feature-fusion/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/gated-feature-fusion/#example-1-numerical-categorical-embeddings","title":"Example 1: Numerical + Categorical Embeddings","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureFusion\n\n# Simulate mixed data: numerical features + categorical embeddings\nbatch_size = 32\nnumerical_features = keras.random.normal((batch_size, 10))  # 10 numerical features\ncategorical_embeddings = keras.random.normal((batch_size, 10))  # 10-dim categorical embeddings\n\n# Build fusion model\nnum_input = keras.Input(shape=(10,), name='numerical')\ncat_input = keras.Input(shape=(10,), name='categorical')\n\n# Process each type\nnum_processed = keras.layers.Dense(16, activation='relu')(num_input)\ncat_processed = keras.layers.Dense(16, activation='relu')(cat_input)\n\n# Fuse representations\nfused = GatedFeatureFusion()([num_processed, cat_processed])\n\n# Final prediction\nx = keras.layers.Dense(32, activation='relu')(fused)\nx = keras.layers.Dropout(0.2)(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model([num_input, cat_input], output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/gated-feature-fusion/#example-2-multi-scale-feature-fusion","title":"Example 2: Multi-Scale Feature Fusion","text":"<pre><code># Fuse features at different scales/resolutions\ndef create_multi_scale_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Fine-grained features\n    fine_features = keras.layers.Dense(64, activation='relu')(inputs)\n    fine_features = keras.layers.Dense(32, activation='relu')(fine_features)\n\n    # Coarse-grained features\n    coarse_features = keras.layers.Dense(32, activation='relu')(inputs)\n    coarse_features = keras.layers.Dense(32, activation='relu')(coarse_features)\n\n    # Fuse different scales\n    fused = GatedFeatureFusion()([fine_features, coarse_features])\n\n    # Multi-task output\n    task1 = keras.layers.Dense(16, activation='relu')(fused)\n    task1 = keras.layers.Dense(3, activation='softmax', name='classification')(task1)\n\n    task2 = keras.layers.Dense(8, activation='relu')(fused)\n    task2 = keras.layers.Dense(1, name='regression')(task2)\n\n    return keras.Model(inputs, [task1, task2])\n\nmodel = create_multi_scale_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/gated-feature-fusion/#example-3-ensemble-feature-fusion","title":"Example 3: Ensemble Feature Fusion","text":"<pre><code># Fuse features from different models/representations\ndef create_ensemble_fusion_model():\n    inputs = keras.Input(shape=(15,))\n\n    # Model 1 representation\n    model1_features = keras.layers.Dense(32, activation='relu')(inputs)\n    model1_features = keras.layers.Dense(16, activation='relu')(model1_features)\n\n    # Model 2 representation\n    model2_features = keras.layers.Dense(32, activation='tanh')(inputs)\n    model2_features = keras.layers.Dense(16, activation='sigmoid')(model2_features)\n\n    # Fuse ensemble representations\n    ensemble_fused = GatedFeatureFusion()([model1_features, model2_features])\n\n    # Final prediction\n    x = keras.layers.Dense(32, activation='relu')(ensemble_fused)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    output = keras.layers.Dense(1, activation='sigmoid')(output)\n\n    return keras.Model(inputs, output)\n\nmodel = create_ensemble_fusion_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/gated-feature-fusion/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Representation Quality: Ensure both representations are meaningful and complementary</li> <li>Feature Alignment: Both inputs must have the same feature dimension</li> <li>Activation Choice: Use 'sigmoid' for balanced fusion, 'tanh' for signed gating</li> <li>Regularization: Combine with dropout to prevent overfitting</li> <li>Interpretability: Monitor gate values to understand fusion behavior</li> <li>Data Preprocessing: Ensure both representations are properly normalized</li> </ul>"},{"location":"layers/gated-feature-fusion/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Shape Mismatch: Both inputs must have identical shapes</li> <li>Input Format: Must provide exactly two inputs as a list</li> <li>Representation Quality: Poor representations will lead to poor fusion</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Gate Interpretation: Gate values are relative, not absolute importance</li> </ul>"},{"location":"layers/gated-feature-fusion/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedFeatureSelection - Gated feature selection mechanism</li> <li>VariableSelection - Dynamic feature selection</li> <li>AdvancedNumericalEmbedding - Advanced numerical embeddings</li> <li>TabularAttention - Attention-based feature processing</li> </ul>"},{"location":"layers/gated-feature-fusion/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Gated Residual Networks - GRN architecture details</li> <li>Feature Fusion in Deep Learning - Feature fusion concepts</li> <li>Multi-Modal Learning - Multi-modal learning approaches</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/gated-feature-selection/","title":"\ud83c\udfaf GatedFeatureSelection\ud83c\udfaf GatedFeatureSelection","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/gated-feature-selection/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>GatedFeatureSelection</code> layer implements a learnable feature selection mechanism using a sophisticated gating network. Each feature is assigned a dynamic importance weight between 0 and 1 through a multi-layer gating network that includes batch normalization and ReLU activations for stable training.</p> <p>This layer is particularly powerful for dynamic feature importance learning, feature selection in time-series data, and implementing attention-like mechanisms for tabular data. It includes a small residual connection to maintain gradient flow and prevent information loss.</p>"},{"location":"layers/gated-feature-selection/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The GatedFeatureSelection layer processes features through a sophisticated gating mechanism:</p> <ol> <li>Feature Analysis: Analyzes all input features to understand their importance</li> <li>Gating Network: Uses a multi-layer network to compute feature weights</li> <li>Weight Generation: Produces sigmoid-activated weights between 0 and 1</li> <li>Residual Connection: Adds a small residual connection for gradient flow</li> <li>Weighted Output: Applies learned weights to scale feature importance</li> </ol> <pre><code>graph TD\n    A[Input Features: batch_size, input_dim] --&gt; B[Gating Network]\n    B --&gt; C[Hidden Layer 1 + ReLU + BatchNorm]\n    C --&gt; D[Hidden Layer 2 + ReLU + BatchNorm]\n    D --&gt; E[Output Layer + Sigmoid]\n    E --&gt; F[Feature Weights: 0-1]\n\n    A --&gt; G[Element-wise Multiplication]\n    F --&gt; G\n    A --&gt; H[Residual Connection \u00d7 0.1]\n    G --&gt; I[Weighted Features]\n    H --&gt; I\n    I --&gt; J[Final Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style F fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/gated-feature-selection/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach GatedFeatureSelection's Solution Feature Importance Manual feature selection or uniform treatment \ud83c\udfaf Automatic learning of feature importance through gating Dynamic Selection Static feature selection decisions \u26a1 Context-aware selection that adapts to input Gradient Flow Potential vanishing gradients in selection \ud83d\udd17 Residual connection maintains gradient flow Noise Reduction All features treated equally \ud83e\udde0 Intelligent filtering of less important features"},{"location":"layers/gated-feature-selection/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Time Series Analysis: Dynamic feature selection for different time periods</li> <li>Noise Reduction: Filtering out irrelevant or noisy features</li> <li>Feature Engineering: Learning which features are most important</li> <li>Attention Mechanisms: Implementing attention-like behavior for tabular data</li> <li>High-Dimensional Data: Intelligently reducing feature space</li> </ul>"},{"location":"layers/gated-feature-selection/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/gated-feature-selection/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureSelection\n\n# Create sample input data\nbatch_size, input_dim = 32, 20\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply gated feature selection\ngated_selection = GatedFeatureSelection(input_dim=input_dim, reduction_ratio=4)\nselected_features = gated_selection(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 20)\nprint(f\"Output shape: {selected_features.shape}\")  # (32, 20)\n</code></pre>"},{"location":"layers/gated-feature-selection/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureSelection\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    GatedFeatureSelection(input_dim=64, reduction_ratio=8),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/gated-feature-selection/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedFeatureSelection\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Process features\nx = keras.layers.Dense(64, activation='relu')(inputs)\nx = GatedFeatureSelection(input_dim=64, reduction_ratio=4)(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-feature-selection/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom reduction ratio\ngated_selection = GatedFeatureSelection(\n    input_dim=128,\n    reduction_ratio=16,  # More aggressive reduction\n    name=\"custom_gated_selection\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50,))\nx = keras.layers.Dense(128, activation='relu')(inputs)\nx = keras.layers.BatchNormalization()(x)\nx = gated_selection(x)  # Apply gated selection\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-feature-selection/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/gated-feature-selection/#kmr.layers.GatedFeatureSelection","title":"kmr.layers.GatedFeatureSelection","text":"<pre><code>GatedFeatureSelection(\n    input_dim: int,\n    reduction_ratio: int = 4,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Gated feature selection layer with residual connection.</p> <p>This layer implements a learnable feature selection mechanism using a gating network. Each feature is assigned a dynamic importance weight between 0 and 1 through a multi-layer gating network. The gating network includes batch normalization and ReLU activations for stable training. A small residual connection (0.1) is added to maintain gradient flow.</p> <p>The layer is particularly useful for: 1. Dynamic feature importance learning 2. Feature selection in time-series data 3. Attention-like mechanisms for tabular data 4. Reducing noise in input features</p> <p>Example: <pre><code>import numpy as np\nfrom keras import layers, Model\nfrom kmr.layers import GatedFeatureSelection\n\n# Create sample input data\ninput_dim = 20\nx = np.random.normal(size=(100, input_dim))\n\n# Build model with gated feature selection\ninputs = layers.Input(shape=(input_dim,))\nx = GatedFeatureSelection(input_dim=input_dim, reduction_ratio=4)(inputs)\noutputs = layers.Dense(1)(x)\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# The layer will learn which features are most important\n# and dynamically adjust their contribution to the output\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features</p> required <code>reduction_ratio</code> <code>int</code> <p>Ratio to reduce the hidden dimension of the gating network. A higher ratio means fewer parameters but potentially less expressive gates. Default is 4, meaning the hidden dimension will be input_dim // 4.</p> <code>4</code> <p>Initialize the gated feature selection layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features. Must match the last dimension of the input tensor.</p> required <code>reduction_ratio</code> <code>int</code> <p>Ratio to reduce the hidden dimension of the gating network. The hidden dimension will be max(input_dim // reduction_ratio, 1). Default is 4.</p> <code>4</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments passed to the parent Layer class.</p> <code>{}</code>"},{"location":"layers/gated-feature-selection/#kmr.layers.GatedFeatureSelection-functions","title":"Functions","text":""},{"location":"layers/gated-feature-selection/#kmr.layers.GatedFeatureSelection.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[str, Any]\n) -&gt; GatedFeatureSelection\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>GatedFeatureSelection</code> <p>GatedFeatureSelection instance</p>"},{"location":"layers/gated-feature-selection/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/gated-feature-selection/#input_dim-int","title":"<code>input_dim</code> (int)","text":"<ul> <li>Purpose: Dimension of the input features</li> <li>Range: 1 to 1000+ (typically 10-100)</li> <li>Impact: Must match the last dimension of your input tensor</li> <li>Recommendation: Set to the output dimension of your previous layer</li> </ul>"},{"location":"layers/gated-feature-selection/#reduction_ratio-int","title":"<code>reduction_ratio</code> (int)","text":"<ul> <li>Purpose: Ratio to reduce the hidden dimension of the gating network</li> <li>Range: 2 to 32+ (typically 4-16)</li> <li>Impact: Higher ratio = fewer parameters but potentially less expressive gates</li> <li>Recommendation: Start with 4, increase for more aggressive feature selection</li> </ul>"},{"location":"layers/gated-feature-selection/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast - simple neural network computation</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Good for feature importance and noise reduction</li> <li>Best For: Tabular data where feature importance varies by context</li> </ul>"},{"location":"layers/gated-feature-selection/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/gated-feature-selection/#example-1-time-series-feature-selection","title":"Example 1: Time Series Feature Selection","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import GatedFeatureSelection\n\n# Simulate time series data with varying feature importance\nbatch_size, time_steps, features = 32, 24, 15\ntime_series_data = keras.random.normal((batch_size, time_steps, features))\n\n# Build time series model with gated selection\ninputs = keras.Input(shape=(time_steps, features))\n\n# Process each time step\nx = keras.layers.Dense(32, activation='relu')(inputs)\nx = keras.layers.Reshape((-1, 32))(x)  # Flatten time dimension\n\n# Apply gated feature selection\nx = GatedFeatureSelection(input_dim=32, reduction_ratio=8)(x)\n\n# Reshape back and process\nx = keras.layers.Reshape((time_steps, 32))(x)\nx = keras.layers.LSTM(64, return_sequences=True)(x)\nx = keras.layers.LSTM(32)(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/gated-feature-selection/#example-2-multi-task-feature-selection","title":"Example 2: Multi-Task Feature Selection","text":"<pre><code># Different tasks may need different feature selections\ndef create_multi_task_model():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Shared feature processing with gated selection\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = GatedFeatureSelection(input_dim=64, reduction_ratio=4)(x)\n\n    # Task-specific processing\n    # Classification task\n    cls_features = keras.layers.Dense(32, activation='relu')(x)\n    cls_features = keras.layers.Dropout(0.3)(cls_features)\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(cls_features)\n\n    # Regression task\n    reg_features = keras.layers.Dense(16, activation='relu')(x)\n    reg_features = keras.layers.Dropout(0.2)(reg_features)\n    regression = keras.layers.Dense(1, name='regression')(reg_features)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_multi_task_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/gated-feature-selection/#example-3-feature-importance-analysis","title":"Example 3: Feature Importance Analysis","text":"<pre><code># Analyze which features are being selected\ndef analyze_feature_selection(model, test_data, feature_names=None):\n    \"\"\"Analyze feature selection patterns.\"\"\"\n    # Get the gated feature selection layer\n    gated_layer = None\n    for layer in model.layers:\n        if isinstance(layer, GatedFeatureSelection):\n            gated_layer = layer\n            break\n\n    if gated_layer is None:\n        print(\"No GatedFeatureSelection layer found\")\n        return\n\n    # Get feature weights\n    weights = gated_layer.gate_net(test_data)\n\n    # Analyze weights\n    avg_weights = np.mean(weights, axis=0)\n    print(\"Average feature weights:\")\n    for i, weight in enumerate(avg_weights):\n        feature_name = feature_names[i] if feature_names else f\"Feature_{i}\"\n        print(f\"{feature_name}: {weight:.4f}\")\n\n    # Find most important features\n    top_features = np.argsort(avg_weights)[-5:]  # Top 5 features\n    print(f\"\\nTop 5 most important features: {top_features}\")\n\n    return weights\n\n# Use with your model\n# weights = analyze_feature_selection(model, test_data, feature_names)\n</code></pre>"},{"location":"layers/gated-feature-selection/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Reduction Ratio: Start with 4, adjust based on feature complexity and model size</li> <li>Residual Connection: The 0.1 residual connection helps maintain gradient flow</li> <li>Batch Normalization: The gating network includes batch norm for stable training</li> <li>Feature Preprocessing: Ensure features are properly normalized before selection</li> <li>Monitoring: Track feature weights to understand selection patterns</li> <li>Regularization: Combine with dropout to prevent overfitting</li> </ul>"},{"location":"layers/gated-feature-selection/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Dimension: Must match the last dimension of your input tensor</li> <li>Reduction Ratio: Too high can lead to underfitting, too low to overfitting</li> <li>Gradient Flow: The residual connection helps but monitor for vanishing gradients</li> <li>Feature Interpretation: Weights are relative, not absolute importance</li> <li>Memory Usage: Scales with input_dim, be careful with very large feature spaces</li> </ul>"},{"location":"layers/gated-feature-selection/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>VariableSelection - Dynamic feature selection using GRNs</li> <li>ColumnAttention - Column-wise attention mechanism</li> <li>TabularAttention - General tabular attention</li> <li>SparseAttentionWeighting - Sparse attention weights</li> </ul>"},{"location":"layers/gated-feature-selection/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Mechanisms in Deep Learning - Understanding attention mechanisms</li> <li>Feature Selection in Machine Learning - Feature selection concepts</li> <li>Gated Networks - Gated network architectures</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/gated-linear-unit/","title":"\ud83d\udeaa GatedLinearUnit\ud83d\udeaa GatedLinearUnit","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/gated-linear-unit/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>GatedLinearUnit</code> applies a gated linear transformation to input tensors, controlling information flow in neural networks. It multiplies the output of a dense linear transformation with the output of a dense sigmoid transformation, creating a gating mechanism that filters information based on learned weights and biases.</p> <p>This layer is particularly powerful for controlling information flow, implementing attention-like mechanisms, and creating sophisticated feature transformations in neural networks.</p>"},{"location":"layers/gated-linear-unit/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The GatedLinearUnit processes data through a gated transformation:</p> <ol> <li>Linear Transformation: Applies dense linear transformation to input</li> <li>Sigmoid Transformation: Applies dense sigmoid transformation to input</li> <li>Gating Mechanism: Multiplies linear output with sigmoid output</li> <li>Information Filtering: The sigmoid output acts as a gate controlling information flow</li> <li>Output Generation: Produces gated and filtered features</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Linear Dense Layer]\n    A --&gt; C[Sigmoid Dense Layer]\n    B --&gt; D[Linear Output]\n    C --&gt; E[Sigmoid Output (Gate)]\n    D --&gt; F[Element-wise Multiplication]\n    E --&gt; F\n    F --&gt; G[Gated Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style F fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/gated-linear-unit/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach GatedLinearUnit's Solution Information Flow No control over information flow \ud83c\udfaf Gated control of information flow Feature Filtering All features treated equally \u26a1 Selective filtering based on learned gates Attention Mechanisms Separate attention layers \ud83e\udde0 Built-in gating for attention-like behavior Feature Transformation Simple linear transformations \ud83d\udd17 Sophisticated gated transformations"},{"location":"layers/gated-linear-unit/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Information Flow Control: Controlling how information flows through networks</li> <li>Feature Filtering: Filtering features based on learned importance</li> <li>Attention Mechanisms: Implementing attention-like behavior</li> <li>Feature Transformation: Sophisticated feature processing</li> <li>Ensemble Learning: As a component in ensemble architectures</li> </ul>"},{"location":"layers/gated-linear-unit/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/gated-linear-unit/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import GatedLinearUnit\n\n# Create sample input data\nbatch_size, input_dim = 32, 16\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply gated linear unit\nglu = GatedLinearUnit(units=8)\noutput = glu(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 16)\nprint(f\"Output shape: {output.shape}\")     # (32, 8)\n</code></pre>"},{"location":"layers/gated-linear-unit/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedLinearUnit\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    GatedLinearUnit(units=16),\n    keras.layers.Dense(8, activation='relu'),\n    GatedLinearUnit(units=4),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/gated-linear-unit/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedLinearUnit\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply gated linear unit\nx = GatedLinearUnit(units=16)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = GatedLinearUnit(units=16)(x)\nx = keras.layers.Dense(8, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-linear-unit/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple GLU layers\ndef create_gated_network():\n    inputs = keras.Input(shape=(30,))\n\n    # Multiple GLU layers with different configurations\n    x = GatedLinearUnit(units=32)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = GatedLinearUnit(units=32)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = GatedLinearUnit(units=16)(x)\n\n    # Final processing\n    x = keras.layers.Dense(8, activation='relu')(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_gated_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/gated-linear-unit/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/gated-linear-unit/#kmr.layers.GatedLinearUnit","title":"kmr.layers.GatedLinearUnit","text":"<p>This module implements a GatedLinearUnit layer that applies a gated linear transformation to input tensors. It's particularly useful for controlling information flow in neural networks.</p>"},{"location":"layers/gated-linear-unit/#kmr.layers.GatedLinearUnit-classes","title":"Classes","text":""},{"location":"layers/gated-linear-unit/#kmr.layers.GatedLinearUnit.GatedLinearUnit","title":"GatedLinearUnit","text":"<pre><code>GatedLinearUnit(\n    units: int, name: str | None = None, **kwargs: Any\n)\n</code></pre> <p>GatedLinearUnit is a custom Keras layer that implements a gated linear unit.</p> <p>This layer applies a dense linear transformation to the input tensor and multiplies the result with the output of a dense sigmoid transformation. The result is a tensor where the input data is filtered based on the learned weights and biases of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>name</code> <code>str</code> <p>Name for the layer.</p> <code>None</code> Input shape <p>Tensor with shape: <code>(batch_size, ..., input_dim)</code></p> Output shape <p>Tensor with shape: <code>(batch_size, ..., units)</code></p> Example <pre><code>import keras\nfrom kmr.layers import GatedLinearUnit\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\n\n# Create the layer\nglu = GatedLinearUnit(units=8)\ny = glu(x)\nprint(\"Output shape:\", y.shape)  # (32, 8)\n</code></pre> <p>Initialize the GatedLinearUnit layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Number of units in the layer.</p> required <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/gated-linear-unit/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/gated-linear-unit/#units-int","title":"<code>units</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the output space</li> <li>Range: 1 to 1000+ (typically 8-128)</li> <li>Impact: Determines the size of the gated output</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/gated-linear-unit/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple mathematical operations</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for information flow control</li> <li>Best For: Networks requiring sophisticated information flow control</li> </ul>"},{"location":"layers/gated-linear-unit/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/gated-linear-unit/#example-1-information-flow-control","title":"Example 1: Information Flow Control","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import GatedLinearUnit\n\n# Create a network with controlled information flow\ndef create_controlled_flow_network():\n    inputs = keras.Input(shape=(25,))\n\n    # Initial processing\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Gated processing stages\n    x = GatedLinearUnit(units=32)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = GatedLinearUnit(units=16)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = GatedLinearUnit(units=8)(x)\n\n    # Final output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_controlled_flow_network()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Controlled flow predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/gated-linear-unit/#example-2-feature-filtering-analysis","title":"Example 2: Feature Filtering Analysis","text":"<pre><code># Analyze how GLU filters features\ndef analyze_feature_filtering():\n    # Create model with GLU\n    inputs = keras.Input(shape=(10,))\n    x = GatedLinearUnit(units=5)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.ops.convert_to_tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),  # First feature only\n        keras.ops.convert_to_tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),  # Second feature only\n        keras.ops.convert_to_tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]),  # First half\n        keras.ops.convert_to_tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]),  # Second half\n    ]\n\n    print(\"Feature Filtering Analysis:\")\n    print(\"=\" * 50)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction = {prediction.numpy()[0][0]:.4f}\")\n\n    return model\n\n# Analyze feature filtering\n# model = analyze_feature_filtering()\n</code></pre>"},{"location":"layers/gated-linear-unit/#example-3-attention-like-behavior","title":"Example 3: Attention-like Behavior","text":"<pre><code># Create attention-like behavior with GLU\ndef create_attention_like_network():\n    inputs = keras.Input(shape=(20,))\n\n    # Create attention-like gates\n    attention_gate = GatedLinearUnit(units=20)(inputs)\n\n    # Apply attention to features\n    attended_features = inputs * attention_gate\n\n    # Process attended features\n    x = keras.layers.Dense(32, activation='relu')(attended_features)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_attention_like_network()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test attention-like behavior\nsample_data = keras.random.normal((50, 20))\npredictions = model(sample_data)\nprint(f\"Attention-like predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/gated-linear-unit/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Units: Start with 16-32 units, scale based on data complexity</li> <li>Information Flow: Use GLU to control how information flows through networks</li> <li>Feature Filtering: GLU can act as a learned feature filter</li> <li>Attention: GLU can implement attention-like mechanisms</li> <li>Combination: Works well with other Keras layers</li> <li>Regularization: Consider adding dropout after GLU layers</li> </ul>"},{"location":"layers/gated-linear-unit/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Units: Must be positive integer</li> <li>Output Size: Output size is determined by units parameter</li> <li>Gradient Flow: GLU can affect gradient flow - monitor training</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Memory Usage: Scales with units parameter</li> </ul>"},{"location":"layers/gated-linear-unit/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedResidualNetwork - GRN using GLU</li> <li>VariableSelection - Variable selection with gating</li> <li>SparseAttentionWeighting - Sparse attention weighting</li> <li>TabularAttention - Attention mechanisms</li> </ul>"},{"location":"layers/gated-linear-unit/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Gated Linear Units - Original GLU paper</li> <li>Information Flow in Neural Networks - Information flow concepts</li> <li>Attention Mechanisms - Attention mechanism concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/gated-residual-network/","title":"\ud83d\udd17 GatedResidualNetwork\ud83d\udd17 GatedResidualNetwork","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/gated-residual-network/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>GatedResidualNetwork</code> is a sophisticated layer that combines residual connections with gated linear units for improved gradient flow and feature transformation. It applies a series of transformations including dense layers, dropout, gated linear units, and layer normalization, all while maintaining residual connections.</p> <p>This layer is particularly powerful for deep neural networks where gradient flow and feature transformation are critical, making it ideal for complex tabular data processing and feature engineering.</p>"},{"location":"layers/gated-residual-network/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The GatedResidualNetwork processes data through a sophisticated transformation pipeline:</p> <ol> <li>ELU Dense Layer: Applies dense transformation with ELU activation</li> <li>Linear Dense Layer: Applies linear transformation</li> <li>Dropout Regularization: Applies dropout for regularization</li> <li>Gated Linear Unit: Applies gated linear transformation</li> <li>Layer Normalization: Normalizes the transformed features</li> <li>Residual Connection: Adds the original input to maintain gradient flow</li> <li>Final Projection: Applies final dense transformation</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[ELU Dense Layer]\n    B --&gt; C[Linear Dense Layer]\n    C --&gt; D[Dropout]\n    D --&gt; E[Gated Linear Unit]\n    E --&gt; F[Layer Normalization]\n    F --&gt; G[Residual Connection]\n    A --&gt; G\n    G --&gt; H[Final Projection]\n    H --&gt; I[Output Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style I fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style E fill:#f3e5f5,stroke:#9c27b0\n    style G fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/gated-residual-network/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach GatedResidualNetwork's Solution Gradient Flow Vanishing gradients in deep networks \ud83c\udfaf Residual connections maintain gradient flow Feature Transformation Simple dense layers \u26a1 Sophisticated transformation with gating Regularization Basic dropout \ud83e\udde0 Advanced regularization with layer normalization Deep Networks Limited depth due to gradient issues \ud83d\udd17 Enables deeper networks with better training"},{"location":"layers/gated-residual-network/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Deep Tabular Networks: Building deep networks for tabular data</li> <li>Feature Transformation: Sophisticated feature processing</li> <li>Gradient Flow: Maintaining gradients in deep architectures</li> <li>Complex Patterns: Capturing complex relationships in data</li> <li>Ensemble Learning: As a component in ensemble architectures</li> </ul>"},{"location":"layers/gated-residual-network/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/gated-residual-network/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import GatedResidualNetwork\n\n# Create sample input data\nbatch_size, input_dim = 32, 16\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply gated residual network\ngrn = GatedResidualNetwork(units=16, dropout_rate=0.2)\noutput = grn(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 16)\nprint(f\"Output shape: {output.shape}\")     # (32, 16)\n</code></pre>"},{"location":"layers/gated-residual-network/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedResidualNetwork\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    GatedResidualNetwork(units=32, dropout_rate=0.2),\n    keras.layers.Dense(16, activation='relu'),\n    GatedResidualNetwork(units=16, dropout_rate=0.1),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/gated-residual-network/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import GatedResidualNetwork\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply gated residual network\nx = GatedResidualNetwork(units=32, dropout_rate=0.2)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/gated-residual-network/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple GRN layers\ndef create_deep_grn_model():\n    inputs = keras.Input(shape=(50,))\n\n    # Multiple GRN layers with different configurations\n    x = GatedResidualNetwork(units=64, dropout_rate=0.3)(inputs)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.2)(x)\n    x = GatedResidualNetwork(units=32, dropout_rate=0.1)(x)\n\n    # Final processing\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_deep_grn_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/gated-residual-network/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/gated-residual-network/#kmr.layers.GatedResidualNetwork","title":"kmr.layers.GatedResidualNetwork","text":"<p>This module implements a GatedResidualNetwork layer that combines residual connections with gated linear units for improved gradient flow and feature transformation.</p>"},{"location":"layers/gated-residual-network/#kmr.layers.GatedResidualNetwork-classes","title":"Classes","text":""},{"location":"layers/gated-residual-network/#kmr.layers.GatedResidualNetwork.GatedResidualNetwork","title":"GatedResidualNetwork","text":"<pre><code>GatedResidualNetwork(\n    units: int,\n    dropout_rate: float = 0.2,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>GatedResidualNetwork is a custom Keras layer that implements a gated residual network.</p> <p>This layer applies a series of transformations to the input tensor and combines the result with the input using a residual connection. The transformations include a dense layer with ELU activation, a dense linear layer, a dropout layer, a gated linear unit layer, layer normalization, and a final dense layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Positive integer, dimensionality of the output space.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization. Defaults to 0.2.</p> <code>0.2</code> <code>name</code> <code>str</code> <p>Name for the layer.</p> <code>None</code> Input shape <p>Tensor with shape: <code>(batch_size, ..., input_dim)</code></p> Output shape <p>Tensor with shape: <code>(batch_size, ..., units)</code></p> Example <pre><code>import keras\nfrom kmr.layers import GatedResidualNetwork\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\n\n# Create the layer\ngrn = GatedResidualNetwork(units=16, dropout_rate=0.2)\ny = grn(x)\nprint(\"Output shape:\", y.shape)  # (32, 16)\n</code></pre> <p>Initialize the GatedResidualNetwork.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Number of units in the network.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.2</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/gated-residual-network/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/gated-residual-network/#units-int","title":"<code>units</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the output space</li> <li>Range: 1 to 1000+ (typically 16-256)</li> <li>Impact: Determines the size of the transformed features</li> <li>Recommendation: Start with 32-64, scale based on data complexity</li> </ul>"},{"location":"layers/gated-residual-network/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Dropout rate for regularization</li> <li>Range: 0.0 to 0.9 (typically 0.1-0.3)</li> <li>Impact: Higher values = more regularization but potential underfitting</li> <li>Recommendation: Start with 0.2, adjust based on overfitting</li> </ul>"},{"location":"layers/gated-residual-network/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast - efficient transformations</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multiple layers</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex feature transformation</li> <li>Best For: Deep networks requiring sophisticated feature processing</li> </ul>"},{"location":"layers/gated-residual-network/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/gated-residual-network/#example-1-deep-tabular-network","title":"Example 1: Deep Tabular Network","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import GatedResidualNetwork\n\n# Create a deep tabular network with GRN layers\ndef create_deep_tabular_network():\n    inputs = keras.Input(shape=(30,))  # 30 features\n\n    # Initial processing\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Multiple GRN layers\n    x = GatedResidualNetwork(units=64, dropout_rate=0.2)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.2)(x)\n    x = GatedResidualNetwork(units=32, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=32, dropout_rate=0.1)(x)\n\n    # Final processing\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_deep_tabular_network()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 30))\npredictions = model(sample_data)\nprint(f\"Deep network predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/gated-residual-network/#example-2-feature-transformation-pipeline","title":"Example 2: Feature Transformation Pipeline","text":"<pre><code># Create a feature transformation pipeline with GRN\ndef create_feature_transformation_pipeline():\n    inputs = keras.Input(shape=(25,))\n\n    # Feature transformation stages\n    # Stage 1: Basic transformation\n    x1 = keras.layers.Dense(32, activation='relu')(inputs)\n    x1 = GatedResidualNetwork(units=32, dropout_rate=0.2)(x1)\n\n    # Stage 2: Advanced transformation\n    x2 = keras.layers.Dense(64, activation='relu')(inputs)\n    x2 = GatedResidualNetwork(units=64, dropout_rate=0.2)(x2)\n    x2 = GatedResidualNetwork(units=32, dropout_rate=0.1)(x2)\n\n    # Stage 3: Final transformation\n    x3 = keras.layers.Dense(48, activation='relu')(inputs)\n    x3 = GatedResidualNetwork(units=48, dropout_rate=0.2)(x3)\n    x3 = GatedResidualNetwork(units=24, dropout_rate=0.1)(x3)\n\n    # Combine transformed features\n    combined = keras.layers.Concatenate()([x1, x2, x3])\n\n    # Final processing\n    x = keras.layers.Dense(32, activation='relu')(combined)\n    x = keras.layers.Dropout(0.2)(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_feature_transformation_pipeline()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/gated-residual-network/#example-3-gradient-flow-analysis","title":"Example 3: Gradient Flow Analysis","text":"<pre><code># Analyze gradient flow in GRN networks\ndef analyze_gradient_flow():\n    # Create model with GRN layers\n    inputs = keras.Input(shape=(20,))\n    x = GatedResidualNetwork(units=32, dropout_rate=0.2)(inputs)\n    x = GatedResidualNetwork(units=32, dropout_rate=0.2)(x)\n    x = GatedResidualNetwork(units=16, dropout_rate=0.1)(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test gradient flow\n    with keras.GradientTape() as tape:\n        x = keras.random.normal((10, 20))\n        y = model(x)\n        loss = keras.ops.mean(y)\n\n    # Compute gradients\n    gradients = tape.gradient(loss, model.trainable_variables)\n\n    # Analyze gradient magnitudes\n    print(\"Gradient Flow Analysis:\")\n    print(\"=\" * 40)\n    for i, grad in enumerate(gradients):\n        if grad is not None:\n            grad_norm = keras.ops.norm(grad)\n            print(f\"Layer {i}: Gradient norm = {grad_norm:.6f}\")\n\n    return model\n\n# Analyze gradient flow\n# model = analyze_gradient_flow()\n</code></pre>"},{"location":"layers/gated-residual-network/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Units: Start with 32-64 units, scale based on data complexity</li> <li>Dropout Rate: Use 0.2-0.3 for regularization, adjust based on overfitting</li> <li>Residual Connections: The layer automatically handles residual connections</li> <li>Layer Normalization: Built-in layer normalization for stable training</li> <li>Gradient Flow: Excellent for maintaining gradients in deep networks</li> <li>Combination: Works well with other Keras layers</li> </ul>"},{"location":"layers/gated-residual-network/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Units: Must be positive integer</li> <li>Dropout Rate: Must be between 0 and 1</li> <li>Memory Usage: Can be memory-intensive with large units</li> <li>Overfitting: Monitor for overfitting with high dropout rates</li> <li>Gradient Explosion: Rare but possible with very deep networks</li> </ul>"},{"location":"layers/gated-residual-network/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedLinearUnit - Gated linear unit component</li> <li>TransformerBlock - Transformer-style processing</li> <li>TabularMoELayer - Mixture of experts</li> <li>VariableSelection - Variable selection with GRN</li> </ul>"},{"location":"layers/gated-residual-network/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Residual Networks - Residual network concepts</li> <li>Gated Linear Units - Gated linear unit paper</li> <li>Layer Normalization - Layer normalization paper</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/graph-feature-aggregation/","title":"\ud83d\udd17 GraphFeatureAggregation\ud83d\udd17 GraphFeatureAggregation","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/graph-feature-aggregation/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>GraphFeatureAggregation</code> layer treats each input feature as a node in a graph and uses self-attention mechanisms to learn relationships between features. It projects features into an embedding space, computes pairwise attention scores, and aggregates feature information based on these scores.</p> <p>This layer is particularly powerful for tabular data where features have inherent relationships, providing a way to learn and exploit these relationships automatically without manual feature engineering.</p>"},{"location":"layers/graph-feature-aggregation/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The GraphFeatureAggregation processes data through a graph-based transformation:</p> <ol> <li>Feature Embedding: Projects each scalar feature to an embedding</li> <li>Pairwise Scoring: Computes pairwise concatenated embeddings and scores them</li> <li>Attention Matrix: Normalizes scores with softmax to create dynamic adjacency matrix</li> <li>Feature Aggregation: Aggregates neighboring features via weighted sum</li> <li>Output Projection: Projects back to original dimension with residual connection</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Feature Embedding]\n    B --&gt; C[Pairwise Scoring]\n    C --&gt; D[Attention Matrix]\n    D --&gt; E[Feature Aggregation]\n    E --&gt; F[Output Projection]\n    A --&gt; G[Residual Connection]\n    F --&gt; G\n    G --&gt; H[Transformed Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style H fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#e1f5fe,stroke:#03a9f4\n    style E fill:#fff3e0,stroke:#ff9800</code></pre>"},{"location":"layers/graph-feature-aggregation/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach GraphFeatureAggregation's Solution Feature Relationships Manual feature engineering \ud83c\udfaf Automatic learning of feature relationships Graph Structure No graph structure \u26a1 Graph-based feature processing Attention Mechanisms No attention \ud83e\udde0 Self-attention for feature interactions Dynamic Adjacency Static relationships \ud83d\udd17 Dynamic adjacency matrix learning"},{"location":"layers/graph-feature-aggregation/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Tabular Data: Learning feature relationships in tabular data</li> <li>Graph Neural Networks: Graph-based processing for tabular data</li> <li>Feature Engineering: Automatic feature relationship learning</li> <li>Attention Mechanisms: Self-attention for feature interactions</li> <li>Dynamic Relationships: Learning dynamic feature relationships</li> </ul>"},{"location":"layers/graph-feature-aggregation/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/graph-feature-aggregation/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import GraphFeatureAggregation\n\n# Create sample input data\nbatch_size, num_features = 32, 10\nx = keras.random.normal((batch_size, num_features))\n\n# Apply graph feature aggregation\ngraph_layer = GraphFeatureAggregation(embed_dim=8, dropout_rate=0.1)\noutput = graph_layer(x, training=True)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {output.shape}\")     # (32, 10)\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import GraphFeatureAggregation\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    GraphFeatureAggregation(embed_dim=16, dropout_rate=0.1),\n    keras.layers.Dense(16, activation='relu'),\n    GraphFeatureAggregation(embed_dim=8, dropout_rate=0.1),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import GraphFeatureAggregation\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply graph feature aggregation\nx = GraphFeatureAggregation(embed_dim=16, dropout_rate=0.1)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = GraphFeatureAggregation(embed_dim=16, dropout_rate=0.1)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple graph layers\ndef create_graph_network():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Multiple graph layers with different configurations\n    x = GraphFeatureAggregation(\n        embed_dim=16,\n        dropout_rate=0.1,\n        leaky_relu_alpha=0.2\n    )(inputs)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = GraphFeatureAggregation(\n        embed_dim=12,\n        dropout_rate=0.1,\n        leaky_relu_alpha=0.2\n    )(x)\n\n    x = keras.layers.Dense(24, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_graph_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/graph-feature-aggregation/#kmr.layers.GraphFeatureAggregation","title":"kmr.layers.GraphFeatureAggregation","text":"<p>This module implements a GraphFeatureAggregation layer that treats features as nodes in a graph and uses attention mechanisms to learn relationships between features. This approach is useful for tabular data where features have inherent relationships.</p>"},{"location":"layers/graph-feature-aggregation/#kmr.layers.GraphFeatureAggregation-classes","title":"Classes","text":""},{"location":"layers/graph-feature-aggregation/#kmr.layers.GraphFeatureAggregation.GraphFeatureAggregation","title":"GraphFeatureAggregation","text":"<pre><code>GraphFeatureAggregation(\n    embed_dim: int = 8,\n    dropout_rate: float = 0.0,\n    leaky_relu_alpha: float = 0.2,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Graph-based feature aggregation layer with self-attention for tabular data.</p> <p>This layer treats each input feature as a node and projects it into an embedding space. It then computes pairwise attention scores between features and aggregates feature information based on these scores. Finally, it projects the aggregated features back to the original feature space and adds a residual connection.</p> The process involves <ol> <li>Projecting each scalar feature to an embedding (shape: [batch, num_features, embed_dim]).</li> <li>Computing pairwise concatenated embeddings and scoring them via a learnable attention vector.</li> <li>Normalizing the scores with softmax to yield a dynamic adjacency (attention) matrix.</li> <li>Aggregating neighboring features via weighted sum.</li> <li>Projecting back to a vector of original dimension, then adding a residual connection.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Dimensionality of the projected feature embeddings. Default is 8.</p> <code>8</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate to apply on attention weights. Default is 0.0.</p> <code>0.0</code> <code>leaky_relu_alpha</code> <code>float</code> <p>Alpha parameter for the LeakyReLU activation. Default is 0.2.</p> <code>0.2</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, num_features)</code> (same as input)</p> Example <pre><code>import keras\nfrom kmr.layers import GraphFeatureAggregation\n\n# Tabular data with 10 features\nx = keras.random.normal((32, 10))\n\n# Create the layer with an embedding dimension of 8 and dropout rate of 0.1\ngraph_layer = GraphFeatureAggregation(embed_dim=8, dropout_rate=0.1)\ny = graph_layer(x, training=True)\nprint(\"Output shape:\", y.shape)  # Expected: (32, 10)\n</code></pre> <p>Initialize the GraphFeatureAggregation layer.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>8</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.0</code> <code>leaky_relu_alpha</code> <code>float</code> <p>Alpha parameter for LeakyReLU.</p> <code>0.2</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/graph-feature-aggregation/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/graph-feature-aggregation/#embed_dim-int","title":"<code>embed_dim</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the projected feature embeddings</li> <li>Range: 4 to 64+ (typically 8-32)</li> <li>Impact: Larger values = more expressive embeddings but more parameters</li> <li>Recommendation: Start with 8-16, scale based on data complexity</li> </ul>"},{"location":"layers/graph-feature-aggregation/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Dropout rate applied to attention weights</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Use 0.1-0.2 for regularization</li> </ul>"},{"location":"layers/graph-feature-aggregation/#leaky_relu_alpha-float","title":"<code>leaky_relu_alpha</code> (float)","text":"<ul> <li>Purpose: Alpha parameter for LeakyReLU activation</li> <li>Range: 0.0 to 1.0 (typically 0.2)</li> <li>Impact: Controls the negative slope of LeakyReLU</li> <li>Recommendation: Use 0.2 for most applications</li> </ul>"},{"location":"layers/graph-feature-aggregation/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with features\u00b2</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to attention computation</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for feature relationship learning</li> <li>Best For: Tabular data with inherent feature relationships</li> </ul>"},{"location":"layers/graph-feature-aggregation/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/graph-feature-aggregation/#example-1-feature-relationship-learning","title":"Example 1: Feature Relationship Learning","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import GraphFeatureAggregation\n\n# Create a model for feature relationship learning\ndef create_relationship_learning_model():\n    inputs = keras.Input(shape=(20,))  # 20 features\n\n    # Multiple graph layers for different relationship levels\n    x = GraphFeatureAggregation(\n        embed_dim=16,\n        dropout_rate=0.1,\n        leaky_relu_alpha=0.2\n    )(inputs)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = GraphFeatureAggregation(\n        embed_dim=12,\n        dropout_rate=0.1,\n        leaky_relu_alpha=0.2\n    )(x)\n\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_relationship_learning_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 20))\npredictions = model(sample_data)\nprint(f\"Relationship learning predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#example-2-graph-structure-analysis","title":"Example 2: Graph Structure Analysis","text":"<pre><code># Analyze graph structure behavior\ndef analyze_graph_structure():\n    # Create model with graph layer\n    inputs = keras.Input(shape=(15,))\n    x = GraphFeatureAggregation(embed_dim=8, dropout_rate=0.1)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Graph Structure Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze graph structure\n# model = analyze_graph_structure()\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#example-3-attention-pattern-analysis","title":"Example 3: Attention Pattern Analysis","text":"<pre><code># Analyze attention patterns in graph layer\ndef analyze_attention_patterns():\n    # Create model with graph layer\n    inputs = keras.Input(shape=(12,))\n    x = GraphFeatureAggregation(embed_dim=8, dropout_rate=0.1)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with sample data\n    sample_data = keras.random.normal((50, 12))\n    predictions = model(sample_data)\n\n    print(\"Attention Pattern Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {sample_data.shape}\")\n    print(f\"Output shape: {predictions.shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze attention patterns\n# model = analyze_attention_patterns()\n</code></pre>"},{"location":"layers/graph-feature-aggregation/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Embedding Dimension: Start with 8-16, scale based on data complexity</li> <li>Dropout Rate: Use 0.1-0.2 for regularization</li> <li>LeakyReLU Alpha: Use 0.2 for most applications</li> <li>Feature Relationships: Works best when features have inherent relationships</li> <li>Residual Connections: Built-in residual connections for gradient flow</li> <li>Attention Patterns: Monitor attention patterns for interpretability</li> </ul>"},{"location":"layers/graph-feature-aggregation/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Embedding Dimension: Must be positive integer</li> <li>Dropout Rate: Must be between 0 and 1</li> <li>Memory Usage: Scales quadratically with number of features</li> <li>Overfitting: Monitor for overfitting with complex configurations</li> <li>Feature Count: Consider feature pre-selection for very large feature sets</li> </ul>"},{"location":"layers/graph-feature-aggregation/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>AdvancedGraphFeature - Advanced graph feature layer</li> <li>MultiHeadGraphFeaturePreprocessor - Multi-head graph preprocessing</li> <li>TabularAttention - Tabular attention mechanisms</li> <li>VariableSelection - Variable selection</li> </ul>"},{"location":"layers/graph-feature-aggregation/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Graph Neural Networks - Graph neural network concepts</li> <li>Self-Attention - Self-attention mechanism</li> <li>Feature Relationships - Feature relationship concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/hyper-zzw-operator/","title":"\u26a1 HyperZZWOperator\u26a1 HyperZZWOperator","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/hyper-zzw-operator/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>HyperZZWOperator</code> computes context-dependent weights by multiplying inputs with hyper-kernels. This specialized layer takes two inputs: the original input tensor and a context tensor, then generates hyper-kernels from the context to perform context-dependent transformations.</p> <p>This layer is particularly powerful for specialized transformations where the processing should depend on contextual information, making it ideal for advanced neural network architectures and context-aware processing.</p>"},{"location":"layers/hyper-zzw-operator/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The HyperZZWOperator processes data through context-dependent transformations:</p> <ol> <li>Input Processing: Takes input tensor and context tensor</li> <li>Hyper-Kernel Generation: Generates hyper-kernels from context</li> <li>Context-Dependent Transformation: Applies context-dependent weights</li> <li>Weight Computation: Computes context-dependent weights</li> <li>Output Generation: Produces transformed features</li> </ol> <pre><code>graph TD\n    A[Input Tensor] --&gt; C[Hyper-Kernel Generation]\n    B[Context Tensor] --&gt; C\n    C --&gt; D[Context-Dependent Weights]\n    D --&gt; E[Weight Computation]\n    E --&gt; F[Transformed Features]\n\n    G[Hyper-Kernels] --&gt; C\n    H[Context Processing] --&gt; C\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style B fill:#e6f3ff,stroke:#4a86e8\n    style F fill:#e8f5e9,stroke:#66bb6a\n    style C fill:#fff9e6,stroke:#ffb74d\n    style D fill:#f3e5f5,stroke:#9c27b0\n    style E fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/hyper-zzw-operator/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach HyperZZWOperator's Solution Context Processing Fixed transformations \ud83c\udfaf Context-dependent transformations Specialized Processing Generic processing \u26a1 Specialized transformations with hyper-kernels Context Awareness No context consideration \ud83e\udde0 Context-aware processing Advanced Architectures Standard layer stacking \ud83d\udd17 Specialized component for advanced architectures"},{"location":"layers/hyper-zzw-operator/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Context-Aware Processing: Processing that depends on contextual information</li> <li>Specialized Transformations: Advanced transformations with hyper-kernels</li> <li>Advanced Architectures: Components for sophisticated neural networks</li> <li>Context-Dependent Weights: Learning context-dependent weight patterns</li> <li>Specialized Models: Building specialized models like Terminator</li> </ul>"},{"location":"layers/hyper-zzw-operator/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/hyper-zzw-operator/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import HyperZZWOperator\n\n# Create sample input data\nbatch_size, input_dim, context_dim = 32, 16, 8\ninputs = keras.random.normal((batch_size, input_dim))\ncontext = keras.random.normal((batch_size, context_dim))\n\n# Apply hyper ZZW operator\nzzw_op = HyperZZWOperator(input_dim=16, context_dim=8)\noutput = zzw_op([inputs, context])\n\nprint(f\"Input shape: {inputs.shape}\")      # (32, 16)\nprint(f\"Context shape: {context.shape}\")   # (32, 8)\nprint(f\"Output shape: {output.shape}\")     # (32, 16)\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import HyperZZWOperator\n\n# Note: Sequential models don't work well with multiple inputs\n# Use functional API for HyperZZWOperator\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import HyperZZWOperator\n\n# Define inputs\ninput_tensor = keras.Input(shape=(20,), name='input_features')\ncontext_tensor = keras.Input(shape=(10,), name='context_features')\n\n# Apply hyper ZZW operator\nx = HyperZZWOperator(input_dim=20, context_dim=10)([input_tensor, context_tensor])\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model([input_tensor, context_tensor], outputs)\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple hyper ZZW operators\ndef create_context_aware_network():\n    # Define inputs\n    input_tensor = keras.Input(shape=(25,), name='input_features')\n    context_tensor = keras.Input(shape=(12,), name='context_features')\n\n    # Multiple hyper ZZW operators\n    x = HyperZZWOperator(input_dim=25, context_dim=12)([input_tensor, context_tensor])\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = HyperZZWOperator(input_dim=64, context_dim=12)([x, context_tensor])\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    x = HyperZZWOperator(input_dim=32, context_dim=12)([x, context_tensor])\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model([input_tensor, context_tensor], [classification, regression])\n\nmodel = create_context_aware_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/hyper-zzw-operator/#kmr.layers.HyperZZWOperator","title":"kmr.layers.HyperZZWOperator","text":"<p>This module implements a HyperZZWOperator layer that computes context-dependent weights by multiplying inputs with hyper-kernels. This is a specialized layer for the Terminator model.</p>"},{"location":"layers/hyper-zzw-operator/#kmr.layers.HyperZZWOperator-classes","title":"Classes","text":""},{"location":"layers/hyper-zzw-operator/#kmr.layers.HyperZZWOperator.HyperZZWOperator","title":"HyperZZWOperator","text":"<pre><code>HyperZZWOperator(\n    input_dim: int,\n    context_dim: int | None = None,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>A layer that computes context-dependent weights by multiplying inputs with hyper-kernels.</p> <p>This layer takes two inputs: the original input tensor and a context tensor. It generates hyper-kernels from the context and performs a context-dependent transformation of the input.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>context_dim</code> <code>int | None</code> <p>Optional dimension of the context features. If not provided, it will be inferred.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input <p>A list of two tensors: - inputs[0]: Input tensor with shape (batch_size, input_dim). - inputs[1]: Context tensor with shape (batch_size, context_dim).</p> Output shape <p>2D tensor with shape: <code>(batch_size, input_dim)</code> (same as input)</p> Example <pre><code>import keras\nfrom kmr.layers import HyperZZWOperator\n\n# Create sample input data\ninputs = keras.random.normal((32, 16))  # 32 samples, 16 features\ncontext = keras.random.normal((32, 8))  # 32 samples, 8 context features\n\n# Create the layer\nzzw_op = HyperZZWOperator(input_dim=16, context_dim=8)\ncontext_weights = zzw_op([inputs, context])\nprint(\"Output shape:\", context_weights.shape)  # (32, 16)\n</code></pre> <p>Initialize the HyperZZWOperator.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>context_dim</code> <code>int | None</code> <p>Context dimension.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/hyper-zzw-operator/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/hyper-zzw-operator/#input_dim-int","title":"<code>input_dim</code> (int)","text":"<ul> <li>Purpose: Dimension of the input features</li> <li>Range: 1 to 1000+ (typically 16-256)</li> <li>Impact: Determines the input feature dimension</li> <li>Recommendation: Match the actual input feature dimension</li> </ul>"},{"location":"layers/hyper-zzw-operator/#context_dim-int-optional","title":"<code>context_dim</code> (int, optional)","text":"<ul> <li>Purpose: Dimension of the context features</li> <li>Range: 1 to 1000+ (typically 8-128)</li> <li>Impact: Determines the context feature dimension</li> <li>Recommendation: Use appropriate context dimension for your use case</li> </ul>"},{"location":"layers/hyper-zzw-operator/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with dimensions</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to hyper-kernel computation</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for context-dependent processing</li> <li>Best For: Context-aware processing and specialized transformations</li> </ul>"},{"location":"layers/hyper-zzw-operator/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/hyper-zzw-operator/#example-1-context-aware-processing","title":"Example 1: Context-Aware Processing","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import HyperZZWOperator\n\n# Create a context-aware processing model\ndef create_context_aware_model():\n    # Define inputs\n    input_tensor = keras.Input(shape=(20,), name='input_features')\n    context_tensor = keras.Input(shape=(8,), name='context_features')\n\n    # Context-aware processing\n    x = HyperZZWOperator(input_dim=20, context_dim=8)([input_tensor, context_tensor])\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = HyperZZWOperator(input_dim=32, context_dim=8)([x, context_tensor])\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model([input_tensor, context_tensor], outputs)\n\nmodel = create_context_aware_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_inputs = keras.random.normal((100, 20))\nsample_context = keras.random.normal((100, 8))\npredictions = model([sample_inputs, sample_context])\nprint(f\"Context-aware predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#example-2-specialized-transformation","title":"Example 2: Specialized Transformation","text":"<pre><code># Create a specialized transformation model\ndef create_specialized_transformation():\n    # Define inputs\n    input_tensor = keras.Input(shape=(15,), name='input_features')\n    context_tensor = keras.Input(shape=(6,), name='context_features')\n\n    # Specialized transformation\n    x = HyperZZWOperator(input_dim=15, context_dim=6)([input_tensor, context_tensor])\n    x = keras.layers.Dense(24, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = HyperZZWOperator(input_dim=24, context_dim=6)([x, context_tensor])\n    x = keras.layers.Dense(12, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model([input_tensor, context_tensor], outputs)\n\nmodel = create_specialized_transformation()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#example-3-context-analysis","title":"Example 3: Context Analysis","text":"<pre><code># Analyze context-dependent behavior\ndef analyze_context_behavior():\n    # Create model with HyperZZWOperator\n    input_tensor = keras.Input(shape=(12,))\n    context_tensor = keras.Input(shape=(6,))\n\n    x = HyperZZWOperator(input_dim=12, context_dim=6)([input_tensor, context_tensor])\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model([input_tensor, context_tensor], outputs)\n\n    # Test with different context patterns\n    test_inputs = keras.random.normal((10, 12))\n    test_contexts = [\n        keras.random.normal((10, 6)),  # Random context\n        keras.random.normal((10, 6)) * 2,  # Scaled context\n        keras.random.normal((10, 6)) + 1,  # Shifted context\n    ]\n\n    print(\"Context Behavior Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_context in enumerate(test_contexts):\n        prediction = model([test_inputs, test_context])\n        print(f\"Context {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze context behavior\n# model = analyze_context_behavior()\n</code></pre>"},{"location":"layers/hyper-zzw-operator/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Input Dimension: Must match the actual input feature dimension</li> <li>Context Dimension: Use appropriate context dimension for your use case</li> <li>Context Quality: Ensure context information is meaningful and relevant</li> <li>Multiple Inputs: Use functional API for multiple input models</li> <li>Specialized Use: Best for specialized transformations and context-aware processing</li> <li>Architecture: Use as components in advanced architectures</li> </ul>"},{"location":"layers/hyper-zzw-operator/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Format: Must be a list of [input_tensor, context_tensor]</li> <li>Input Dimension: Must be positive integer</li> <li>Context Dimension: Must be positive integer</li> <li>Memory Usage: Scales with input and context dimensions</li> <li>Complexity: More complex than standard layers</li> </ul>"},{"location":"layers/hyper-zzw-operator/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>SlowNetwork - Multi-layer network processing</li> <li>GatedResidualNetwork - Gated residual networks</li> <li>TransformerBlock - Transformer processing</li> <li>SparseAttentionWeighting - Sparse attention weighting</li> </ul>"},{"location":"layers/hyper-zzw-operator/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Hyper-Kernels - Hyper-parameter concepts</li> <li>Context-Aware Processing - Context awareness concepts</li> <li>Specialized Transformations - Transformation concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/","title":"\ud83d\udc41\ufe0f InterpretableMultiHeadAttention\ud83d\udc41\ufe0f InterpretableMultiHeadAttention","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/interpretable-multi-head-attention/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>InterpretableMultiHeadAttention</code> layer is a specialized multi-head attention mechanism designed for interpretability and explainability. Unlike standard attention layers that hide their internal workings, this layer exposes attention scores, allowing you to understand exactly what the model is focusing on during its decision-making process.</p> <p>This layer is particularly valuable for applications where model interpretability is crucial, such as healthcare, finance, and other high-stakes domains where understanding model decisions is as important as accuracy.</p>"},{"location":"layers/interpretable-multi-head-attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The InterpretableMultiHeadAttention layer extends the standard multi-head attention mechanism with interpretability features:</p> <ol> <li>Multi-Head Processing: Processes input through multiple attention heads in parallel</li> <li>Attention Score Storage: Captures and stores attention weights for each head</li> <li>Score Accessibility: Provides easy access to attention scores for analysis</li> <li>Interpretable Output: Returns both the attention output and accessible attention weights</li> </ol> <pre><code>graph TD\n    A[Query, Key, Value] --&gt; B[Multi-Head Attention]\n    B --&gt; C[Head 1]\n    B --&gt; D[Head 2]\n    B --&gt; E[Head N]\n\n    C --&gt; F[Attention Scores 1]\n    D --&gt; G[Attention Scores 2]\n    E --&gt; H[Attention Scores N]\n\n    F --&gt; I[Concatenate Heads]\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt; J[Output + Stored Scores]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style F fill:#fff9e6,stroke:#ffb74d\n    style G fill:#fff9e6,stroke:#ffb74d\n    style H fill:#fff9e6,stroke:#ffb74d</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach InterpretableMultiHeadAttention's Solution Model Interpretability Black-box attention with no visibility \ud83d\udc41\ufe0f Transparent attention with accessible attention scores Debugging Models Difficult to understand what model focuses on \ud83d\udd0d Clear visibility into attention patterns and focus areas Regulatory Compliance Limited explainability for high-stakes decisions \ud83d\udccb Full traceability of attention decisions for compliance Model Validation Hard to validate attention behavior \u2705 Easy validation through attention score analysis"},{"location":"layers/interpretable-multi-head-attention/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Healthcare AI: Understanding which medical features drive diagnoses</li> <li>Financial Risk: Explaining which factors influence risk assessments</li> <li>Regulatory Compliance: Providing interpretable decisions for auditors</li> <li>Model Debugging: Identifying attention patterns and potential issues</li> <li>Research: Analyzing attention mechanisms in academic studies</li> <li>Customer-Facing AI: Explaining decisions to end users</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/interpretable-multi-head-attention/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import InterpretableMultiHeadAttention\n\n# Create sample data\nbatch_size, seq_len, d_model = 32, 10, 64\nquery = keras.random.normal((batch_size, seq_len, d_model))\nkey = keras.random.normal((batch_size, seq_len, d_model))\nvalue = keras.random.normal((batch_size, seq_len, d_model))\n\n# Apply interpretable attention\nattention = InterpretableMultiHeadAttention(\n    d_model=d_model,\n    n_head=8,\n    dropout_rate=0.1\n)\noutput = attention(query, key, value)\n\nprint(f\"Output shape: {output.shape}\")  # (32, 10, 64)\n\n# Access attention scores for interpretability\nattention_scores = attention.attention_scores\nprint(f\"Attention scores shape: {attention_scores.shape}\")  # (32, 8, 10, 10)\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import InterpretableMultiHeadAttention\n\n# Create a model with interpretable attention\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Reshape((1, 64)),  # Reshape for attention\n    InterpretableMultiHeadAttention(d_model=64, n_head=4),\n    keras.layers.Flatten(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import InterpretableMultiHeadAttention\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Process features\nx = keras.layers.Dense(64, activation='relu')(inputs)\nx = keras.layers.Reshape((1, 64))(x)  # Reshape for attention\n\n# Apply interpretable attention\nx = InterpretableMultiHeadAttention(d_model=64, n_head=8)(x, x, x)\n\n# Continue processing\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nattention = InterpretableMultiHeadAttention(\n    d_model=128,\n    n_head=16,\n    dropout_rate=0.2,\n    kernel_initializer='he_normal',\n    use_bias=False,\n    name=\"advanced_interpretable_attention\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50,))\nx = keras.layers.Dense(128, activation='relu')(inputs)\nx = keras.layers.Reshape((1, 128))(x)\nx = attention(x, x, x)  # Self-attention\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/interpretable-multi-head-attention/#kmr.layers.InterpretableMultiHeadAttention","title":"kmr.layers.InterpretableMultiHeadAttention","text":"<p>Interpretable Multi-Head Attention layer implementation.</p>"},{"location":"layers/interpretable-multi-head-attention/#kmr.layers.InterpretableMultiHeadAttention-classes","title":"Classes","text":""},{"location":"layers/interpretable-multi-head-attention/#kmr.layers.InterpretableMultiHeadAttention.InterpretableMultiHeadAttention","title":"InterpretableMultiHeadAttention","text":"<pre><code>InterpretableMultiHeadAttention(\n    d_model: int,\n    n_head: int,\n    dropout_rate: float = 0.1,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Interpretable Multi-Head Attention layer.</p> <p>This layer wraps Keras MultiHeadAttention and stores the attention scores for interpretability purposes. The attention scores can be accessed via the <code>attention_scores</code> attribute after calling the layer.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Size of each attention head for query, key, value.</p> required <code>n_head</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout probability. Default: 0.1.</p> <code>0.1</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments passed to MultiHeadAttention. Supported arguments: - value_dim: Size of each attention head for value. - use_bias: Whether to use bias. Default: True. - output_shape: Expected output shape. Default: None. - attention_axes: Axes for attention. Default: None. - kernel_initializer: Initializer for kernels. Default: 'glorot_uniform'. - bias_initializer: Initializer for biases. Default: 'zeros'. - kernel_regularizer: Regularizer for kernels. Default: None. - bias_regularizer: Regularizer for biases. Default: None. - activity_regularizer: Regularizer for activity. Default: None. - kernel_constraint: Constraint for kernels. Default: None. - bias_constraint: Constraint for biases. Default: None. - seed: Random seed for dropout. Default: None.</p> <code>{}</code> Call Args <p>query: Query tensor of shape <code>(B, S, E)</code> where B is batch size,     S is sequence length, and E is the feature dimension. key: Key tensor of shape <code>(B, S, E)</code>. value: Value tensor of shape <code>(B, S, E)</code>. training: Python boolean indicating whether the layer should behave in     training mode (applying dropout) or in inference mode (no dropout).</p> <p>Returns:</p> Name Type Description <code>output</code> <p>Attention output of shape <code>(B, S, E)</code>.</p> Example <pre><code>d_model = 64\nn_head = 4\nseq_len = 10\nbatch_size = 32\n\nlayer = InterpretableMultiHeadAttention(\n    d_model=d_model,\n    n_head=n_head,\n    kernel_initializer='he_normal',\n    use_bias=False\n)\nquery = tf.random.normal((batch_size, seq_len, d_model))\noutput = layer(query, query, query)\nattention_scores = layer.attention_scores  # Access attention weights\n</code></pre> <p>Initialize the layer.</p>"},{"location":"layers/interpretable-multi-head-attention/#kmr.layers.InterpretableMultiHeadAttention.InterpretableMultiHeadAttention-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(\n    config: dict[str, Any]\n) -&gt; InterpretableMultiHeadAttention\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>InterpretableMultiHeadAttention</code> <p>Layer instance</p>"},{"location":"layers/interpretable-multi-head-attention/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/interpretable-multi-head-attention/#d_model-int","title":"<code>d_model</code> (int)","text":"<ul> <li>Purpose: Size of each attention head for query, key, and value</li> <li>Range: 16 to 512+ (typically 64-256)</li> <li>Impact: Higher values = richer representations but more parameters</li> <li>Recommendation: Start with 64, scale based on data complexity</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#n_head-int","title":"<code>n_head</code> (int)","text":"<ul> <li>Purpose: Number of attention heads for parallel processing</li> <li>Range: 1 to 32+ (typically 4, 8, or 16)</li> <li>Impact: More heads = better pattern recognition but higher computational cost</li> <li>Recommendation: Start with 8, increase for complex patterns</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Regularization to prevent overfitting</li> <li>Range: 0.0 to 0.9</li> <li>Impact: Higher values = more regularization but potentially less learning</li> <li>Recommendation: Start with 0.1, adjust based on overfitting</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium datasets, scales with head count</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to attention score storage</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex patterns with interpretability</li> <li>Best For: Applications requiring both high performance and interpretability</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/interpretable-multi-head-attention/#example-1-medical-diagnosis-with-interpretability","title":"Example 1: Medical Diagnosis with Interpretability","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import InterpretableMultiHeadAttention\n\n# Simulate medical data: symptoms, lab values, demographics\nbatch_size, num_features = 32, 20\nmedical_data = keras.random.normal((batch_size, num_features))\n\n# Build interpretable diagnosis model\ninputs = keras.Input(shape=(num_features,))\nx = keras.layers.Dense(64, activation='relu')(inputs)\nx = keras.layers.Reshape((1, 64))(x)\n\n# Interpretable attention\nx = InterpretableMultiHeadAttention(d_model=64, n_head=8)(x, x, x)\n\n# Get attention scores for interpretability\nattention_layer = model.layers[3]  # InterpretableMultiHeadAttention layer\noutput = attention_layer(x, x, x)\nattention_scores = attention_layer.attention_scores\n\n# Process output\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(32, activation='relu')(x)\ndiagnosis = keras.layers.Dense(5, activation='softmax')(x)  # 5 possible diagnoses\n\nmodel = keras.Model(inputs, diagnosis)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n\n# Train model\nmodel.fit(medical_data, np.random.randint(0, 5, (batch_size, 1)), epochs=10, verbose=0)\n\n# Analyze attention patterns\nprint(\"Attention scores shape:\", attention_scores.shape)  # (32, 8, 1, 1)\nprint(\"Average attention per head:\", np.mean(attention_scores, axis=(0, 2, 3)))\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#example-2-financial-risk-assessment","title":"Example 2: Financial Risk Assessment","text":"<pre><code># Financial risk model with interpretable attention\ndef create_interpretable_risk_model():\n    inputs = keras.Input(shape=(25,))  # 25 financial features\n\n    # Feature processing\n    x = keras.layers.Dense(128, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Reshape((1, 128))(x)\n\n    # Interpretable attention\n    attention = InterpretableMultiHeadAttention(d_model=128, n_head=12)\n    x = attention(x, x, x)\n\n    # Get attention scores for analysis\n    attention_scores = attention.attention_scores\n\n    # Risk prediction\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n    risk_score = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, risk_score), attention_scores\n\nmodel, attention_scores = create_interpretable_risk_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# The attention_scores can be used for interpretability analysis\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#example-3-attention-visualization","title":"Example 3: Attention Visualization","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Function to visualize attention patterns\ndef visualize_attention(attention_scores, feature_names=None, head_idx=0):\n    \"\"\"Visualize attention patterns for a specific head.\"\"\"\n    # Get attention scores for the first sample and specified head\n    scores = attention_scores[0, head_idx, :, :]  # Shape: (seq_len, seq_len)\n\n    plt.figure(figsize=(10, 8))\n    plt.imshow(scores, cmap='Blues', aspect='auto')\n    plt.colorbar(label='Attention Weight')\n    plt.title(f'Attention Pattern - Head {head_idx}')\n    plt.xlabel('Key Position')\n    plt.ylabel('Query Position')\n\n    if feature_names:\n        plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n        plt.yticks(range(len(feature_names)), feature_names)\n\n    plt.tight_layout()\n    plt.show()\n\n# Use with your model\n# visualize_attention(attention_scores, feature_names=['feature1', 'feature2', ...])\n</code></pre>"},{"location":"layers/interpretable-multi-head-attention/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Interpretability: Access <code>attention_scores</code> attribute after each forward pass</li> <li>Head Analysis: Analyze individual heads to understand different attention patterns</li> <li>Visualization: Use attention scores for heatmap visualizations</li> <li>Regularization: Use appropriate dropout to prevent overfitting</li> <li>Head Count: Start with 8 heads, adjust based on complexity</li> <li>Memory: Be aware that attention scores increase memory usage</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Memory Usage: Storing attention scores increases memory consumption</li> <li>Score Access: Must access scores immediately after forward pass</li> <li>Head Interpretation: Different heads may focus on different patterns</li> <li>Overfitting: Complex attention can overfit on small datasets</li> <li>Performance: More heads = higher computational cost</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>TabularAttention - General tabular attention mechanism</li> <li>MultiResolutionTabularAttention - Multi-resolution attention</li> <li>ColumnAttention - Column-wise attention</li> <li>RowAttention - Row-wise attention</li> </ul>"},{"location":"layers/interpretable-multi-head-attention/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Is All You Need - Original Transformer paper</li> <li>The Annotated Transformer - Detailed attention explanation</li> <li>Attention Visualization in Deep Learning - Attention visualization techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Model Interpretability Tutorial - Complete guide to model interpretability</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/","title":"\ud83d\udd00 MultiHeadGraphFeaturePreprocessor\ud83d\udd00 MultiHeadGraphFeaturePreprocessor","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/multi-head-graph-feature-preprocessor/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>MultiHeadGraphFeaturePreprocessor</code> treats each feature as a node in a graph and applies multi-head self-attention to capture and aggregate complex interactions among features. It learns multiple relational views among features, which can significantly boost performance on tabular data.</p> <p>This layer is particularly powerful for tabular data where complex feature relationships need to be captured, providing a sophisticated preprocessing step that can learn multiple aspects of feature interactions.</p>"},{"location":"layers/multi-head-graph-feature-preprocessor/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The MultiHeadGraphFeaturePreprocessor processes data through multi-head graph-based transformation:</p> <ol> <li>Feature Embedding: Projects each scalar input into an embedding</li> <li>Multi-Head Split: Splits the embedding into multiple heads</li> <li>Query-Key-Value: Computes queries, keys, and values for each head</li> <li>Scaled Dot-Product Attention: Calculates attention across feature dimension</li> <li>Head Concatenation: Concatenates head outputs</li> <li>Output Projection: Projects back to original dimension with residual connection</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Feature Embedding]\n    B --&gt; C[Multi-Head Split]\n    C --&gt; D[Query-Key-Value]\n    D --&gt; E[Scaled Dot-Product Attention]\n    E --&gt; F[Head Concatenation]\n    F --&gt; G[Output Projection]\n    A --&gt; H[Residual Connection]\n    G --&gt; H\n    H --&gt; I[Transformed Features]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style I fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#e1f5fe,stroke:#03a9f4\n    style E fill:#fff3e0,stroke:#ff9800</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach MultiHeadGraphFeaturePreprocessor's Solution Feature Interactions Manual feature engineering \ud83c\udfaf Automatic learning of complex feature interactions Multiple Views Single perspective \u26a1 Multi-head attention for multiple relational views Graph Structure No graph structure \ud83e\udde0 Graph-based feature preprocessing Complex Relationships Limited relationship modeling \ud83d\udd17 Sophisticated relationship learning"},{"location":"layers/multi-head-graph-feature-preprocessor/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Tabular Data: Complex feature relationship preprocessing</li> <li>Graph Neural Networks: Graph-based preprocessing for tabular data</li> <li>Feature Engineering: Automatic feature interaction learning</li> <li>Multi-Head Attention: Multiple relational views of features</li> <li>Complex Patterns: Capturing complex feature relationships</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/multi-head-graph-feature-preprocessor/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import MultiHeadGraphFeaturePreprocessor\n\n# Create sample input data\nbatch_size, num_features = 32, 10\nx = keras.random.normal((batch_size, num_features))\n\n# Apply multi-head graph feature preprocessor\ngraph_preproc = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)\noutput = graph_preproc(x, training=True)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {output.shape}\")     # (32, 10)\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import MultiHeadGraphFeaturePreprocessor\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4),\n    keras.layers.Dense(16, activation='relu'),\n    MultiHeadGraphFeaturePreprocessor(embed_dim=8, num_heads=2),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import MultiHeadGraphFeaturePreprocessor\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply multi-head graph feature preprocessor\nx = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple graph preprocessors\ndef create_multi_head_graph_network():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Multiple graph preprocessors with different configurations\n    x = MultiHeadGraphFeaturePreprocessor(\n        embed_dim=24,\n        num_heads=6,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = MultiHeadGraphFeaturePreprocessor(\n        embed_dim=20,\n        num_heads=5,\n        dropout_rate=0.1\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_multi_head_graph_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/multi-head-graph-feature-preprocessor/#kmr.layers.MultiHeadGraphFeaturePreprocessor","title":"kmr.layers.MultiHeadGraphFeaturePreprocessor","text":"<p>This module implements a MultiHeadGraphFeaturePreprocessor layer that treats features as nodes in a graph and learns multiple \"views\" (heads) of the feature interactions via self-attention. This approach is useful for tabular data where complex feature relationships need to be captured.</p>"},{"location":"layers/multi-head-graph-feature-preprocessor/#kmr.layers.MultiHeadGraphFeaturePreprocessor-classes","title":"Classes","text":""},{"location":"layers/multi-head-graph-feature-preprocessor/#kmr.layers.MultiHeadGraphFeaturePreprocessor.MultiHeadGraphFeaturePreprocessor","title":"MultiHeadGraphFeaturePreprocessor","text":"<pre><code>MultiHeadGraphFeaturePreprocessor(\n    embed_dim: int = 16,\n    num_heads: int = 4,\n    dropout_rate: float = 0.0,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Multi-head graph-based feature preprocessor for tabular data.</p> <p>This layer treats each feature as a node and applies multi-head self-attention to capture and aggregate complex interactions among features. The process is:</p> <ol> <li>Project each scalar input into an embedding of dimension <code>embed_dim</code>.</li> <li>Split the embedding into <code>num_heads</code> heads.</li> <li>For each head, compute queries, keys, and values and calculate scaled dot-product    attention across the feature dimension.</li> <li>Concatenate the head outputs, project back to the original feature dimension,    and add a residual connection.</li> </ol> <p>This mechanism allows the network to learn multiple relational views among features, which can significantly boost performance on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Dimension of the feature embeddings. Default is 16.</p> <code>16</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads. Default is 4.</p> <code>4</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied to attention weights. Default is 0.0.</p> <code>0.0</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, num_features)</code> (same as input)</p> Example <pre><code>import keras\nfrom kmr.layers import MultiHeadGraphFeaturePreprocessor\n\n# Tabular data with 10 features\nx = keras.random.normal((32, 10))\n\n# Create the layer with 16-dim embeddings and 4 attention heads\ngraph_preproc = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)\ny = graph_preproc(x, training=True)\nprint(\"Output shape:\", y.shape)  # Expected: (32, 10)\n</code></pre> <p>Initialize the MultiHeadGraphFeaturePreprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>16</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>4</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.0</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/multi-head-graph-feature-preprocessor/#kmr.layers.MultiHeadGraphFeaturePreprocessor.MultiHeadGraphFeaturePreprocessor-functions","title":"Functions","text":"split_heads <pre><code>split_heads(\n    x: KerasTensor, batch_size: KerasTensor\n) -&gt; KerasTensor\n</code></pre> <p>Split the last dimension into (num_heads, depth) and transpose.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>KerasTensor</code> <p>Input tensor with shape (batch_size, num_features, embed_dim).</p> required <code>batch_size</code> <code>KerasTensor</code> <p>Batch size tensor.</p> required <p>Returns:</p> Type Description <code>KerasTensor</code> <p>Tensor with shape (batch_size, num_heads, num_features, depth).</p>"},{"location":"layers/multi-head-graph-feature-preprocessor/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/multi-head-graph-feature-preprocessor/#embed_dim-int","title":"<code>embed_dim</code> (int)","text":"<ul> <li>Purpose: Dimension of the feature embeddings</li> <li>Range: 8 to 128+ (typically 16-64)</li> <li>Impact: Larger values = more expressive embeddings but more parameters</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#num_heads-int","title":"<code>num_heads</code> (int)","text":"<ul> <li>Purpose: Number of attention heads</li> <li>Range: 1 to 16+ (typically 4-8)</li> <li>Impact: More heads = more diverse attention patterns</li> <li>Recommendation: Use 4-8 heads for most applications</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Dropout rate applied to attention weights</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Use 0.1-0.2 for regularization</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with heads and features</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multi-head attention</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex feature relationship learning</li> <li>Best For: Tabular data with complex feature relationships</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/multi-head-graph-feature-preprocessor/#example-1-complex-feature-relationships","title":"Example 1: Complex Feature Relationships","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import MultiHeadGraphFeaturePreprocessor\n\n# Create a model for complex feature relationships\ndef create_complex_relationship_model():\n    inputs = keras.Input(shape=(20,))  # 20 features\n\n    # Multiple graph preprocessors for different relationship levels\n    x = MultiHeadGraphFeaturePreprocessor(\n        embed_dim=24,\n        num_heads=6,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = MultiHeadGraphFeaturePreprocessor(\n        embed_dim=20,\n        num_heads=5,\n        dropout_rate=0.1\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_complex_relationship_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 20))\npredictions = model(sample_data)\nprint(f\"Complex relationship predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#example-2-multi-head-analysis","title":"Example 2: Multi-Head Analysis","text":"<pre><code># Analyze multi-head behavior\ndef analyze_multi_head_behavior():\n    # Create model with multi-head graph preprocessor\n    inputs = keras.Input(shape=(15,))\n    x = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Multi-Head Behavior Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze multi-head behavior\n# model = analyze_multi_head_behavior()\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#example-3-attention-head-analysis","title":"Example 3: Attention Head Analysis","text":"<pre><code># Analyze attention head patterns\ndef analyze_attention_heads():\n    # Create model with multi-head graph preprocessor\n    inputs = keras.Input(shape=(12,))\n    x = MultiHeadGraphFeaturePreprocessor(embed_dim=16, num_heads=4)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with sample data\n    sample_data = keras.random.normal((50, 12))\n    predictions = model(sample_data)\n\n    print(\"Attention Head Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {sample_data.shape}\")\n    print(f\"Output shape: {predictions.shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze attention heads\n# model = analyze_attention_heads()\n</code></pre>"},{"location":"layers/multi-head-graph-feature-preprocessor/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Embedding Dimension: Start with 16-32, scale based on data complexity</li> <li>Number of Heads: Use 4-8 heads for most applications</li> <li>Dropout Rate: Use 0.1-0.2 for regularization</li> <li>Feature Relationships: Works best when features have complex relationships</li> <li>Residual Connections: Built-in residual connections for gradient flow</li> <li>Attention Patterns: Monitor attention patterns for interpretability</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Embedding Dimension: Must be divisible by num_heads</li> <li>Number of Heads: Must be positive integer</li> <li>Dropout Rate: Must be between 0 and 1</li> <li>Memory Usage: Scales with number of heads and features</li> <li>Overfitting: Monitor for overfitting with complex configurations</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>AdvancedGraphFeature - Advanced graph feature layer</li> <li>GraphFeatureAggregation - Graph feature aggregation</li> <li>TabularAttention - Tabular attention mechanisms</li> <li>VariableSelection - Variable selection</li> </ul>"},{"location":"layers/multi-head-graph-feature-preprocessor/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Multi-Head Attention - Multi-head attention mechanism</li> <li>Graph Neural Networks - Graph neural network concepts</li> <li>Feature Relationships - Feature relationship concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/","title":"\ud83d\udd0d MultiResolutionTabularAttention\ud83d\udd0d MultiResolutionTabularAttention","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/multi-resolution-tabular-attention/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>MultiResolutionTabularAttention</code> layer is a sophisticated attention mechanism designed specifically for mixed-type tabular data. Unlike standard attention layers that treat all features uniformly, this layer recognizes that numerical and categorical features have fundamentally different characteristics and require specialized processing.</p> <p>This layer implements separate attention mechanisms for numerical and categorical features, along with cross-attention between them, enabling the model to learn optimal representations for each data type while capturing their interactions.</p>"},{"location":"layers/multi-resolution-tabular-attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The MultiResolutionTabularAttention processes mixed-type tabular data through specialized attention pathways:</p> <ol> <li>Numerical Feature Processing: Dedicated attention for continuous numerical features</li> <li>Categorical Feature Processing: Specialized attention for discrete categorical features  </li> <li>Cross-Attention: Bidirectional attention between numerical and categorical features</li> <li>Feature Fusion: Intelligent combination of both feature types</li> </ol> <pre><code>graph TD\n    A[Numerical Features] --&gt; B[Numerical Projection]\n    C[Categorical Features] --&gt; D[Categorical Projection]\n\n    B --&gt; E[Numerical Self-Attention]\n    D --&gt; F[Categorical Self-Attention]\n\n    E --&gt; G[Numerical Cross-Attention]\n    F --&gt; H[Categorical Cross-Attention]\n\n    G --&gt; I[Numerical LayerNorm + Residual]\n    H --&gt; J[Categorical LayerNorm + Residual]\n\n    I --&gt; K[Numerical Output]\n    J --&gt; L[Categorical Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style C fill:#fff9e6,stroke:#ffb74d\n    style K fill:#e8f5e9,stroke:#66bb6a\n    style L fill:#e8f5e9,stroke:#66bb6a</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach MultiResolutionTabularAttention's Solution Mixed Data Types Treat all features the same way \ud83c\udfaf Specialized processing for numerical vs categorical features Feature Interactions Simple concatenation or basic attention \ud83d\udd17 Cross-attention between different feature types Information Loss One-size-fits-all representations \ud83d\udcca Preserved semantics of each data type Complex Relationships Limited cross-type learning \ud83e\udde0 Rich interactions between numerical and categorical features"},{"location":"layers/multi-resolution-tabular-attention/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Customer Analytics: Combining numerical metrics (age, income) with categorical data (region, product category)</li> <li>Medical Diagnosis: Processing lab values (numerical) alongside symptoms and demographics (categorical)</li> <li>E-commerce: Analyzing purchase amounts and quantities (numerical) with product categories and user segments (categorical)</li> <li>Financial Modeling: Combining market indicators (numerical) with sector classifications and risk categories (categorical)</li> <li>Survey Analysis: Processing rating scales (numerical) with demographic and preference data (categorical)</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/multi-resolution-tabular-attention/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import MultiResolutionTabularAttention\n\n# Create mixed-type tabular data\nbatch_size, num_samples = 32, 100\nnumerical_features = keras.random.normal((batch_size, num_samples, 10))  # 10 numerical features\ncategorical_features = keras.random.normal((batch_size, num_samples, 5))  # 5 categorical features\n\n# Apply multi-resolution attention\nattention = MultiResolutionTabularAttention(num_heads=8, d_model=64, dropout_rate=0.1)\nnum_output, cat_output = attention([numerical_features, categorical_features])\n\nprint(f\"Numerical output shape: {num_output.shape}\")  # (32, 100, 64)\nprint(f\"Categorical output shape: {cat_output.shape}\")  # (32, 100, 64)\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import MultiResolutionTabularAttention\n\n# For mixed-type data, you'll need to handle inputs separately\ndef create_mixed_model():\n    # Define separate inputs\n    num_input = keras.Input(shape=(100, 10), name='numerical')\n    cat_input = keras.Input(shape=(100, 5), name='categorical')\n\n    # Apply multi-resolution attention\n    num_out, cat_out = MultiResolutionTabularAttention(\n        num_heads=4, d_model=32, dropout_rate=0.1\n    )([num_input, cat_input])\n\n    # Combine outputs\n    combined = keras.layers.Concatenate()([num_out, cat_out])\n    combined = keras.layers.Dense(64, activation='relu')(combined)\n    output = keras.layers.Dense(1, activation='sigmoid')(combined)\n\n    return keras.Model([num_input, cat_input], output)\n\nmodel = create_mixed_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import MultiResolutionTabularAttention\n\n# Define inputs for mixed data\nnum_inputs = keras.Input(shape=(50, 15), name='numerical_features')\ncat_inputs = keras.Input(shape=(50, 8), name='categorical_features')\n\n# Apply multi-resolution attention\nnum_attended, cat_attended = MultiResolutionTabularAttention(\n    num_heads=8, d_model=128, dropout_rate=0.15\n)([num_inputs, cat_inputs])\n\n# Process each type separately\nnum_processed = keras.layers.Dense(64, activation='relu')(num_attended)\ncat_processed = keras.layers.Dense(64, activation='relu')(cat_attended)\n\n# Combine and final prediction\ncombined = keras.layers.Concatenate()([num_processed, cat_processed])\ncombined = keras.layers.Dropout(0.2)(combined)\noutputs = keras.layers.Dense(3, activation='softmax')(combined)\n\nmodel = keras.Model([num_inputs, cat_inputs], outputs)\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nattention = MultiResolutionTabularAttention(\n    num_heads=16,           # More heads for complex cross-attention\n    d_model=256,            # Higher dimensionality for rich representations\n    dropout_rate=0.2,       # Higher dropout for regularization\n    name=\"advanced_multi_resolution\"\n)\n\n# Use in a complex multi-task model\ndef create_advanced_model():\n    num_input = keras.Input(shape=(100, 20), name='numerical')\n    cat_input = keras.Input(shape=(100, 10), name='categorical')\n\n    # Multi-resolution attention\n    num_out, cat_out = attention([num_input, cat_input])\n\n    # Task-specific processing\n    num_features = keras.layers.GlobalAveragePooling1D()(num_out)\n    cat_features = keras.layers.GlobalAveragePooling1D()(cat_out)\n\n    # Multiple outputs\n    combined = keras.layers.Concatenate()([num_features, cat_features])\n\n    # Classification head\n    classification = keras.layers.Dense(64, activation='relu')(combined)\n    classification = keras.layers.Dropout(0.3)(classification)\n    classification_out = keras.layers.Dense(5, activation='softmax', name='classification')(classification)\n\n    # Regression head\n    regression = keras.layers.Dense(32, activation='relu')(combined)\n    regression_out = keras.layers.Dense(1, name='regression')(regression)\n\n    return keras.Model([num_input, cat_input], [classification_out, regression_out])\n\nmodel = create_advanced_model()\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/multi-resolution-tabular-attention/#kmr.layers.MultiResolutionTabularAttention","title":"kmr.layers.MultiResolutionTabularAttention","text":"<p>This module implements a MultiResolutionTabularAttention layer that applies separate attention mechanisms for numerical and categorical features, along with cross-attention between them. It's particularly useful for mixed-type tabular data.</p>"},{"location":"layers/multi-resolution-tabular-attention/#kmr.layers.MultiResolutionTabularAttention-classes","title":"Classes","text":""},{"location":"layers/multi-resolution-tabular-attention/#kmr.layers.MultiResolutionTabularAttention.MultiResolutionTabularAttention","title":"MultiResolutionTabularAttention","text":"<pre><code>MultiResolutionTabularAttention(\n    num_heads: int,\n    d_model: int,\n    dropout_rate: float = 0.1,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Custom layer to apply multi-resolution attention for mixed-type tabular data.</p> <p>This layer implements separate attention mechanisms for numerical and categorical features, along with cross-attention between them. It's designed to handle the different characteristics of numerical and categorical features in tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>name</code> <code>str</code> <p>Name for the layer</p> <code>None</code> Input shape <p>List of two tensors: - Numerical features: <code>(batch_size, num_samples, num_numerical_features)</code> - Categorical features: <code>(batch_size, num_samples, num_categorical_features)</code></p> Output shape <p>List of two tensors with shapes: - <code>(batch_size, num_samples, d_model)</code> (numerical features) - <code>(batch_size, num_samples, d_model)</code> (categorical features)</p> Example <pre><code>import keras\nfrom kmr.layers import MultiResolutionTabularAttention\n\n# Create sample input data\nnumerical = keras.random.normal((32, 100, 10))  # 32 batches, 100 samples, 10 numerical features\ncategorical = keras.random.normal((32, 100, 5))  # 32 batches, 100 samples, 5 categorical features\n\n# Apply multi-resolution attention\nattention = MultiResolutionTabularAttention(num_heads=4, d_model=32, dropout_rate=0.1)\nnum_out, cat_out = attention([numerical, categorical])\nprint(\"Numerical output shape:\", num_out.shape)  # (32, 100, 32)\nprint(\"Categorical output shape:\", cat_out.shape)  # (32, 100, 32)\n</code></pre> <p>Initialize the MultiResolutionTabularAttention.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/multi-resolution-tabular-attention/#kmr.layers.MultiResolutionTabularAttention.MultiResolutionTabularAttention-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: list[tuple[int, ...]]\n) -&gt; list[tuple[int, ...]]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>list[tuple[int, ...]]</code> <p>List of shapes of the input tensors.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, ...]]</code> <p>List of shapes of the output tensors.</p>"},{"location":"layers/multi-resolution-tabular-attention/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/multi-resolution-tabular-attention/#num_heads-int","title":"<code>num_heads</code> (int)","text":"<ul> <li>Purpose: Number of attention heads for parallel processing</li> <li>Range: 1 to 32+ (typically 4, 8, or 16)</li> <li>Impact: More heads = better cross-type pattern recognition</li> <li>Recommendation: Start with 8, increase for complex mixed-type interactions</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#d_model-int","title":"<code>d_model</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the attention model</li> <li>Range: 32 to 512+ (must be divisible by num_heads)</li> <li>Impact: Higher values = richer cross-type representations</li> <li>Recommendation: Start with 64-128, scale based on data complexity</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Regularization to prevent overfitting</li> <li>Range: 0.0 to 0.9</li> <li>Impact: Higher values = more regularization for complex interactions</li> <li>Recommendation: Start with 0.1-0.2, adjust based on overfitting</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1 Fast for small to medium datasets, scales with feature complexity</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Higher memory usage due to dual attention mechanisms</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for mixed-type tabular data with complex interactions</li> <li>Best For: Mixed-type tabular data requiring specialized feature processing</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/multi-resolution-tabular-attention/#example-1-e-commerce-recommendation","title":"Example 1: E-commerce Recommendation","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import MultiResolutionTabularAttention\n\n# Simulate e-commerce data\nbatch_size, num_users = 32, 1000\n\n# Numerical features: purchase_amount, session_duration, page_views, etc.\nnumerical_data = keras.random.normal((batch_size, num_users, 8))\n\n# Categorical features: user_segment, product_category, device_type, etc.\ncategorical_data = keras.random.normal((batch_size, num_users, 6))\n\n# Build recommendation model\nnum_input = keras.Input(shape=(num_users, 8), name='numerical')\ncat_input = keras.Input(shape=(num_users, 6), name='categorical')\n\n# Multi-resolution attention\nnum_out, cat_out = MultiResolutionTabularAttention(\n    num_heads=8, d_model=64, dropout_rate=0.1\n)([num_input, cat_input])\n\n# User-level features\nuser_features = keras.layers.Concatenate()([\n    keras.layers.GlobalAveragePooling1D()(num_out),\n    keras.layers.GlobalAveragePooling1D()(cat_out)\n])\n\n# Recommendation score\nrecommendation = keras.layers.Dense(128, activation='relu')(user_features)\nrecommendation = keras.layers.Dropout(0.2)(recommendation)\nrecommendation_score = keras.layers.Dense(1, activation='sigmoid')(recommendation)\n\nmodel = keras.Model([num_input, cat_input], recommendation_score)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#example-2-medical-diagnosis","title":"Example 2: Medical Diagnosis","text":"<pre><code># Medical data with lab values and categorical symptoms\nlab_values = keras.random.normal((32, 200, 12))  # 12 lab tests\nsymptoms = keras.random.normal((32, 200, 8))     # 8 symptom categories\n\n# Diagnosis model\nlab_input = keras.Input(shape=(200, 12), name='lab_values')\nsymptom_input = keras.Input(shape=(200, 8), name='symptoms')\n\n# Multi-resolution attention\nlab_out, symptom_out = MultiResolutionTabularAttention(\n    num_heads=6, d_model=96, dropout_rate=0.15\n)([lab_input, symptom_input])\n\n# Patient-level representation\npatient_features = keras.layers.Concatenate()([\n    keras.layers.GlobalMaxPooling1D()(lab_out),\n    keras.layers.GlobalMaxPooling1D()(symptom_out)\n])\n\n# Diagnosis prediction\ndiagnosis = keras.layers.Dense(64, activation='relu')(patient_features)\ndiagnosis = keras.layers.Dropout(0.3)(diagnosis)\ndiagnosis_out = keras.layers.Dense(10, activation='softmax')(diagnosis)  # 10 possible diagnoses\n\nmodel = keras.Model([lab_input, symptom_input], diagnosis_out)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#example-3-financial-risk-assessment","title":"Example 3: Financial Risk Assessment","text":"<pre><code># Financial data with numerical metrics and categorical risk factors\nfinancial_metrics = keras.random.normal((32, 500, 15))  # 15 financial indicators\nrisk_factors = keras.random.normal((32, 500, 7))        # 7 risk categories\n\n# Risk assessment model\nmetrics_input = keras.Input(shape=(500, 15), name='financial_metrics')\nrisk_input = keras.Input(shape=(500, 7), name='risk_factors')\n\n# Multi-resolution attention\nmetrics_out, risk_out = MultiResolutionTabularAttention(\n    num_heads=12, d_model=144, dropout_rate=0.2\n)([metrics_input, risk_input])\n\n# Portfolio-level risk assessment\nportfolio_risk = keras.layers.Concatenate()([\n    keras.layers.GlobalAveragePooling1D()(metrics_out),\n    keras.layers.GlobalAveragePooling1D()(risk_out)\n])\n\n# Risk prediction\nrisk_score = keras.layers.Dense(128, activation='relu')(portfolio_risk)\nrisk_score = keras.layers.Dropout(0.25)(risk_score)\nrisk_score = keras.layers.Dense(64, activation='relu')(risk_score)\nrisk_output = keras.layers.Dense(1, activation='sigmoid')(risk_score)  # Risk probability\n\nmodel = keras.Model([metrics_input, risk_input], risk_output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/multi-resolution-tabular-attention/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Data Preprocessing: Ensure numerical features are normalized and categorical features are properly encoded</li> <li>Feature Balance: Maintain reasonable balance between numerical and categorical feature counts</li> <li>Head Configuration: Use more attention heads for complex cross-type interactions</li> <li>Regularization: Apply appropriate dropout to prevent overfitting in cross-attention</li> <li>Output Processing: Consider different pooling strategies for different feature types</li> <li>Monitoring: Track attention weights to understand cross-type learning</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Format: Must provide exactly two inputs: [numerical_features, categorical_features]</li> <li>Shape Mismatch: Ensure both inputs have the same batch_size and num_samples dimensions</li> <li>Memory Usage: Higher memory consumption due to dual attention mechanisms</li> <li>Overfitting: Complex cross-attention can lead to overfitting on small datasets</li> <li>Feature Imbalance: Severe imbalance between feature types can hurt performance</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>TabularAttention - General tabular attention for uniform feature processing</li> <li>ColumnAttention - Column-wise attention for feature relationships</li> <li>AdvancedNumericalEmbedding - Specialized numerical feature processing</li> <li>DistributionAwareEncoder - Distribution-aware feature encoding</li> </ul>"},{"location":"layers/multi-resolution-tabular-attention/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>TabNet: Attentive Interpretable Tabular Learning - Tabular-specific attention mechanisms</li> <li>Attention Is All You Need - Original Transformer architecture</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Mixed-Type Data Tutorial - Complete guide to mixed-type tabular modeling</li> </ul>"},{"location":"layers/numerical-anomaly-detection/","title":"\ud83d\udd0d NumericalAnomalyDetection\ud83d\udd0d NumericalAnomalyDetection","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/numerical-anomaly-detection/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>NumericalAnomalyDetection</code> layer learns a distribution for each numerical feature and outputs an anomaly score for each feature based on how far it deviates from the learned distribution. It uses a combination of mean, variance, and autoencoder reconstruction error to detect anomalies.</p> <p>This layer is particularly powerful for identifying outliers in numerical data, providing a comprehensive approach that combines statistical and neural network-based anomaly detection methods.</p>"},{"location":"layers/numerical-anomaly-detection/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The NumericalAnomalyDetection processes data through a multi-component anomaly detection system:</p> <ol> <li>Autoencoder Processing: Encodes and decodes features through a neural network</li> <li>Reconstruction Error: Computes reconstruction error for each feature</li> <li>Distribution Learning: Learns mean and variance for each feature</li> <li>Distribution Error: Computes distribution-based error</li> <li>Anomaly Scoring: Combines reconstruction and distribution errors</li> <li>Output Generation: Produces anomaly scores for each feature</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Autoencoder Encoder]\n    B --&gt; C[Autoencoder Decoder]\n    C --&gt; D[Reconstruction Error]\n\n    A --&gt; E[Distribution Learning]\n    E --&gt; F[Mean Learning]\n    E --&gt; G[Variance Learning]\n    F --&gt; H[Distribution Error]\n    G --&gt; H\n\n    D --&gt; I[Anomaly Scoring]\n    H --&gt; I\n    I --&gt; J[Anomaly Scores]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#fff9e6,stroke:#ffb74d\n    style E fill:#f3e5f5,stroke:#9c27b0\n    style I fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/numerical-anomaly-detection/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach NumericalAnomalyDetection's Solution Outlier Detection Statistical methods only \ud83c\udfaf Combined approach with neural networks Feature-Specific Global anomaly detection \u26a1 Per-feature anomaly scoring Reconstruction Error No reconstruction learning \ud83e\udde0 Autoencoder-based reconstruction error Distribution Learning Fixed distributions \ud83d\udd17 Learned distributions for each feature"},{"location":"layers/numerical-anomaly-detection/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Outlier Detection: Identifying outliers in numerical features</li> <li>Data Quality: Ensuring data quality through anomaly detection</li> <li>Feature Analysis: Analyzing feature-level anomalies</li> <li>Autoencoder Applications: Using autoencoders for anomaly detection</li> <li>Distribution Learning: Learning feature distributions</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/numerical-anomaly-detection/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import NumericalAnomalyDetection\n\n# Create sample input data\nbatch_size, num_features = 32, 5\nx = keras.random.normal((batch_size, num_features))\n\n# Apply numerical anomaly detection\nanomaly_layer = NumericalAnomalyDetection(\n    hidden_dims=[8, 4],\n    reconstruction_weight=0.5,\n    distribution_weight=0.5\n)\nanomaly_scores = anomaly_layer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 5)\nprint(f\"Anomaly scores shape: {anomaly_scores.shape}\")  # (32, 5)\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import NumericalAnomalyDetection\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    NumericalAnomalyDetection(hidden_dims=[16, 8], reconstruction_weight=0.3, distribution_weight=0.7),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import NumericalAnomalyDetection\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply numerical anomaly detection\nanomaly_scores = NumericalAnomalyDetection(\n    hidden_dims=[16, 8],\n    reconstruction_weight=0.4,\n    distribution_weight=0.6\n)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(inputs)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, [outputs, anomaly_scores])\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple anomaly detection layers\ndef create_anomaly_detection_network():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Multiple anomaly detection layers\n    anomaly_scores1 = NumericalAnomalyDetection(\n        hidden_dims=[32, 16],\n        reconstruction_weight=0.3,\n        distribution_weight=0.7\n    )(inputs)\n\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    anomaly_scores2 = NumericalAnomalyDetection(\n        hidden_dims=[24, 12],\n        reconstruction_weight=0.4,\n        distribution_weight=0.6\n    )(x)\n\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [classification, regression, anomaly, anomaly_scores1, anomaly_scores2])\n\nmodel = create_anomaly_detection_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'classification': 1.0, 'regression': 0.5, 'anomaly': 0.3}\n)\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/numerical-anomaly-detection/#kmr.layers.NumericalAnomalyDetection","title":"kmr.layers.NumericalAnomalyDetection","text":""},{"location":"layers/numerical-anomaly-detection/#kmr.layers.NumericalAnomalyDetection-classes","title":"Classes","text":""},{"location":"layers/numerical-anomaly-detection/#kmr.layers.NumericalAnomalyDetection.NumericalAnomalyDetection","title":"NumericalAnomalyDetection","text":"<pre><code>NumericalAnomalyDetection(\n    hidden_dims: list[int],\n    reconstruction_weight: float = 0.5,\n    distribution_weight: float = 0.5,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Numerical anomaly detection layer for identifying outliers in numerical features.</p> <p>This layer learns a distribution for each numerical feature and outputs an anomaly score for each feature based on how far it deviates from the learned distribution. The layer uses a combination of mean, variance, and autoencoder reconstruction error to detect anomalies.</p> Example <pre><code>import tensorflow as tf\nfrom kmr.layers import NumericalAnomalyDetection\n\n# Suppose we have 5 numerical features\nx = tf.random.normal((32, 5))  # Batch of 32 samples\n# Create a NumericalAnomalyDetection layer\nanomaly_layer = NumericalAnomalyDetection(\n    hidden_dims=[8, 4],\n    reconstruction_weight=0.5,\n    distribution_weight=0.5\n)\nanomaly_scores = anomaly_layer(x)\nprint(\"Anomaly scores shape:\", anomaly_scores.shape)  # Expected: (32, 5)\n</code></pre> <p>Initialize the layer.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dims</code> <code>list[int]</code> <p>List of hidden dimensions for the autoencoder.</p> required <code>reconstruction_weight</code> <code>float</code> <p>Weight for reconstruction error in anomaly score.</p> <code>0.5</code> <code>distribution_weight</code> <code>float</code> <p>Weight for distribution-based error in anomaly score.</p> <code>0.5</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/numerical-anomaly-detection/#kmr.layers.NumericalAnomalyDetection.NumericalAnomalyDetection-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute output shape.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Input shape tuple.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Output shape tuple.</p>"},{"location":"layers/numerical-anomaly-detection/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/numerical-anomaly-detection/#hidden_dims-list","title":"<code>hidden_dims</code> (list)","text":"<ul> <li>Purpose: List of hidden dimensions for the autoencoder</li> <li>Range: [4, 2] to [128, 64, 32] (typically [16, 8] or [32, 16])</li> <li>Impact: Larger values = more complex autoencoder but more parameters</li> <li>Recommendation: Start with [16, 8], scale based on data complexity</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#reconstruction_weight-float","title":"<code>reconstruction_weight</code> (float)","text":"<ul> <li>Purpose: Weight for reconstruction error in anomaly score</li> <li>Range: 0.0 to 1.0 (typically 0.3-0.7)</li> <li>Impact: Higher values = more emphasis on reconstruction error</li> <li>Recommendation: Use 0.3-0.7 based on data characteristics</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#distribution_weight-float","title":"<code>distribution_weight</code> (float)","text":"<ul> <li>Purpose: Weight for distribution-based error in anomaly score</li> <li>Range: 0.0 to 1.0 (typically 0.3-0.7)</li> <li>Impact: Higher values = more emphasis on distribution error</li> <li>Recommendation: Use 0.3-0.7, should sum to 1.0 with reconstruction_weight</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with hidden dimensions</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to autoencoder</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for numerical anomaly detection</li> <li>Best For: Numerical data with potential outliers</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/numerical-anomaly-detection/#example-1-outlier-detection","title":"Example 1: Outlier Detection","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import NumericalAnomalyDetection\n\n# Create a model for outlier detection\ndef create_outlier_detection_model():\n    inputs = keras.Input(shape=(15,))  # 15 features\n\n    # Anomaly detection layer\n    anomaly_scores = NumericalAnomalyDetection(\n        hidden_dims=[16, 8],\n        reconstruction_weight=0.4,\n        distribution_weight=0.6\n    )(inputs)\n\n    # Process features\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, [outputs, anomaly_scores])\n\nmodel = create_outlier_detection_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 15))\npredictions, anomaly_scores = model(sample_data)\nprint(f\"Outlier detection predictions shape: {predictions.shape}\")\nprint(f\"Anomaly scores shape: {anomaly_scores.shape}\")\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#example-2-anomaly-analysis","title":"Example 2: Anomaly Analysis","text":"<pre><code># Analyze anomaly detection behavior\ndef analyze_anomaly_detection():\n    # Create model with anomaly detection\n    inputs = keras.Input(shape=(12,))\n    anomaly_scores = NumericalAnomalyDetection(\n        hidden_dims=[8, 4],\n        reconstruction_weight=0.5,\n        distribution_weight=0.5\n    )(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(inputs)\n\n    model = keras.Model(inputs, [outputs, anomaly_scores])\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 12)),  # Random data\n        keras.random.normal((10, 12)) * 2,  # Scaled data\n        keras.random.normal((10, 12)) + 1,  # Shifted data\n    ]\n\n    print(\"Anomaly Detection Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction, anomaly = model(test_input)\n        print(f\"Test {i+1}: Anomaly mean = {keras.ops.mean(anomaly):.4f}\")\n\n    return model\n\n# Analyze anomaly detection\n# model = analyze_anomaly_detection()\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#example-3-reconstruction-analysis","title":"Example 3: Reconstruction Analysis","text":"<pre><code># Analyze reconstruction behavior\ndef analyze_reconstruction():\n    # Create model with anomaly detection\n    inputs = keras.Input(shape=(10,))\n    anomaly_scores = NumericalAnomalyDetection(\n        hidden_dims=[8, 4],\n        reconstruction_weight=0.5,\n        distribution_weight=0.5\n    )(inputs)\n\n    model = keras.Model(inputs, anomaly_scores)\n\n    # Test with sample data\n    sample_data = keras.random.normal((50, 10))\n    anomaly_scores = model(sample_data)\n\n    print(\"Reconstruction Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {sample_data.shape}\")\n    print(f\"Anomaly scores shape: {anomaly_scores.shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze reconstruction\n# model = analyze_reconstruction()\n</code></pre>"},{"location":"layers/numerical-anomaly-detection/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Hidden Dimensions: Start with [16, 8], scale based on data complexity</li> <li>Weight Balance: Balance reconstruction and distribution weights</li> <li>Feature Normalization: Works best with normalized input features</li> <li>Anomaly Threshold: Set appropriate thresholds for anomaly detection</li> <li>Autoencoder Training: Ensure autoencoder is well-trained</li> <li>Distribution Learning: Monitor distribution learning progress</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Hidden Dimensions: Must be positive integers</li> <li>Weight Sum: Reconstruction and distribution weights should sum to 1.0</li> <li>Memory Usage: Scales with hidden dimensions</li> <li>Overfitting: Monitor for overfitting with complex autoencoders</li> <li>Anomaly Threshold: May need tuning for different datasets</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>CategoricalAnomalyDetectionLayer - Categorical anomaly detection</li> <li>BusinessRulesLayer - Business rules validation</li> <li>FeatureCutout - Feature regularization</li> <li>DistributionAwareEncoder - Distribution-aware encoding</li> </ul>"},{"location":"layers/numerical-anomaly-detection/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Anomaly Detection - Anomaly detection concepts</li> <li>Autoencoders - Autoencoder concepts</li> <li>Outlier Detection - Outlier detection techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/row-attention/","title":"\ud83d\udccb RowAttention\ud83d\udccb RowAttention","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/row-attention/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>RowAttention</code> layer implements a row-wise attention mechanism that dynamically weights samples based on their importance and relevance. Unlike traditional attention mechanisms that focus on feature relationships, this layer learns to assign attention weights to each sample (row) in the batch, allowing the model to focus on the most informative samples for each prediction.</p> <p>This layer is particularly useful for sample weighting, outlier handling, and improving model performance by learning which samples are most important for the current context.</p>"},{"location":"layers/row-attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The RowAttention layer processes tabular data through a sample-wise attention mechanism:</p> <ol> <li>Sample Analysis: Analyzes each sample to understand its importance</li> <li>Attention Weight Generation: Uses a neural network to compute attention weights for each sample</li> <li>Softmax Normalization: Normalizes weights across the batch using softmax</li> <li>Dynamic Weighting: Applies learned weights to scale sample importance</li> </ol> <pre><code>graph TD\n    A[Input: batch_size, num_features] --&gt; B[Sample Analysis]\n    B --&gt; C[Attention Network]\n    C --&gt; D[Sigmoid Activation]\n    D --&gt; E[Softmax Normalization]\n    E --&gt; F[Attention Weights]\n    A --&gt; G[Element-wise Multiplication]\n    F --&gt; G\n    G --&gt; H[Weighted Samples Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style H fill:#e8f5e9,stroke:#66bb6a\n    style C fill:#fff9e6,stroke:#ffb74d\n    style E fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/row-attention/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach RowAttention's Solution Sample Importance Treat all samples equally \ud83c\udfaf Automatic learning of sample importance per batch Outlier Handling Outliers can skew predictions \u26a1 Dynamic weighting to down-weight outliers Data Quality No distinction between good/bad samples \ud83d\udc41\ufe0f Quality-aware processing based on sample characteristics Batch Effects Ignore sample relationships within batch \ud83d\udd17 Context-aware weighting based on batch composition"},{"location":"layers/row-attention/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Sample Weighting: Automatically identifying and emphasizing important samples</li> <li>Outlier Detection: Down-weighting outliers and noisy samples</li> <li>Data Quality: Handling datasets with varying sample quality</li> <li>Batch Processing: Learning sample importance within each batch</li> <li>Imbalanced Data: Balancing the influence of different sample types</li> </ul>"},{"location":"layers/row-attention/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/row-attention/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import RowAttention\n\n# Create sample tabular data\nbatch_size, num_features = 32, 10\nx = keras.random.normal((batch_size, num_features))\n\n# Apply row attention\nattention = RowAttention(feature_dim=num_features)\nweighted_samples = attention(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10)\nprint(f\"Output shape: {weighted_samples.shape}\")  # (32, 10)\n</code></pre>"},{"location":"layers/row-attention/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import RowAttention\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    RowAttention(feature_dim=64),  # Apply attention to 64 features\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/row-attention/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import RowAttention\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Process features\nx = keras.layers.Dense(64, activation='relu')(inputs)\nx = RowAttention(feature_dim=64)(x)  # Apply row attention\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/row-attention/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom hidden dimension\nattention = RowAttention(\n    feature_dim=128,\n    hidden_dim=64,  # Custom hidden layer size\n    name=\"custom_row_attention\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50,))\nx = keras.layers.Dense(128, activation='relu')(inputs)\nx = attention(x)  # Apply row attention\nx = keras.layers.LayerNormalization()(x)\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.3)(x)\noutputs = keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/row-attention/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/row-attention/#kmr.layers.RowAttention","title":"kmr.layers.RowAttention","text":"<p>Row attention mechanism for weighting samples in a batch.</p>"},{"location":"layers/row-attention/#kmr.layers.RowAttention-classes","title":"Classes","text":""},{"location":"layers/row-attention/#kmr.layers.RowAttention.RowAttention","title":"RowAttention","text":"<pre><code>RowAttention(\n    feature_dim: int,\n    hidden_dim: int | None = None,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Row attention mechanism to weight samples dynamically.</p> <p>This layer applies attention weights to each sample (row) in the input tensor. The attention weights are computed using a two-layer neural network that takes each sample as input and outputs a scalar attention weight.</p> Example <pre><code>import tensorflow as tf\nfrom kmr.layers import RowAttention\n\n# Create sample data\nbatch_size = 32\nfeature_dim = 10\ninputs = tf.random.normal((batch_size, feature_dim))\n\n# Apply row attention\nattention = RowAttention(feature_dim=feature_dim)\nweighted_outputs = attention(inputs)\n</code></pre> <p>Initialize row attention.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>Number of input features</p> required <code>hidden_dim</code> <code>int | None</code> <p>Hidden layer dimension. If None, uses feature_dim // 2</p> <code>None</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments</p> <code>{}</code>"},{"location":"layers/row-attention/#kmr.layers.RowAttention.RowAttention-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; RowAttention\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>RowAttention</code> <p>RowAttention instance</p>"},{"location":"layers/row-attention/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/row-attention/#feature_dim-int","title":"<code>feature_dim</code> (int)","text":"<ul> <li>Purpose: Number of input features for each sample</li> <li>Range: 1 to 1000+ (typically 10-100)</li> <li>Impact: Must match the number of features in your input</li> <li>Recommendation: Set to the output dimension of your previous layer</li> </ul>"},{"location":"layers/row-attention/#hidden_dim-int-optional","title":"<code>hidden_dim</code> (int, optional)","text":"<ul> <li>Purpose: Size of the hidden layer in the attention network</li> <li>Range: 1 to feature_dim (default: feature_dim // 2)</li> <li>Impact: Larger values = more complex attention patterns but more parameters</li> <li>Recommendation: Start with default, increase for complex sample relationships</li> </ul>"},{"location":"layers/row-attention/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple neural network computation</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Good for sample importance and outlier handling</li> <li>Best For: Tabular data where sample importance varies by context</li> </ul>"},{"location":"layers/row-attention/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/row-attention/#example-1-outlier-detection-and-handling","title":"Example 1: Outlier Detection and Handling","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import RowAttention\n\n# Create data with outliers\nnp.random.seed(42)\nbatch_size, num_features = 100, 8\n\n# Normal samples\nnormal_samples = np.random.normal(0, 1, (80, num_features))\n# Outlier samples (much higher variance)\noutlier_samples = np.random.normal(0, 5, (20, num_features))\nx = np.vstack([normal_samples, outlier_samples])\n\n# Build model with row attention to handle outliers\ninputs = keras.Input(shape=(num_features,))\nx = keras.layers.Dense(16, activation='relu')(inputs)\nx = RowAttention(feature_dim=16)(x)  # Learn sample importance\nx = keras.layers.Dense(8, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train and analyze attention weights\ny = np.concatenate([np.zeros(80), np.ones(20)])  # Outliers are class 1\nmodel.fit(x, y, epochs=20, verbose=0)\n\n# Get attention weights for interpretability\nattention_layer = model.layers[2]  # RowAttention layer\nattention_weights = attention_layer.attention_net(x[:10])  # Get weights for first 10 samples\nprint(\"Attention weights shape:\", attention_weights.shape)\nprint(\"Sample attention weights:\", attention_weights.flatten()[:10])\n</code></pre>"},{"location":"layers/row-attention/#example-2-imbalanced-data-handling","title":"Example 2: Imbalanced Data Handling","text":"<pre><code># Handle imbalanced data with row attention\ndef create_balanced_model():\n    inputs = keras.Input(shape=(15,))\n\n    # Feature processing\n    x = keras.layers.Dense(64, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Row attention to balance sample influence\n    x = RowAttention(feature_dim=64, hidden_dim=32)(x)\n\n    # Additional processing\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.3)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(outputs)\n\n    return keras.Model(inputs, outputs)\n\n# Use with imbalanced data\nmodel = create_balanced_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# The row attention will automatically learn to balance sample influence\n</code></pre>"},{"location":"layers/row-attention/#example-3-quality-aware-processing","title":"Example 3: Quality-Aware Processing","text":"<pre><code># Process data with varying quality using row attention\ndef create_quality_aware_model():\n    inputs = keras.Input(shape=(25,))\n\n    # Initial feature processing\n    x = keras.layers.Dense(128, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Row attention to focus on high-quality samples\n    x = RowAttention(feature_dim=128, hidden_dim=64)(x)\n\n    # Quality-aware processing\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Multiple outputs\n    quality_score = keras.layers.Dense(1, activation='sigmoid', name='quality')(x)\n    prediction = keras.layers.Dense(3, activation='softmax', name='prediction')(x)\n\n    return keras.Model(inputs, [quality_score, prediction])\n\nmodel = create_quality_aware_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'quality': 'binary_crossentropy', 'prediction': 'categorical_crossentropy'},\n    loss_weights={'quality': 0.3, 'prediction': 1.0}\n)\n</code></pre>"},{"location":"layers/row-attention/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Placement: Use after initial feature processing but before final predictions</li> <li>Hidden Dimension: Start with feature_dim // 2, adjust based on complexity</li> <li>Batch Size: Works best with larger batch sizes for better softmax normalization</li> <li>Regularization: Combine with dropout and batch normalization for better generalization</li> <li>Interpretability: Access attention weights to understand sample importance</li> <li>Data Quality: Particularly effective with noisy or imbalanced data</li> </ul>"},{"location":"layers/row-attention/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 2D tensor (batch_size, feature_dim)</li> <li>Dimension Mismatch: feature_dim must match the number of features</li> <li>Small Batches: Softmax normalization works better with larger batches</li> <li>Overfitting: Can overfit on small datasets - use regularization</li> <li>Memory: Hidden dimension affects memory usage - keep reasonable</li> </ul>"},{"location":"layers/row-attention/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>ColumnAttention - Column-wise attention for feature relationships</li> <li>TabularAttention - General tabular attention mechanism</li> <li>SparseAttentionWeighting - Sparse attention weights</li> <li>VariableSelection - Feature selection layer</li> </ul>"},{"location":"layers/row-attention/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Mechanisms in Deep Learning - Understanding attention mechanisms</li> <li>Sample Weighting in Machine Learning - Sample weighting concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Quality Tutorial - Complete guide to data quality handling</li> </ul>"},{"location":"layers/season-layer/","title":"\ud83c\udf38 SeasonLayer\ud83c\udf38 SeasonLayer","text":"\ud83d\udfe2 Beginner \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/season-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>SeasonLayer</code> adds seasonal information based on the month, encoding it as a one-hot vector for the four seasons: Winter, Spring, Summer, and Fall. This layer is essential for temporal feature engineering where seasonal patterns are important.</p> <p>This layer takes date components and adds seasonal information, making it perfect for time series analysis, weather forecasting, and any application where seasonal patterns matter.</p>"},{"location":"layers/season-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The SeasonLayer processes date components through seasonal encoding:</p> <ol> <li>Month Extraction: Extracts month from date components</li> <li>Season Classification: Classifies months into four seasons:</li> <li>Winter: December (12), January (1), February (2)</li> <li>Spring: March (3), April (4), May (5)</li> <li>Summer: June (6), July (7), August (8)</li> <li>Fall: September (9), October (10), November (11)</li> <li>One-Hot Encoding: Creates one-hot vectors for each season</li> <li>Feature Combination: Combines original date components with seasonal features</li> <li>Output Generation: Produces 8-dimensional feature vector</li> </ol> <pre><code>graph TD\n    A[Date Components: year, month, day, day_of_week] --&gt; B[Extract Month]\n    B --&gt; C[Season Classification]\n\n    C --&gt; D[Winter: Dec, Jan, Feb]\n    C --&gt; E[Spring: Mar, Apr, May]\n    C --&gt; F[Summer: Jun, Jul, Aug]\n    C --&gt; G[Fall: Sep, Oct, Nov]\n\n    D --&gt; H[One-Hot Encoding]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    A --&gt; I[Original Components]\n    H --&gt; I\n    I --&gt; J[Combined Features: 8 dimensions]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style H fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/season-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach SeasonLayer's Solution Seasonal Patterns Manual season calculation \ud83c\udfaf Automatic season classification and encoding Temporal Features Separate season processing \u26a1 Integrated seasonal information with date components One-Hot Encoding Manual one-hot encoding \ud83e\udde0 Built-in one-hot encoding for seasons Feature Engineering Complex season extraction \ud83d\udd17 Simple seasonal feature addition"},{"location":"layers/season-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Weather Forecasting: Adding seasonal context to weather predictions</li> <li>Sales Analysis: Analyzing seasonal sales patterns</li> <li>Agricultural Planning: Incorporating seasonal information for crop planning</li> <li>Energy Consumption: Predicting energy usage based on seasons</li> <li>Event Planning: Considering seasonal factors in event planning</li> </ul>"},{"location":"layers/season-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/season-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import SeasonLayer\n\n# Create sample date components [year, month, day, day_of_week]\ndate_components = keras.ops.convert_to_tensor([\n    [2023, 1, 15, 6],   # Winter (January)\n    [2023, 4, 15, 5],   # Spring (April)\n    [2023, 7, 15, 5],   # Summer (July)\n    [2023, 10, 15, 6]   # Fall (October)\n], dtype=\"float32\")\n\n# Apply seasonal encoding\nseason_layer = SeasonLayer()\nseasonal_features = season_layer(date_components)\n\nprint(f\"Input shape: {date_components.shape}\")    # (4, 4)\nprint(f\"Output shape: {seasonal_features.shape}\") # (4, 8)\nprint(f\"Seasonal features: {seasonal_features}\")\n# Output: [year, month, day, day_of_week, winter, spring, summer, fall]\n</code></pre>"},{"location":"layers/season-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import SeasonLayer\n\nmodel = keras.Sequential([\n    SeasonLayer(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/season-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import SeasonLayer\n\n# Define inputs\ninputs = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n# Apply seasonal encoding\nx = SeasonLayer()(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/season-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with seasonal analysis\ndef create_seasonal_analysis_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply seasonal encoding\n    seasonal_features = SeasonLayer()(date_input)\n\n    # Process seasonal features\n    x = keras.layers.Dense(64, activation='relu')(seasonal_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Multi-task output\n    temperature = keras.layers.Dense(1, name='temperature')(x)\n    humidity = keras.layers.Dense(1, name='humidity')(x)\n    season = keras.layers.Dense(4, activation='softmax', name='season')(x)\n\n    return keras.Model(date_input, [temperature, humidity, season])\n\nmodel = create_seasonal_analysis_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'temperature': 'mse', 'humidity': 'mse', 'season': 'categorical_crossentropy'},\n    loss_weights={'temperature': 1.0, 'humidity': 0.5, 'season': 0.3}\n)\n</code></pre>"},{"location":"layers/season-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/season-layer/#kmr.layers.SeasonLayer","title":"kmr.layers.SeasonLayer","text":"<p>SeasonLayer for adding seasonal information based on month.</p> <p>This layer adds seasonal information based on the month, encoding it as a one-hot vector for the four seasons: Winter, Spring, Summer, and Fall.</p>"},{"location":"layers/season-layer/#kmr.layers.SeasonLayer-classes","title":"Classes","text":""},{"location":"layers/season-layer/#kmr.layers.SeasonLayer.SeasonLayer","title":"SeasonLayer","text":"<pre><code>SeasonLayer(**kwargs)\n</code></pre> <p>Layer for adding seasonal information based on month.</p> <p>This layer adds seasonal information based on the month, encoding it as a one-hot vector for the four seasons: Winter, Spring, Summer, and Fall.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional layer arguments</p> <code>{}</code> Input shape <p>Tensor with shape: <code>(..., 4)</code> containing [year, month, day, day_of_week]</p> Output shape <p>Tensor with shape: <code>(..., 8)</code> containing the original 4 components plus 4 one-hot encoded season values</p> <p>Initialize the layer.</p>"},{"location":"layers/season-layer/#kmr.layers.SeasonLayer.SeasonLayer-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape,\n) -&gt; tuple[tuple[int, ...], tuple[int, ...]]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <p>Shape of the input tensor</p> required <p>Returns:</p> Type Description <code>tuple[tuple[int, ...], tuple[int, ...]]</code> <p>Output shape</p>"},{"location":"layers/season-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/season-layer/#no-parameters","title":"No Parameters","text":"<ul> <li>Purpose: This layer has no configurable parameters</li> <li>Behavior: Automatically classifies months into four seasons</li> <li>Output: Always produces 8-dimensional output (4 original + 4 seasonal)</li> </ul>"},{"location":"layers/season-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple conditional logic</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for seasonal feature extraction</li> <li>Best For: Temporal data requiring seasonal information</li> </ul>"},{"location":"layers/season-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/season-layer/#example-1-weather-prediction-with-seasons","title":"Example 1: Weather Prediction with Seasons","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import SeasonLayer\n\n# Create weather prediction model with seasonal information\ndef create_weather_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply seasonal encoding\n    seasonal_features = SeasonLayer()(date_input)\n\n    # Process seasonal features\n    x = keras.layers.Dense(128, activation='relu')(seasonal_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.3)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Weather predictions\n    temperature = keras.layers.Dense(1, name='temperature')(x)\n    humidity = keras.layers.Dense(1, name='humidity')(x)\n    precipitation = keras.layers.Dense(1, name='precipitation')(x)\n    wind_speed = keras.layers.Dense(1, name='wind_speed')(x)\n\n    return keras.Model(date_input, [temperature, humidity, precipitation, wind_speed])\n\nmodel = create_weather_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'temperature': 'mse', 'humidity': 'mse', 'precipitation': 'mse', 'wind_speed': 'mse'},\n    loss_weights={'temperature': 1.0, 'humidity': 0.5, 'precipitation': 0.3, 'wind_speed': 0.2}\n)\n\n# Test with sample data\nsample_dates = keras.ops.convert_to_tensor([\n    [2023, 1, 15, 6],   # Winter Sunday\n    [2023, 4, 15, 5],   # Spring Saturday\n    [2023, 7, 15, 5],   # Summer Saturday\n    [2023, 10, 15, 6]   # Fall Sunday\n], dtype=\"float32\")\n\npredictions = model(sample_dates)\nprint(f\"Weather predictions: {[p.shape for p in predictions]}\")\n</code></pre>"},{"location":"layers/season-layer/#example-2-sales-analysis-with-seasonal-patterns","title":"Example 2: Sales Analysis with Seasonal Patterns","text":"<pre><code># Analyze sales patterns with seasonal information\ndef create_sales_analysis_model():\n    # Input for date components\n    date_input = keras.Input(shape=(4,))  # [year, month, day, day_of_week]\n\n    # Apply seasonal encoding\n    seasonal_features = SeasonLayer()(date_input)\n\n    # Process seasonal features\n    x = keras.layers.Dense(64, activation='relu')(seasonal_features)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Sales predictions\n    sales_volume = keras.layers.Dense(1, name='sales_volume')(x)\n    revenue = keras.layers.Dense(1, name='revenue')(x)\n    is_peak_season = keras.layers.Dense(1, activation='sigmoid', name='is_peak_season')(x)\n\n    return keras.Model(date_input, [sales_volume, revenue, is_peak_season])\n\nmodel = create_sales_analysis_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'sales_volume': 'mse', 'revenue': 'mse', 'is_peak_season': 'binary_crossentropy'},\n    loss_weights={'sales_volume': 1.0, 'revenue': 0.5, 'is_peak_season': 0.3}\n)\n</code></pre>"},{"location":"layers/season-layer/#example-3-seasonal-feature-analysis","title":"Example 3: Seasonal Feature Analysis","text":"<pre><code># Analyze seasonal features produced by the layer\ndef analyze_seasonal_features():\n    # Create sample date components for each season\n    dates = keras.ops.convert_to_tensor([\n        [2023, 1, 15, 6],   # Winter (January)\n        [2023, 4, 15, 5],   # Spring (April)\n        [2023, 7, 15, 5],   # Summer (July)\n        [2023, 10, 15, 6]   # Fall (October)\n    ], dtype=\"float32\")\n\n    # Apply seasonal encoding\n    season_layer = SeasonLayer()\n    seasonal_features = season_layer(dates)\n\n    # Analyze seasonal patterns\n    print(\"Seasonal Feature Analysis:\")\n    print(\"=\" * 60)\n    print(\"Date\\t\\tMonth\\tWinter\\tSpring\\tSummer\\tFall\")\n    print(\"-\" * 60)\n\n    for i, date in enumerate(dates):\n        year, month, day, dow = date.numpy()\n        year, month, day, dow, winter, spring, summer, fall = seasonal_features[i].numpy()\n\n        print(f\"{int(year)}-{int(month):02d}-{int(day):02d}\\t{int(month)}\\t\"\n              f\"{int(winter)}\\t{int(spring)}\\t{int(summer)}\\t{int(fall)}\")\n\n    return seasonal_features\n\n# Analyze seasonal features\n# seasonal_data = analyze_seasonal_features()\n</code></pre>"},{"location":"layers/season-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Input Format: Input must be [year, month, day, day_of_week] format</li> <li>Season Classification: Automatically classifies months into four seasons</li> <li>One-Hot Encoding: Produces one-hot encoded seasonal features</li> <li>Feature Combination: Combines original date components with seasonal features</li> <li>Neural Networks: Works well with neural networks for seasonal patterns</li> <li>Integration: Combines well with other temporal processing layers</li> </ul>"},{"location":"layers/season-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be (..., 4) tensor with date components</li> <li>Component Order: Must be [year, month, day, day_of_week] in that order</li> <li>Data Type: Input should be float32 tensor</li> <li>Missing Values: Doesn't handle missing values - preprocess first</li> <li>Season Definition: Uses standard Northern Hemisphere season definitions</li> </ul>"},{"location":"layers/season-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>DateParsingLayer - Date string parsing</li> <li>DateEncodingLayer - Cyclical date encoding</li> <li>CastToFloat32Layer - Type casting utility</li> <li>DifferentiableTabularPreprocessor - End-to-end preprocessing</li> </ul>"},{"location":"layers/season-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Seasonal Patterns in Time Series - Seasonality concepts</li> <li>One-Hot Encoding - One-hot encoding techniques</li> <li>Temporal Feature Engineering - Feature engineering techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Data Preprocessing Tutorial - Complete guide to data preprocessing</li> </ul>"},{"location":"layers/slow-network/","title":"\ud83d\udc0c SlowNetwork\ud83d\udc0c SlowNetwork","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/slow-network/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>SlowNetwork</code> is a multi-layer network with configurable depth and width that processes input features through multiple dense layers with ReLU activations, then projects the output back to the original feature dimension. This layer is designed to be used as a component in more complex architectures.</p> <p>This layer is particularly powerful for complex feature processing where you need deep transformations while maintaining the original feature dimension, making it ideal for sophisticated feature engineering and complex pattern recognition.</p>"},{"location":"layers/slow-network/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The SlowNetwork processes data through a multi-layer transformation:</p> <ol> <li>Input Processing: Takes input features of specified dimension</li> <li>Hidden Layers: Applies multiple dense layers with ReLU activations</li> <li>Feature Transformation: Transforms features through the hidden layers</li> <li>Output Projection: Projects back to original input dimension</li> <li>Output Generation: Produces transformed features with same shape</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Hidden Layer 1]\n    B --&gt; C[Hidden Layer 2]\n    C --&gt; D[Hidden Layer N]\n    D --&gt; E[Output Projection]\n    E --&gt; F[Transformed Features]\n\n    G[ReLU Activation] --&gt; B\n    G --&gt; C\n    G --&gt; D\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style F fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#fff9e6,stroke:#ffb74d\n    style D fill:#fff9e6,stroke:#ffb74d\n    style E fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/slow-network/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach SlowNetwork's Solution Complex Processing Single dense layer \ud83c\udfaf Multi-layer processing for complex transformations Feature Dimension Fixed output dimension \u26a1 Maintains input dimension while processing Deep Transformations Limited transformation depth \ud83e\udde0 Configurable depth for complex patterns Architecture Components Manual layer stacking \ud83d\udd17 Pre-built component for complex architectures"},{"location":"layers/slow-network/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Complex Feature Processing: Deep transformations of input features</li> <li>Architecture Components: Building blocks for complex architectures</li> <li>Feature Engineering: Sophisticated feature transformation</li> <li>Pattern Recognition: Complex pattern recognition in features</li> <li>Dimensionality Preservation: Maintaining input dimension while processing</li> </ul>"},{"location":"layers/slow-network/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/slow-network/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import SlowNetwork\n\n# Create sample input data\nbatch_size, input_dim = 32, 16\nx = keras.random.normal((batch_size, input_dim))\n\n# Apply slow network\nslow_net = SlowNetwork(input_dim=16, num_layers=3, units=64)\noutput = slow_net(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 16)\nprint(f\"Output shape: {output.shape}\")     # (32, 16)\n</code></pre>"},{"location":"layers/slow-network/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import SlowNetwork\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    SlowNetwork(input_dim=32, num_layers=3, units=64),\n    keras.layers.Dense(16, activation='relu'),\n    SlowNetwork(input_dim=16, num_layers=2, units=32),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/slow-network/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import SlowNetwork\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply slow network\nx = SlowNetwork(input_dim=20, num_layers=3, units=64)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = SlowNetwork(input_dim=32, num_layers=2, units=32)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/slow-network/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple slow networks\ndef create_complex_network():\n    inputs = keras.Input(shape=(30,))\n\n    # Multiple slow networks with different configurations\n    x = SlowNetwork(input_dim=30, num_layers=4, units=128)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = SlowNetwork(input_dim=64, num_layers=3, units=96)(x)\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    x = SlowNetwork(input_dim=48, num_layers=2, units=64)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_complex_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/slow-network/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/slow-network/#kmr.layers.SlowNetwork","title":"kmr.layers.SlowNetwork","text":"<p>This module implements a SlowNetwork layer that processes features through multiple dense layers. It's designed to be used as a component in more complex architectures.</p>"},{"location":"layers/slow-network/#kmr.layers.SlowNetwork-classes","title":"Classes","text":""},{"location":"layers/slow-network/#kmr.layers.SlowNetwork.SlowNetwork","title":"SlowNetwork","text":"<pre><code>SlowNetwork(\n    input_dim: int,\n    num_layers: int = 3,\n    units: int = 128,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>A multi-layer network with configurable depth and width.</p> <p>This layer processes input features through multiple dense layers with ReLU activations, and projects the output back to the original feature dimension.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers. Default is 3.</p> <code>3</code> <code>units</code> <code>int</code> <p>Number of units per hidden layer. Default is 128.</p> <code>128</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, input_dim)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, input_dim)</code> (same as input)</p> Example <pre><code>import keras\nfrom kmr.layers import SlowNetwork\n\n# Create sample input data\nx = keras.random.normal((32, 16))  # 32 samples, 16 features\n\n# Create the layer\nslow_net = SlowNetwork(input_dim=16, num_layers=3, units=64)\ny = slow_net(x)\nprint(\"Output shape:\", y.shape)  # (32, 16)\n</code></pre> <p>Initialize the SlowNetwork layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>num_layers</code> <code>int</code> <p>Number of hidden layers.</p> <code>3</code> <code>units</code> <code>int</code> <p>Number of units in each layer.</p> <code>128</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/slow-network/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/slow-network/#input_dim-int","title":"<code>input_dim</code> (int)","text":"<ul> <li>Purpose: Dimension of the input features</li> <li>Range: 1 to 1000+ (typically 16-256)</li> <li>Impact: Determines the input and output feature dimension</li> <li>Recommendation: Match the actual input feature dimension</li> </ul>"},{"location":"layers/slow-network/#num_layers-int","title":"<code>num_layers</code> (int)","text":"<ul> <li>Purpose: Number of hidden layers</li> <li>Range: 1 to 20+ (typically 2-5)</li> <li>Impact: More layers = more complex transformations</li> <li>Recommendation: Start with 3, scale based on complexity needs</li> </ul>"},{"location":"layers/slow-network/#units-int","title":"<code>units</code> (int)","text":"<ul> <li>Purpose: Number of units per hidden layer</li> <li>Range: 16 to 512+ (typically 64-256)</li> <li>Impact: Larger values = more complex transformations</li> <li>Recommendation: Start with 64-128, scale based on data complexity</li> </ul>"},{"location":"layers/slow-network/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium networks, scales with layers and units</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multiple dense layers</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex feature transformation</li> <li>Best For: Complex feature processing while maintaining input dimension</li> </ul>"},{"location":"layers/slow-network/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/slow-network/#example-1-complex-feature-processing","title":"Example 1: Complex Feature Processing","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import SlowNetwork\n\n# Create a complex feature processing model\ndef create_complex_feature_processor():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # Multiple slow networks for different processing stages\n    x = SlowNetwork(input_dim=25, num_layers=4, units=128)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = SlowNetwork(input_dim=64, num_layers=3, units=96)(x)\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    x = SlowNetwork(input_dim=48, num_layers=2, units=64)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_complex_feature_processor()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Complex feature processor predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/slow-network/#example-2-architecture-component","title":"Example 2: Architecture Component","text":"<pre><code># Use SlowNetwork as a component in complex architecture\ndef create_component_based_architecture():\n    inputs = keras.Input(shape=(20,))\n\n    # Initial processing\n    x = keras.layers.Dense(32, activation='relu')(inputs)\n    x = keras.layers.BatchNormalization()(x)\n\n    # SlowNetwork component 1\n    x = SlowNetwork(input_dim=32, num_layers=3, units=64)(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # SlowNetwork component 2\n    x = SlowNetwork(input_dim=32, num_layers=2, units=48)(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Final processing\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_component_based_architecture()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/slow-network/#example-3-layer-analysis","title":"Example 3: Layer Analysis","text":"<pre><code># Analyze SlowNetwork behavior\ndef analyze_slow_network():\n    # Create model with SlowNetwork\n    inputs = keras.Input(shape=(15,))\n    x = SlowNetwork(input_dim=15, num_layers=3, units=32)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"SlowNetwork Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze SlowNetwork\n# model = analyze_slow_network()\n</code></pre>"},{"location":"layers/slow-network/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Input Dimension: Must match the actual input feature dimension</li> <li>Number of Layers: Start with 3, scale based on complexity needs</li> <li>Units: Use 64-128 units for most applications</li> <li>Activation Functions: ReLU is used by default, consider alternatives if needed</li> <li>Regularization: Consider adding dropout between SlowNetwork layers</li> <li>Architecture: Use as components in larger architectures</li> </ul>"},{"location":"layers/slow-network/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Dimension: Must be positive integer</li> <li>Number of Layers: Must be positive integer</li> <li>Units: Must be positive integer</li> <li>Memory Usage: Scales with number of layers and units</li> <li>Overfitting: Can overfit with too many layers/units on small datasets</li> </ul>"},{"location":"layers/slow-network/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedResidualNetwork - Gated residual networks</li> <li>TransformerBlock - Transformer processing</li> <li>BoostingBlock - Boosting block processing</li> <li>VariableSelection - Variable selection</li> </ul>"},{"location":"layers/slow-network/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Multi-Layer Networks - Multi-layer network concepts</li> <li>Feature Engineering - Feature engineering techniques</li> <li>Deep Learning - Deep learning concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/sparse-attention-weighting/","title":"\ud83c\udfaf SparseAttentionWeighting\ud83c\udfaf SparseAttentionWeighting","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/sparse-attention-weighting/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>SparseAttentionWeighting</code> layer implements a learnable attention mechanism that combines outputs from multiple modules using temperature-scaled attention weights. The attention weights are learned during training and can be made more or less sparse by adjusting the temperature parameter.</p> <p>This layer is particularly powerful for ensemble learning, multi-branch architectures, and any scenario where you need to intelligently combine outputs from different processing modules.</p>"},{"location":"layers/sparse-attention-weighting/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The SparseAttentionWeighting layer processes multiple module outputs through temperature-scaled attention:</p> <ol> <li>Module Weighting: Learns importance weights for each input module</li> <li>Temperature Scaling: Applies temperature scaling to control sparsity</li> <li>Softmax Normalization: Converts weights to attention probabilities</li> <li>Weighted Combination: Combines module outputs using attention weights</li> <li>Output Generation: Produces final combined output</li> </ol> <pre><code>graph TD\n    A[Module 1 Output] --&gt; D[Attention Weights]\n    B[Module 2 Output] --&gt; D\n    C[Module N Output] --&gt; D\n\n    D --&gt; E[Temperature Scaling]\n    E --&gt; F[Softmax Normalization]\n    F --&gt; G[Attention Probabilities]\n\n    A --&gt; H[Weighted Sum]\n    B --&gt; H\n    C --&gt; H\n    G --&gt; H\n    H --&gt; I[Combined Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style B fill:#e6f3ff,stroke:#4a86e8\n    style C fill:#e6f3ff,stroke:#4a86e8\n    style I fill:#e8f5e9,stroke:#66bb6a\n    style D fill:#fff9e6,stroke:#ffb74d\n    style G fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/sparse-attention-weighting/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach SparseAttentionWeighting's Solution Module Combination Simple concatenation or averaging \ud83c\udfaf Learned attention weights for optimal combination Sparsity Control Fixed combination strategies \u26a1 Temperature scaling for controllable sparsity Ensemble Learning Uniform weighting of models \ud83e\udde0 Adaptive weighting based on module performance Multi-Branch Networks Manual branch combination \ud83d\udd17 Automatic learning of optimal combination weights"},{"location":"layers/sparse-attention-weighting/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Ensemble Learning: Combining multiple model outputs intelligently</li> <li>Multi-Branch Architectures: Weighting different processing branches</li> <li>Attention Mechanisms: Implementing sparse attention for efficiency</li> <li>Module Selection: Learning which modules are most important</li> <li>Transfer Learning: Combining pre-trained and fine-tuned features</li> </ul>"},{"location":"layers/sparse-attention-weighting/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/sparse-attention-weighting/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import SparseAttentionWeighting\n\n# Create sample module outputs\nbatch_size, feature_dim = 32, 64\nmodule1 = keras.random.normal((batch_size, feature_dim))\nmodule2 = keras.random.normal((batch_size, feature_dim))\nmodule3 = keras.random.normal((batch_size, feature_dim))\n\n# Apply sparse attention weighting\nattention = SparseAttentionWeighting(\n    num_modules=3,\n    temperature=0.5  # Lower temperature for sharper attention\n)\ncombined = attention([module1, module2, module3])\n\nprint(f\"Input shapes: {[m.shape for m in [module1, module2, module3]]}\")\nprint(f\"Output shape: {combined.shape}\")  # (32, 64)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import SparseAttentionWeighting\n\n# Create multiple processing branches\ninputs = keras.Input(shape=(20,))\n\n# Branch 1: Linear processing\nbranch1 = keras.layers.Dense(32, activation='relu')(inputs)\nbranch1 = keras.layers.Dense(16, activation='relu')(branch1)\n\n# Branch 2: Non-linear processing\nbranch2 = keras.layers.Dense(32, activation='tanh')(inputs)\nbranch2 = keras.layers.Dense(16, activation='tanh')(branch2)\n\n# Branch 3: Residual processing\nbranch3 = keras.layers.Dense(32, activation='relu')(inputs)\nbranch3 = keras.layers.Dense(16, activation='relu')(branch3)\nbranch3 = keras.layers.Add()([branch3, inputs[:, :16]])\n\n# Combine branches with sparse attention\ncombined = SparseAttentionWeighting(\n    num_modules=3,\n    temperature=0.7\n)([branch1, branch2, branch3])\n\n# Final processing\noutputs = keras.layers.Dense(1, activation='sigmoid')(combined)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import SparseAttentionWeighting\n\n# Define inputs\ninputs = keras.Input(shape=(25,))  # 25 features\n\n# Create multiple processing paths\npath1 = keras.layers.Dense(64, activation='relu')(inputs)\npath1 = keras.layers.Dropout(0.2)(path1)\npath1 = keras.layers.Dense(32, activation='relu')(path1)\n\npath2 = keras.layers.Dense(64, activation='tanh')(inputs)\npath2 = keras.layers.BatchNormalization()(path2)\npath2 = keras.layers.Dense(32, activation='tanh')(path2)\n\npath3 = keras.layers.Dense(64, activation='swish')(inputs)\npath3 = keras.layers.Dense(32, activation='swish')(path3)\n\n# Combine paths with attention\nx = SparseAttentionWeighting(\n    num_modules=3,\n    temperature=0.5\n)([path1, path2, path3])\n\n# Final layers\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with different temperature settings\ndef create_adaptive_model():\n    inputs = keras.Input(shape=(30,))\n\n    # Create multiple specialized branches\n    branches = []\n    for i in range(4):\n        x = keras.layers.Dense(64, activation='relu')(inputs)\n        x = keras.layers.Dropout(0.1 * (i + 1))(x)  # Different dropout rates\n        x = keras.layers.Dense(32, activation='relu')(x)\n        branches.append(x)\n\n    # Combine with different temperature settings\n    # Lower temperature = more sparse attention\n    combined = SparseAttentionWeighting(\n        num_modules=4,\n        temperature=0.3  # Very sparse attention\n    )(branches)\n\n    # Multi-task output\n    task1 = keras.layers.Dense(1, activation='sigmoid', name='binary')(combined)\n    task2 = keras.layers.Dense(5, activation='softmax', name='multiclass')(combined)\n\n    return keras.Model(inputs, [task1, task2])\n\nmodel = create_adaptive_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'binary': 'binary_crossentropy', 'multiclass': 'categorical_crossentropy'},\n    loss_weights={'binary': 1.0, 'multiclass': 0.5}\n)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/sparse-attention-weighting/#kmr.layers.SparseAttentionWeighting","title":"kmr.layers.SparseAttentionWeighting","text":""},{"location":"layers/sparse-attention-weighting/#kmr.layers.SparseAttentionWeighting-classes","title":"Classes","text":""},{"location":"layers/sparse-attention-weighting/#kmr.layers.SparseAttentionWeighting.SparseAttentionWeighting","title":"SparseAttentionWeighting","text":"<pre><code>SparseAttentionWeighting(\n    num_modules: int,\n    temperature: float = 1.0,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Sparse attention mechanism with temperature scaling for module outputs combination.</p> <p>This layer implements a learnable attention mechanism that combines outputs from multiple modules using temperature-scaled attention weights. The attention weights are learned during training and can be made more or less sparse by adjusting the temperature parameter. A higher temperature leads to more uniform weights, while a lower temperature makes the weights more concentrated on specific modules.</p> <p>Key features: 1. Learnable module importance weights 2. Temperature-controlled sparsity 3. Softmax-based attention mechanism 4. Support for variable number of input features per module</p> <p>Example: <pre><code>import numpy as np\nfrom keras import layers, Model\nfrom kmr.layers import SparseAttentionWeighting\n\n# Create sample module outputs\nbatch_size = 32\nnum_modules = 3\nfeature_dim = 64\n\n# Create three different module outputs\nmodule1 = layers.Dense(feature_dim)(inputs)\nmodule2 = layers.Dense(feature_dim)(inputs)\nmodule3 = layers.Dense(feature_dim)(inputs)\n\n# Combine module outputs using sparse attention\nattention = SparseAttentionWeighting(\n    num_modules=num_modules,\n    temperature=0.5  # Lower temperature for sharper attention\n)\ncombined_output = attention([module1, module2, module3])\n\n# The layer will learn which modules are most important\n# and weight their outputs accordingly\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>num_modules</code> <code>int</code> <p>Number of input modules whose outputs will be combined.</p> required <code>temperature</code> <code>float</code> <p>Temperature parameter for softmax scaling. Default is 1.0. - temperature &gt; 1.0: More uniform attention weights - temperature &lt; 1.0: More sparse attention weights - temperature = 1.0: Standard softmax behavior</p> <code>1.0</code> <p>Initialize sparse attention weighting layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_modules</code> <code>int</code> <p>Number of input modules to weight. Must be positive.</p> required <code>temperature</code> <code>float</code> <p>Temperature parameter for softmax scaling. Must be positive. Controls the sparsity of attention weights: - Higher values (&gt;1.0) lead to more uniform weights - Lower values (&lt;1.0) lead to more concentrated weights</p> <code>1.0</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments passed to the parent Layer class.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_modules &lt;= 0 or temperature &lt;= 0</p>"},{"location":"layers/sparse-attention-weighting/#kmr.layers.SparseAttentionWeighting.SparseAttentionWeighting-functions","title":"Functions","text":"from_config <code>classmethod</code> <pre><code>from_config(\n    config: dict[str, Any]\n) -&gt; SparseAttentionWeighting\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>SparseAttentionWeighting</code> <p>SparseAttentionWeighting instance</p>"},{"location":"layers/sparse-attention-weighting/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/sparse-attention-weighting/#num_modules-int","title":"<code>num_modules</code> (int)","text":"<ul> <li>Purpose: Number of input modules whose outputs will be combined</li> <li>Range: 2 to 20+ (typically 2-8)</li> <li>Impact: Must match the number of input tensors</li> <li>Recommendation: Start with 2-4 modules, scale based on architecture complexity</li> </ul>"},{"location":"layers/sparse-attention-weighting/#temperature-float","title":"<code>temperature</code> (float)","text":"<ul> <li>Purpose: Temperature parameter for softmax scaling</li> <li>Range: 0.1 to 10.0 (typically 0.3-2.0)</li> <li>Impact: Controls attention sparsity</li> <li>Recommendation: </li> <li>0.1-0.5: Very sparse attention (focus on 1-2 modules)</li> <li>0.5-1.0: Moderate sparsity (balanced attention)</li> <li>1.0-2.0: More uniform attention (all modules contribute)</li> </ul>"},{"location":"layers/sparse-attention-weighting/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple weighted combination</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe Low memory usage - minimal additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for ensemble and multi-branch architectures</li> <li>Best For: Multi-module architectures requiring intelligent combination</li> </ul>"},{"location":"layers/sparse-attention-weighting/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/sparse-attention-weighting/#example-1-ensemble-model-combination","title":"Example 1: Ensemble Model Combination","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import SparseAttentionWeighting\n\n# Create ensemble of different model types\ndef create_ensemble_model():\n    inputs = keras.Input(shape=(20,))\n\n    # Model 1: Linear model\n    linear = keras.layers.Dense(32, activation='linear')(inputs)\n    linear = keras.layers.Dense(16, activation='linear')(linear)\n\n    # Model 2: Non-linear model\n    nonlinear = keras.layers.Dense(32, activation='relu')(inputs)\n    nonlinear = keras.layers.Dense(16, activation='relu')(nonlinear)\n\n    # Model 3: Deep model\n    deep = keras.layers.Dense(64, activation='relu')(inputs)\n    deep = keras.layers.Dense(32, activation='relu')(deep)\n    deep = keras.layers.Dense(16, activation='relu')(deep)\n\n    # Combine with sparse attention\n    ensemble_output = SparseAttentionWeighting(\n        num_modules=3,\n        temperature=0.4  # Sparse attention to focus on best models\n    )([linear, nonlinear, deep])\n\n    # Final prediction\n    prediction = keras.layers.Dense(1, activation='sigmoid')(ensemble_output)\n\n    return keras.Model(inputs, prediction)\n\nmodel = create_ensemble_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#example-2-multi-scale-feature-processing","title":"Example 2: Multi-Scale Feature Processing","text":"<pre><code># Process features at different scales with attention weighting\ndef create_multi_scale_model():\n    inputs = keras.Input(shape=(50,))\n\n    # Different scale processing\n    # Fine-grained features\n    fine = keras.layers.Dense(128, activation='relu')(inputs)\n    fine = keras.layers.Dense(64, activation='relu')(fine)\n    fine = keras.layers.Dense(32, activation='relu')(fine)\n\n    # Medium-grained features\n    medium = keras.layers.Dense(64, activation='relu')(inputs)\n    medium = keras.layers.Dense(32, activation='relu')(medium)\n\n    # Coarse-grained features\n    coarse = keras.layers.Dense(32, activation='relu')(inputs)\n\n    # Combine with attention\n    combined = SparseAttentionWeighting(\n        num_modules=3,\n        temperature=0.6\n    )([fine, medium, coarse])\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='class')(combined)\n    regression = keras.layers.Dense(1, name='reg')(combined)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_multi_scale_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'class': 'categorical_crossentropy', 'reg': 'mse'},\n    loss_weights={'class': 1.0, 'reg': 0.5}\n)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#example-3-attention-analysis","title":"Example 3: Attention Analysis","text":"<pre><code># Analyze attention patterns to understand module importance\ndef analyze_attention_patterns(model, test_data):\n    \"\"\"Analyze which modules are getting the most attention.\"\"\"\n    # Get the sparse attention weighting layer\n    attention_layer = None\n    for layer in model.layers:\n        if isinstance(layer, SparseAttentionWeighting):\n            attention_layer = layer\n            break\n\n    if attention_layer is None:\n        print(\"No SparseAttentionWeighting layer found\")\n        return\n\n    # Get attention weights\n    weights = attention_layer.attention_weights\n\n    # Apply temperature scaling and softmax (same as in the layer)\n    scaled_weights = weights / attention_layer.temperature\n    attention_probs = keras.ops.softmax(scaled_weights, axis=0)\n\n    print(\"Module attention weights:\")\n    for i, prob in enumerate(attention_probs):\n        print(f\"Module {i+1}: {prob:.4f}\")\n\n    # Find most important module\n    most_important = keras.ops.argmax(attention_probs)\n    print(f\"Most important module: {most_important + 1}\")\n\n    return attention_probs\n\n# Use with your model\n# attention_probs = analyze_attention_patterns(model, test_data)\n</code></pre>"},{"location":"layers/sparse-attention-weighting/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Temperature Tuning: Start with 0.5-1.0, adjust based on desired sparsity</li> <li>Module Diversity: Ensure modules have different characteristics for effective combination</li> <li>Weight Initialization: Weights are initialized to ones (equal importance)</li> <li>Gradient Flow: Attention weights are learnable and differentiable</li> <li>Monitoring: Track attention patterns to understand module importance</li> <li>Regularization: Consider adding L1 regularization to encourage sparsity</li> </ul>"},{"location":"layers/sparse-attention-weighting/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Module Count: Must match the number of input tensors exactly</li> <li>Temperature Range: Very low temperatures (&lt;0.1) can cause numerical instability</li> <li>Input Consistency: All input tensors must have the same shape</li> <li>Gradient Vanishing: Very sparse attention can lead to gradient issues</li> <li>Overfitting: Too many modules without regularization can cause overfitting</li> </ul>"},{"location":"layers/sparse-attention-weighting/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedFeatureFusion - Gated feature fusion mechanism</li> <li>VariableSelection - Dynamic feature selection</li> <li>TabularAttention - General attention mechanisms</li> <li>InterpretableMultiHeadAttention - Interpretable attention</li> </ul>"},{"location":"layers/sparse-attention-weighting/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Mechanisms in Deep Learning - Understanding attention mechanisms</li> <li>Ensemble Learning Methods - Ensemble learning concepts</li> <li>Temperature Scaling in Neural Networks - Temperature scaling techniques</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/stochastic-depth/","title":"\ud83c\udfb2 StochasticDepth\ud83c\udfb2 StochasticDepth","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/stochastic-depth/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>StochasticDepth</code> layer randomly drops entire residual branches with a specified probability during training, helping reduce overfitting and training time in deep networks. During inference, all branches are kept and scaled appropriately.</p> <p>This layer is particularly powerful for deep neural networks where overfitting is a concern, providing a regularization technique that's specifically designed for residual architectures.</p>"},{"location":"layers/stochastic-depth/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The StochasticDepth layer processes data through stochastic branch dropping:</p> <ol> <li>Training Mode: Randomly drops residual branches based on survival probability</li> <li>Inference Mode: Keeps all branches and scales by survival probability</li> <li>Random Generation: Uses random number generation for branch selection</li> <li>Scaling: Applies appropriate scaling for inference</li> <li>Output Generation: Produces regularized output</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B{Training Mode?}\n    B --&gt;|Yes| C[Random Branch Selection]\n    B --&gt;|No| D[Scale by Survival Probability]\n\n    C --&gt; E[Drop Residual Branch]\n    C --&gt; F[Keep Residual Branch]\n\n    E --&gt; G[Output = Shortcut]\n    F --&gt; H[Output = Shortcut + Residual]\n    D --&gt; I[Output = Shortcut + (Survival Prob \u00d7 Residual)]\n\n    G --&gt; J[Final Output]\n    H --&gt; J\n    I --&gt; J\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C fill:#f3e5f5,stroke:#9c27b0\n    style D fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/stochastic-depth/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach StochasticDepth's Solution Overfitting Dropout on individual neurons \ud83c\udfaf Branch-level dropout for better regularization Deep Networks Limited depth due to overfitting \u26a1 Enables deeper networks with regularization Training Time Slower training with deep networks \ud83e\udde0 Faster training by dropping branches Residual Networks Standard dropout not optimal \ud83d\udd17 Designed for residual architectures"},{"location":"layers/stochastic-depth/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Deep Neural Networks: Regularizing very deep networks</li> <li>Residual Architectures: Optimizing residual network training</li> <li>Overfitting Prevention: Reducing overfitting in complex models</li> <li>Training Acceleration: Faster training through branch dropping</li> <li>Ensemble Learning: Creating diverse network behaviors</li> </ul>"},{"location":"layers/stochastic-depth/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/stochastic-depth/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import StochasticDepth\n\n# Create sample residual branch\ninputs = keras.random.normal((32, 64, 64, 128))\nresidual = keras.layers.Conv2D(128, 3, padding=\"same\")(inputs)\nresidual = keras.layers.BatchNormalization()(residual)\nresidual = keras.layers.ReLU()(residual)\n\n# Apply stochastic depth\nstochastic_depth = StochasticDepth(survival_prob=0.8)\noutput = stochastic_depth([inputs, residual])\n\nprint(f\"Input shape: {inputs.shape}\")      # (32, 64, 64, 128)\nprint(f\"Output shape: {output.shape}\")     # (32, 64, 64, 128)\n</code></pre>"},{"location":"layers/stochastic-depth/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import StochasticDepth\n\n# Create a residual block with stochastic depth\ndef create_residual_block(inputs, filters, survival_prob=0.8):\n    # Shortcut connection\n    shortcut = inputs\n\n    # Residual branch\n    x = keras.layers.Conv2D(filters, 3, padding=\"same\")(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n    x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    # Apply stochastic depth\n    x = StochasticDepth(survival_prob=survival_prob)([shortcut, x])\n    x = keras.layers.ReLU()(x)\n\n    return x\n\n# Build model with stochastic depth\ninputs = keras.Input(shape=(32, 32, 3))\nx = keras.layers.Conv2D(64, 3, padding=\"same\")(inputs)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.ReLU()(x)\n\n# Add residual blocks with stochastic depth\nx = create_residual_block(x, 64, survival_prob=0.9)\nx = create_residual_block(x, 64, survival_prob=0.8)\nx = create_residual_block(x, 64, survival_prob=0.7)\n\n# Final layers\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = keras.Model(inputs, x)\n</code></pre>"},{"location":"layers/stochastic-depth/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import StochasticDepth\n\n# Define inputs\ninputs = keras.Input(shape=(28, 28, 3))\n\n# Initial processing\nx = keras.layers.Conv2D(32, 3, padding=\"same\")(inputs)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.ReLU()(x)\n\n# Residual block with stochastic depth\nshortcut = x\nx = keras.layers.Conv2D(32, 3, padding=\"same\")(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.ReLU()(x)\nx = keras.layers.Conv2D(32, 3, padding=\"same\")(x)\nx = keras.layers.BatchNormalization()(x)\n\n# Apply stochastic depth\nx = StochasticDepth(survival_prob=0.8)([shortcut, x])\nx = keras.layers.ReLU()(x)\n\n# Final processing\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = keras.Model(inputs, x)\n</code></pre>"},{"location":"layers/stochastic-depth/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with progressive stochastic depth\ndef create_progressive_stochastic_model():\n    inputs = keras.Input(shape=(32, 32, 3))\n\n    # Initial processing\n    x = keras.layers.Conv2D(64, 3, padding=\"same\")(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n\n    # Progressive stochastic depth (decreasing survival probability)\n    survival_probs = [0.9, 0.8, 0.7, 0.6, 0.5]\n\n    for i, survival_prob in enumerate(survival_probs):\n        shortcut = x\n        x = keras.layers.Conv2D(64, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n        x = keras.layers.Conv2D(64, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        # Apply stochastic depth with decreasing survival probability\n        x = StochasticDepth(survival_prob=survival_prob, seed=42)([shortcut, x])\n        x = keras.layers.ReLU()(x)\n\n    # Final processing\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(100, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(10, activation='softmax')(x)\n\n    return keras.Model(inputs, x)\n\nmodel = create_progressive_stochastic_model()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/stochastic-depth/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/stochastic-depth/#kmr.layers.StochasticDepth","title":"kmr.layers.StochasticDepth","text":"<p>Stochastic depth layer for neural networks.</p>"},{"location":"layers/stochastic-depth/#kmr.layers.StochasticDepth-classes","title":"Classes","text":""},{"location":"layers/stochastic-depth/#kmr.layers.StochasticDepth.StochasticDepth","title":"StochasticDepth","text":"<pre><code>StochasticDepth(\n    survival_prob: float = 0.5,\n    seed: int | None = None,\n    **kwargs: dict[str, Any]\n)\n</code></pre> <p>Stochastic depth layer for regularization.</p> <p>This layer randomly drops entire residual branches with a specified probability during training. During inference, all branches are kept and scaled appropriately. This technique helps reduce overfitting and training time in deep networks.</p> Reference <ul> <li>Deep Networks with Stochastic Depth</li> </ul> Example <pre><code>from keras import random, layers\nfrom kmr.layers import StochasticDepth\n\n# Create sample residual branch\ninputs = random.normal((32, 64, 64, 128))\nresidual = layers.Conv2D(128, 3, padding=\"same\")(inputs)\nresidual = layers.BatchNormalization()(residual)\nresidual = layers.ReLU()(residual)\n\n# Apply stochastic depth\noutputs = StochasticDepth(survival_prob=0.8)([inputs, residual])\n</code></pre> <p>Initialize stochastic depth.</p> <p>Parameters:</p> Name Type Description Default <code>survival_prob</code> <code>float</code> <p>Probability of keeping the residual branch (default: 0.5)</p> <code>0.5</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional layer arguments</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If survival_prob is not in [0, 1]</p>"},{"location":"layers/stochastic-depth/#kmr.layers.StochasticDepth.StochasticDepth-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: list[tuple[int, ...]]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute output shape.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>list[tuple[int, ...]]</code> <p>List of input shape tuples</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Output shape tuple</p> from_config <code>classmethod</code> <pre><code>from_config(config: dict[str, Any]) -&gt; StochasticDepth\n</code></pre> <p>Create layer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Layer configuration dictionary</p> required <p>Returns:</p> Type Description <code>StochasticDepth</code> <p>StochasticDepth instance</p>"},{"location":"layers/stochastic-depth/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/stochastic-depth/#survival_prob-float","title":"<code>survival_prob</code> (float)","text":"<ul> <li>Purpose: Probability of keeping the residual branch</li> <li>Range: 0.0 to 1.0 (typically 0.5-0.9)</li> <li>Impact: Higher values = less regularization, lower values = more regularization</li> <li>Recommendation: Start with 0.8, adjust based on overfitting</li> </ul>"},{"location":"layers/stochastic-depth/#seed-int-optional","title":"<code>seed</code> (int, optional)","text":"<ul> <li>Purpose: Random seed for reproducibility</li> <li>Default: None (random)</li> <li>Impact: Controls randomness of branch dropping</li> <li>Recommendation: Use fixed seed for reproducible experiments</li> </ul>"},{"location":"layers/stochastic-depth/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1\u26a1 Very fast - simple conditional logic</li> <li>Memory: \ud83d\udcbe Low memory usage - no additional parameters</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for deep network regularization</li> <li>Best For: Deep residual networks where overfitting is a concern</li> </ul>"},{"location":"layers/stochastic-depth/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/stochastic-depth/#example-1-deep-residual-network","title":"Example 1: Deep Residual Network","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import StochasticDepth\n\n# Create a deep residual network with stochastic depth\ndef create_deep_residual_network():\n    inputs = keras.Input(shape=(32, 32, 3))\n\n    # Initial processing\n    x = keras.layers.Conv2D(64, 3, padding=\"same\")(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n\n    # Multiple residual blocks with stochastic depth\n    for i in range(10):  # 10 residual blocks\n        shortcut = x\n        x = keras.layers.Conv2D(64, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n        x = keras.layers.Conv2D(64, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        # Apply stochastic depth with decreasing survival probability\n        survival_prob = 0.9 - (i * 0.05)  # Decrease from 0.9 to 0.45\n        x = StochasticDepth(survival_prob=survival_prob)([shortcut, x])\n        x = keras.layers.ReLU()(x)\n\n    # Final processing\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(100, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(10, activation='softmax')(x)\n\n    return keras.Model(inputs, x)\n\nmodel = create_deep_residual_network()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Test with sample data\nsample_data = keras.random.normal((100, 32, 32, 3))\npredictions = model(sample_data)\nprint(f\"Deep residual network predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/stochastic-depth/#example-2-stochastic-depth-analysis","title":"Example 2: Stochastic Depth Analysis","text":"<pre><code># Analyze stochastic depth behavior\ndef analyze_stochastic_depth():\n    # Create model with stochastic depth\n    inputs = keras.Input(shape=(16, 16, 64))\n    shortcut = inputs\n    residual = keras.layers.Conv2D(64, 3, padding=\"same\")(inputs)\n    residual = keras.layers.BatchNormalization()(residual)\n    residual = keras.layers.ReLU()(residual)\n\n    # Apply stochastic depth\n    x = StochasticDepth(survival_prob=0.8, seed=42)([shortcut, residual])\n\n    model = keras.Model(inputs, x)\n\n    # Test with sample data\n    test_data = keras.random.normal((10, 16, 16, 64))\n\n    print(\"Stochastic Depth Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {test_data.shape}\")\n    print(f\"Output shape: {model(test_data).shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze stochastic depth\n# model = analyze_stochastic_depth()\n</code></pre>"},{"location":"layers/stochastic-depth/#example-3-progressive-stochastic-depth","title":"Example 3: Progressive Stochastic Depth","text":"<pre><code># Create model with progressive stochastic depth\ndef create_progressive_stochastic_model():\n    inputs = keras.Input(shape=(28, 28, 3))\n\n    # Initial processing\n    x = keras.layers.Conv2D(32, 3, padding=\"same\")(inputs)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.ReLU()(x)\n\n    # Progressive stochastic depth\n    survival_probs = [0.9, 0.8, 0.7, 0.6, 0.5]\n\n    for i, survival_prob in enumerate(survival_probs):\n        shortcut = x\n        x = keras.layers.Conv2D(32, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n        x = keras.layers.Conv2D(32, 3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        # Apply stochastic depth\n        x = StochasticDepth(survival_prob=survival_prob, seed=42)([shortcut, x])\n        x = keras.layers.ReLU()(x)\n\n    # Final processing\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dense(10, activation='softmax')(x)\n\n    return keras.Model(inputs, x)\n\nmodel = create_progressive_stochastic_model()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/stochastic-depth/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Survival Probability: Start with 0.8, adjust based on overfitting</li> <li>Progressive Depth: Use decreasing survival probability for deeper layers</li> <li>Seed Setting: Use fixed seed for reproducible experiments</li> <li>Residual Networks: Works best with residual architectures</li> <li>Training Mode: Only applies during training, not inference</li> <li>Scaling: Automatic scaling during inference</li> </ul>"},{"location":"layers/stochastic-depth/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Format: Must be a list of [shortcut, residual] tensors</li> <li>Survival Probability: Must be between 0 and 1</li> <li>Training Mode: Only applies during training</li> <li>Memory Usage: No additional memory overhead</li> <li>Gradient Flow: May affect gradient flow during training</li> </ul>"},{"location":"layers/stochastic-depth/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>BoostingBlock - Boosting block with residual connections</li> <li>GatedResidualNetwork - Gated residual networks</li> <li>FeatureCutout - Feature regularization</li> <li>BusinessRulesLayer - Business rules validation</li> </ul>"},{"location":"layers/stochastic-depth/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Deep Networks with Stochastic Depth - Original stochastic depth paper</li> <li>Residual Networks - Residual network paper</li> <li>Regularization Techniques - Regularization concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/tabular-attention/","title":"\ud83e\udde0 TabularAttention\ud83e\udde0 TabularAttention","text":"\ud83d\udd25 Popular \u2705 Stable \ud83d\udfe1 Intermediate"},{"location":"layers/tabular-attention/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>TabularAttention</code> layer implements a sophisticated dual attention mechanism specifically designed for tabular data. Unlike traditional attention mechanisms that focus on sequential data, this layer captures both inter-feature relationships (how features interact within each sample) and inter-sample relationships (how samples relate to each other across features).</p> <p>This layer is particularly powerful for tabular datasets where understanding feature interactions and sample similarities is crucial for making accurate predictions. It's especially useful in scenarios where you have complex feature dependencies that traditional neural networks struggle to capture.</p>"},{"location":"layers/tabular-attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The TabularAttention layer processes tabular data through a two-stage attention mechanism:</p> <ol> <li>Inter-Feature Attention: Analyzes relationships between different features within each sample</li> <li>Inter-Sample Attention: Examines relationships between different samples across features</li> </ol> <pre><code>graph TD\n    A[Input: batch_size, num_samples, num_features] --&gt; B[Input Projection to d_model]\n    B --&gt; C[Inter-Feature Attention]\n    C --&gt; D[Feature LayerNorm + Residual]\n    D --&gt; E[Feed-Forward Network]\n    E --&gt; F[Feature LayerNorm + Residual]\n    F --&gt; G[Inter-Sample Attention]\n    G --&gt; H[Sample LayerNorm + Residual]\n    H --&gt; I[Output Projection]\n    I --&gt; J[Output: batch_size, num_samples, d_model]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style J fill:#e8f5e9,stroke:#66bb6a\n    style C fill:#fff9e6,stroke:#ffb74d\n    style G fill:#fff9e6,stroke:#ffb74d</code></pre>"},{"location":"layers/tabular-attention/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach TabularAttention's Solution Feature Interactions Manual feature engineering or simple concatenation \ud83e\udde0 Automatic discovery of complex feature relationships through attention Sample Relationships Treating samples independently \ud83d\udd17 Cross-sample learning to identify similar patterns and outliers High-Dimensional Data Dimensionality reduction or feature selection \u26a1 Efficient attention that scales to high-dimensional tabular data Interpretability Black-box models with limited insights \ud83d\udc41\ufe0f Attention weights provide insights into feature and sample importance"},{"location":"layers/tabular-attention/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Financial Risk Assessment: Understanding how different financial indicators interact and identifying similar risk profiles</li> <li>Medical Diagnosis: Capturing complex relationships between symptoms and patient characteristics</li> <li>Recommendation Systems: Learning user-item interactions and finding similar users/items</li> <li>Anomaly Detection: Identifying unusual patterns by comparing samples across features</li> <li>Feature Engineering: Automatically discovering meaningful feature combinations</li> </ul>"},{"location":"layers/tabular-attention/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/tabular-attention/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import TabularAttention\n\n# Create sample tabular data\nbatch_size, num_samples, num_features = 32, 100, 20\nx = keras.random.normal((batch_size, num_samples, num_features))\n\n# Apply tabular attention\nattention = TabularAttention(num_heads=8, d_model=64, dropout_rate=0.1)\noutput = attention(x)\n\nprint(f\"Input shape: {x.shape}\")   # (32, 100, 20)\nprint(f\"Output shape: {output.shape}\")  # (32, 100, 64)\n</code></pre>"},{"location":"layers/tabular-attention/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import TabularAttention\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    TabularAttention(num_heads=4, d_model=64, dropout_rate=0.1),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/tabular-attention/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import TabularAttention\n\n# Define inputs\ninputs = keras.Input(shape=(100, 20))  # 100 samples, 20 features\n\n# Apply attention\nx = TabularAttention(num_heads=8, d_model=128, dropout_rate=0.1)(inputs)\n\n# Add more processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = keras.layers.Dropout(0.2)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n# Create model\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/tabular-attention/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nattention = TabularAttention(\n    num_heads=16,           # More attention heads for complex patterns\n    d_model=256,            # Higher dimensionality for rich representations\n    dropout_rate=0.2,       # Higher dropout for regularization\n    name=\"advanced_attention\"\n)\n\n# Use in a complex model\ninputs = keras.Input(shape=(50, 30))\nx = keras.layers.Dense(256)(inputs)\nx = attention(x)\nx = keras.layers.LayerNormalization()(x)\nx = keras.layers.Dense(128, activation='relu')(x)\noutputs = keras.layers.Dense(10, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/tabular-attention/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/tabular-attention/#kmr.layers.TabularAttention","title":"kmr.layers.TabularAttention","text":"<p>This module implements a TabularAttention layer that applies inter-feature and inter-sample attention mechanisms for tabular data. It's particularly useful for capturing complex relationships between features and samples in tabular datasets.</p>"},{"location":"layers/tabular-attention/#kmr.layers.TabularAttention-classes","title":"Classes","text":""},{"location":"layers/tabular-attention/#kmr.layers.TabularAttention.TabularAttention","title":"TabularAttention","text":"<pre><code>TabularAttention(\n    num_heads: int,\n    d_model: int,\n    dropout_rate: float = 0.1,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Custom layer to apply inter-feature and inter-sample attention for tabular data.</p> <p>This layer implements a dual attention mechanism: 1. Inter-feature attention: Captures dependencies between features for each sample 2. Inter-sample attention: Captures dependencies between samples for each feature</p> <p>The layer uses MultiHeadAttention for both attention mechanisms and includes layer normalization, dropout, and a feed-forward network.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>name</code> <code>str</code> <p>Name for the layer</p> <code>None</code> Input shape <p>Tensor with shape: <code>(batch_size, num_samples, num_features)</code></p> Output shape <p>Tensor with shape: <code>(batch_size, num_samples, d_model)</code></p> Example <pre><code>import keras\nfrom kmr.layers import TabularAttention\n\n# Create sample input data\nx = keras.random.normal((32, 100, 20))  # 32 batches, 100 samples, 20 features\n\n# Apply tabular attention\nattention = TabularAttention(num_heads=4, d_model=32, dropout_rate=0.1)\ny = attention(x)\nprint(\"Output shape:\", y.shape)  # (32, 100, 32)\n</code></pre> <p>Initialize the TabularAttention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/tabular-attention/#kmr.layers.TabularAttention.TabularAttention-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape of the output tensor.</p>"},{"location":"layers/tabular-attention/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/tabular-attention/#num_heads-int","title":"<code>num_heads</code> (int)","text":"<ul> <li>Purpose: Number of attention heads for parallel processing</li> <li>Range: 1 to 64+ (typically 4, 8, or 16)</li> <li>Impact: More heads = better pattern recognition but higher computational cost</li> <li>Recommendation: Start with 8, increase if you have complex feature interactions</li> </ul>"},{"location":"layers/tabular-attention/#d_model-int","title":"<code>d_model</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the attention model</li> <li>Range: 32 to 512+ (must be divisible by num_heads)</li> <li>Impact: Higher values = richer representations but more parameters</li> <li>Recommendation: Start with 64-128, scale based on your data complexity</li> </ul>"},{"location":"layers/tabular-attention/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Regularization to prevent overfitting</li> <li>Range: 0.0 to 0.9</li> <li>Impact: Higher values = more regularization but potentially less learning</li> <li>Recommendation: Start with 0.1, increase if overfitting occurs</li> </ul>"},{"location":"layers/tabular-attention/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium datasets, scales well with parallel processing</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to attention computations</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex tabular data with feature interactions</li> <li>Best For: Tabular data with complex feature relationships and sample similarities</li> </ul>"},{"location":"layers/tabular-attention/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/tabular-attention/#example-1-customer-segmentation","title":"Example 1: Customer Segmentation","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TabularAttention\n\n# Simulate customer data: age, income, spending, credit_score, etc.\nnum_customers, num_features = 1000, 15\ncustomer_data = keras.random.normal((32, num_customers, num_features))\n\n# Build segmentation model\ninputs = keras.Input(shape=(num_customers, num_features))\nx = TabularAttention(num_heads=8, d_model=64)(inputs)\nx = keras.layers.GlobalAveragePooling1D()(x)  # Pool across samples\nx = keras.layers.Dense(32, activation='relu')(x)\nsegments = keras.layers.Dense(5, activation='softmax')(x)  # 5 customer segments\n\nmodel = keras.Model(inputs, segments)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n</code></pre>"},{"location":"layers/tabular-attention/#example-2-time-series-forecasting","title":"Example 2: Time Series Forecasting","text":"<pre><code># For time series data where each sample is a time point\ntime_steps, features = 30, 10\nts_data = keras.random.normal((32, time_steps, features))\n\n# Build forecasting model\ninputs = keras.Input(shape=(time_steps, features))\nx = TabularAttention(num_heads=4, d_model=32)(inputs)\nx = keras.layers.Dense(16, activation='relu')(x)\nforecast = keras.layers.Dense(1)(x)  # Predict next value\n\nmodel = keras.Model(inputs, forecast)\nmodel.compile(optimizer='adam', loss='mse')\n</code></pre>"},{"location":"layers/tabular-attention/#example-3-multi-task-learning","title":"Example 3: Multi-Task Learning","text":"<pre><code># Shared attention for multiple related tasks\ninputs = keras.Input(shape=(100, 20))\n\n# Shared attention layer\nshared_attention = TabularAttention(num_heads=8, d_model=128)\nx = shared_attention(inputs)\n\n# Task-specific heads\ntask1_output = keras.layers.Dense(1, activation='sigmoid', name='classification')(x)\ntask2_output = keras.layers.Dense(1, name='regression')(x)\n\nmodel = keras.Model(inputs, [task1_output, task2_output])\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'binary_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/tabular-attention/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Start Simple: Begin with 4-8 attention heads and d_model=64, then scale up</li> <li>Data Preprocessing: Ensure your tabular data is properly normalized before applying attention</li> <li>Batch Size: Use larger batch sizes (32+) for better attention learning</li> <li>Layer Order: Place TabularAttention after initial feature processing but before final predictions</li> <li>Regularization: Use dropout and layer normalization to prevent overfitting</li> <li>Monitoring: Watch attention weights to understand what the model is learning</li> </ul>"},{"location":"layers/tabular-attention/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Memory Issues: Large d_model values can cause memory problems - start smaller</li> <li>Overfitting: Too many heads or too high d_model can lead to overfitting on small datasets</li> <li>Input Shape: Ensure input is 3D: (batch_size, num_samples, num_features)</li> <li>Divisibility: d_model must be divisible by num_heads</li> <li>Gradient Issues: Use gradient clipping if training becomes unstable</li> </ul>"},{"location":"layers/tabular-attention/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>MultiResolutionTabularAttention - Multi-scale attention for different feature granularities</li> <li>ColumnAttention - Focused column-wise attention mechanism</li> <li>RowAttention - Specialized row-wise attention for sample relationships</li> <li>VariableSelection - Feature selection that works well with attention layers</li> </ul>"},{"location":"layers/tabular-attention/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Is All You Need - Original Transformer paper</li> <li>TabNet: Attentive Interpretable Tabular Learning - Tabular-specific attention mechanisms</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Tabular Data Tutorial - Complete guide to tabular modeling</li> </ul>"},{"location":"layers/tabular-moe-layer/","title":"\ud83c\udfaf TabularMoELayer\ud83c\udfaf TabularMoELayer","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/tabular-moe-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>TabularMoELayer</code> implements a Mixture-of-Experts (MoE) architecture for tabular data, routing input features through multiple expert sub-networks and aggregating their outputs via a learnable gating mechanism. Each expert is a small MLP that can specialize in different feature patterns.</p> <p>This layer is particularly powerful for tabular data where different experts can specialize in different feature patterns, making it ideal for complex datasets with diverse feature types and interactions.</p>"},{"location":"layers/tabular-moe-layer/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The TabularMoELayer processes data through a mixture-of-experts architecture:</p> <ol> <li>Expert Networks: Creates multiple expert MLPs for different feature patterns</li> <li>Gating Mechanism: Learns to weight expert contributions based on input</li> <li>Expert Processing: Each expert processes the input independently</li> <li>Weighted Aggregation: Combines expert outputs using learned weights</li> <li>Output Generation: Produces final aggregated output</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Gating Network]\n    A --&gt; C1[Expert 1]\n    A --&gt; C2[Expert 2]\n    A --&gt; C3[Expert N]\n\n    B --&gt; D[Gating Weights]\n    C1 --&gt; E1[Expert 1 Output]\n    C2 --&gt; E2[Expert 2 Output]\n    C3 --&gt; E3[Expert N Output]\n\n    D --&gt; F[Weighted Aggregation]\n    E1 --&gt; F\n    E2 --&gt; F\n    E3 --&gt; F\n    F --&gt; G[Final Output]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style G fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style C1 fill:#f3e5f5,stroke:#9c27b0\n    style C2 fill:#f3e5f5,stroke:#9c27b0\n    style C3 fill:#f3e5f5,stroke:#9c27b0\n    style F fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/tabular-moe-layer/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach TabularMoELayer's Solution Feature Diversity Single model for all features \ud83c\udfaf Multiple experts specialize in different patterns Complex Patterns Limited pattern recognition \u26a1 Specialized experts for different feature types Ensemble Learning Separate ensemble models \ud83e\udde0 Integrated ensemble with learned weighting Scalability Fixed model capacity \ud83d\udd17 Scalable capacity with more experts"},{"location":"layers/tabular-moe-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Complex Tabular Data: Datasets with diverse feature types</li> <li>Feature Specialization: Different experts for different feature patterns</li> <li>Ensemble Learning: Integrated ensemble with learned weighting</li> <li>Scalable Models: Models that can scale with more experts</li> <li>Pattern Recognition: Complex pattern recognition in tabular data</li> </ul>"},{"location":"layers/tabular-moe-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/tabular-moe-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import TabularMoELayer\n\n# Create sample input data\nbatch_size, num_features = 32, 8\nx = keras.random.normal((batch_size, num_features))\n\n# Apply mixture of experts\nmoe_layer = TabularMoELayer(num_experts=4, expert_units=16)\noutput = moe_layer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 8)\nprint(f\"Output shape: {output.shape}\")     # (32, 8)\n</code></pre>"},{"location":"layers/tabular-moe-layer/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import TabularMoELayer\n\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    TabularMoELayer(num_experts=4, expert_units=16),\n    keras.layers.Dense(16, activation='relu'),\n    TabularMoELayer(num_experts=2, expert_units=8),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/tabular-moe-layer/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import TabularMoELayer\n\n# Define inputs\ninputs = keras.Input(shape=(20,))  # 20 features\n\n# Apply mixture of experts\nx = TabularMoELayer(num_experts=4, expert_units=16)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(32, activation='relu')(x)\nx = TabularMoELayer(num_experts=2, expert_units=16)(x)\nx = keras.layers.Dense(16, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/tabular-moe-layer/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple MoE layers\ndef create_moe_network():\n    inputs = keras.Input(shape=(30,))\n\n    # Multiple MoE layers with different configurations\n    x = TabularMoELayer(num_experts=6, expert_units=32)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = TabularMoELayer(num_experts=4, expert_units=32)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = TabularMoELayer(num_experts=2, expert_units=16)(x)\n\n    # Final processing\n    x = keras.layers.Dense(16, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_moe_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/tabular-moe-layer/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/tabular-moe-layer/#kmr.layers.TabularMoELayer","title":"kmr.layers.TabularMoELayer","text":"<p>This module implements a TabularMoELayer (Mixture-of-Experts) that routes input features through multiple expert sub-networks and aggregates their outputs via a learnable gating mechanism. This approach is useful for tabular data where different experts can specialize in different feature patterns.</p>"},{"location":"layers/tabular-moe-layer/#kmr.layers.TabularMoELayer-classes","title":"Classes","text":""},{"location":"layers/tabular-moe-layer/#kmr.layers.TabularMoELayer.TabularMoELayer","title":"TabularMoELayer","text":"<pre><code>TabularMoELayer(\n    num_experts: int = 4,\n    expert_units: int = 16,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Mixture-of-Experts layer for tabular data.</p> <p>This layer routes input features through multiple expert sub-networks and aggregates their outputs via a learnable gating mechanism. Each expert is a small MLP, and the gate learns to weight their contributions.</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of expert networks. Default is 4.</p> <code>4</code> <code>expert_units</code> <code>int</code> <p>Number of hidden units in each expert network. Default is 16.</p> <code>16</code> <code>name</code> <code>str | None</code> <p>Optional name for the layer.</p> <code>None</code> Input shape <p>2D tensor with shape: <code>(batch_size, num_features)</code></p> Output shape <p>2D tensor with shape: <code>(batch_size, num_features)</code> (same as input)</p> Example <pre><code>import keras\nfrom kmr.layers import TabularMoELayer\n\n# Tabular data with 8 features\nx = keras.random.normal((32, 8))\n\n# Create the layer with 4 experts and 16 units per expert\nmoe_layer = TabularMoELayer(num_experts=4, expert_units=16)\ny = moe_layer(x)\nprint(\"MoE output shape:\", y.shape)  # Expected: (32, 8)\n</code></pre> <p>Initialize the TabularMoELayer.</p> <p>Parameters:</p> Name Type Description Default <code>num_experts</code> <code>int</code> <p>Number of expert networks.</p> <code>4</code> <code>expert_units</code> <code>int</code> <p>Number of units in each expert.</p> <code>16</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/tabular-moe-layer/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/tabular-moe-layer/#num_experts-int","title":"<code>num_experts</code> (int)","text":"<ul> <li>Purpose: Number of expert networks</li> <li>Range: 2 to 20+ (typically 4-8)</li> <li>Impact: More experts = more specialization but more parameters</li> <li>Recommendation: Start with 4-6, scale based on data complexity</li> </ul>"},{"location":"layers/tabular-moe-layer/#expert_units-int","title":"<code>expert_units</code> (int)","text":"<ul> <li>Purpose: Number of hidden units in each expert network</li> <li>Range: 8 to 128+ (typically 16-64)</li> <li>Impact: Larger values = more complex expert transformations</li> <li>Recommendation: Start with 16-32, scale based on data complexity</li> </ul>"},{"location":"layers/tabular-moe-layer/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with experts</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to multiple experts</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex pattern recognition</li> <li>Best For: Tabular data with diverse feature patterns</li> </ul>"},{"location":"layers/tabular-moe-layer/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/tabular-moe-layer/#example-1-feature-specialization","title":"Example 1: Feature Specialization","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TabularMoELayer\n\n# Create a MoE model for feature specialization\ndef create_feature_specialized_moe():\n    inputs = keras.Input(shape=(25,))  # 25 features\n\n    # MoE layer with multiple experts\n    x = TabularMoELayer(num_experts=6, expert_units=32)(inputs)\n\n    # Process expert outputs\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Another MoE layer\n    x = TabularMoELayer(num_experts=4, expert_units=32)(x)\n\n    # Final processing\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_feature_specialized_moe()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25))\npredictions = model(sample_data)\nprint(f\"Feature specialized MoE predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/tabular-moe-layer/#example-2-expert-analysis","title":"Example 2: Expert Analysis","text":"<pre><code># Analyze expert usage patterns\ndef analyze_expert_usage():\n    # Create model with MoE\n    inputs = keras.Input(shape=(15,))\n    x = TabularMoELayer(num_experts=4, expert_units=16)(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with different input patterns\n    test_inputs = [\n        keras.random.normal((10, 15)),  # Random data\n        keras.random.normal((10, 15)) * 2,  # Scaled data\n        keras.random.normal((10, 15)) + 1,  # Shifted data\n    ]\n\n    print(\"Expert Usage Analysis:\")\n    print(\"=\" * 40)\n\n    for i, test_input in enumerate(test_inputs):\n        prediction = model(test_input)\n        print(f\"Test {i+1}: Prediction mean = {keras.ops.mean(prediction):.4f}\")\n\n    return model\n\n# Analyze expert usage\n# model = analyze_expert_usage()\n</code></pre>"},{"location":"layers/tabular-moe-layer/#example-3-scalable-moe-architecture","title":"Example 3: Scalable MoE Architecture","text":"<pre><code># Create a scalable MoE architecture\ndef create_scalable_moe_architecture():\n    inputs = keras.Input(shape=(40,))\n\n    # Progressive MoE layers with increasing specialization\n    x = TabularMoELayer(num_experts=8, expert_units=32)(inputs)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n\n    x = TabularMoELayer(num_experts=6, expert_units=32)(x)\n    x = keras.layers.Dense(48, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    x = TabularMoELayer(num_experts=4, expert_units=24)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.1)(x)\n\n    x = TabularMoELayer(num_experts=2, expert_units=16)(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(5, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [classification, regression, anomaly])\n\nmodel = create_scalable_moe_architecture()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'classification': 1.0, 'regression': 0.5, 'anomaly': 0.3}\n)\n</code></pre>"},{"location":"layers/tabular-moe-layer/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Number of Experts: Start with 4-6 experts, scale based on data complexity</li> <li>Expert Units: Use 16-32 units per expert for most applications</li> <li>Gating Mechanism: The layer automatically learns expert weighting</li> <li>Specialization: Different experts will specialize in different patterns</li> <li>Scalability: Can scale by adding more experts</li> <li>Regularization: Consider adding dropout between MoE layers</li> </ul>"},{"location":"layers/tabular-moe-layer/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Number of Experts: Must be positive integer</li> <li>Expert Units: Must be positive integer</li> <li>Memory Usage: Scales with number of experts and units</li> <li>Overfitting: Can overfit with too many experts on small datasets</li> <li>Expert Utilization: Some experts may not be used effectively</li> </ul>"},{"location":"layers/tabular-moe-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>SparseAttentionWeighting - Sparse attention weighting</li> <li>GatedFeatureFusion - Gated feature fusion</li> <li>VariableSelection - Variable selection</li> <li>TransformerBlock - Transformer processing</li> </ul>"},{"location":"layers/tabular-moe-layer/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Mixture of Experts - MoE concepts</li> <li>Gating Networks - Gating mechanism paper</li> <li>Ensemble Learning - Ensemble learning concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/text-preprocessing-layer/","title":"\ud83d\udcdd TextPreprocessingLayer","text":"<p>Process and preprocess text features in tabular data with advanced tokenization and encoding capabilities.</p>"},{"location":"layers/text-preprocessing-layer/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>TextPreprocessingLayer</code> provides comprehensive text preprocessing functionality for tabular data, including tokenization, vocabulary building, and text encoding. This layer is essential when your tabular dataset contains text features that need to be converted into numerical representations.</p>"},{"location":"layers/text-preprocessing-layer/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TD\n    A[Text Input] --&gt; B[Tokenization]\n    B --&gt; C[Vocabulary Building]\n    C --&gt; D[Text Encoding]\n    D --&gt; E[Numerical Output]\n\n    B --&gt; F[Word Tokenizer]\n    B --&gt; G[Character Tokenizer]\n\n    C --&gt; H[Vocabulary Size]\n    C --&gt; I[OOV Handling]\n\n    D --&gt; J[Padding/Truncation]\n    D --&gt; K[Embedding Lookup]</code></pre>"},{"location":"layers/text-preprocessing-layer/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/text-preprocessing-layer/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TextPreprocessingLayer\n\n# Create sample text data\ntext_data = [\n    \"This is a sample text\",\n    \"Another example sentence\",\n    \"Text preprocessing layer\"\n]\n\n# Create the layer\ntext_preprocessor = TextPreprocessingLayer(\n    max_length=100,\n    vocab_size=10000,\n    tokenizer='word'\n)\n\n# Process text\nprocessed_text = text_preprocessor(text_data)\nprint(\"Processed text shape:\", processed_text.shape)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#integration-in-model","title":"Integration in Model","text":"<pre><code>def create_text_model(vocab_size, max_length, num_classes):\n    \"\"\"Create a model with text preprocessing.\"\"\"\n\n    # Input for text\n    text_input = keras.Input(shape=(max_length,), dtype='string')\n\n    # Text preprocessing\n    x = TextPreprocessingLayer(\n        max_length=max_length,\n        vocab_size=vocab_size,\n        tokenizer='word'\n    )(text_input)\n\n    # Additional processing\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(text_input, outputs)\n\n# Usage\nmodel = create_text_model(vocab_size=10000, max_length=100, num_classes=3)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#parameters","title":"\u2699\ufe0f Parameters","text":"Parameter Type Default Description <code>max_length</code> int 100 Maximum length of processed text sequences <code>vocab_size</code> int 10000 Size of vocabulary for tokenization <code>tokenizer</code> str 'word' Type of tokenizer ('word' or 'char') <code>padding</code> str 'post' Padding strategy ('pre' or 'post') <code>truncation</code> str 'post' Truncation strategy ('pre' or 'post') <code>oov_token</code> str '' Token for out-of-vocabulary words"},{"location":"layers/text-preprocessing-layer/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"layers/text-preprocessing-layer/#custom-tokenization","title":"Custom Tokenization","text":"<pre><code># Word-level tokenization\nword_preprocessor = TextPreprocessingLayer(\n    max_length=50,\n    vocab_size=5000,\n    tokenizer='word',\n    padding='post',\n    truncation='post'\n)\n\n# Character-level tokenization\nchar_preprocessor = TextPreprocessingLayer(\n    max_length=200,\n    vocab_size=256,\n    tokenizer='char',\n    padding='post',\n    truncation='post'\n)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#text-feature-integration","title":"Text Feature Integration","text":"<pre><code>def create_mixed_model(text_vocab_size, text_max_length, num_features, num_classes):\n    \"\"\"Create a model with both text and numerical features.\"\"\"\n\n    # Text input\n    text_input = keras.Input(shape=(text_max_length,), dtype='string')\n\n    # Numerical input\n    numerical_input = keras.Input(shape=(num_features,))\n\n    # Process text\n    text_processed = TextPreprocessingLayer(\n        max_length=text_max_length,\n        vocab_size=text_vocab_size,\n        tokenizer='word'\n    )(text_input)\n\n    # Process numerical features\n    numerical_processed = keras.layers.Dense(64, activation='relu')(numerical_input)\n\n    # Combine features\n    combined = keras.layers.Concatenate()([text_processed, numerical_processed])\n\n    # Final processing\n    x = keras.layers.Dense(128, activation='relu')(combined)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model([text_input, numerical_input], outputs)\n\n# Usage\nmodel = create_mixed_model(\n    text_vocab_size=10000,\n    text_max_length=100,\n    num_features=20,\n    num_classes=3\n)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#use-cases","title":"\ud83d\udcca Use Cases","text":""},{"location":"layers/text-preprocessing-layer/#1-customer-reviews-analysis","title":"1. Customer Reviews Analysis","text":"<pre><code># Process customer reviews\nreviews = [\n    \"Great product, highly recommended!\",\n    \"Poor quality, would not buy again\",\n    \"Average product, nothing special\"\n]\n\npreprocessor = TextPreprocessingLayer(\n    max_length=50,\n    vocab_size=5000,\n    tokenizer='word'\n)\n\nprocessed_reviews = preprocessor(reviews)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#2-product-descriptions","title":"2. Product Descriptions","text":"<pre><code># Process product descriptions\ndescriptions = [\n    \"High-quality wireless headphones with noise cancellation\",\n    \"Durable laptop bag with multiple compartments\",\n    \"Organic cotton t-shirt in various sizes\"\n]\n\npreprocessor = TextPreprocessingLayer(\n    max_length=30,\n    vocab_size=3000,\n    tokenizer='word'\n)\n\nprocessed_descriptions = preprocessor(descriptions)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#3-social-media-text","title":"3. Social Media Text","text":"<pre><code># Process social media posts\nposts = [\n    \"Just had an amazing dinner at the new restaurant!\",\n    \"Traffic is terrible today #commute\",\n    \"Looking forward to the weekend!\"\n]\n\npreprocessor = TextPreprocessingLayer(\n    max_length=40,\n    vocab_size=8000,\n    tokenizer='word'\n)\n\nprocessed_posts = preprocessor(posts)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#performance-characteristics","title":"\ud83c\udfaf Performance Characteristics","text":"Aspect Performance Speed \u26a1 Fast - Optimized tokenization Memory \ud83d\udcbe Moderate - Depends on vocab size Accuracy \ud83c\udfaf High - Preserves text semantics Scalability \ud83d\udcc8 Good - Handles large vocabularies"},{"location":"layers/text-preprocessing-layer/#best-practices","title":"\ud83d\udd0d Best Practices","text":""},{"location":"layers/text-preprocessing-layer/#1-vocabulary-size-selection","title":"1. Vocabulary Size Selection","text":"<pre><code># For small datasets\nsmall_vocab = TextPreprocessingLayer(vocab_size=1000)\n\n# For large datasets\nlarge_vocab = TextPreprocessingLayer(vocab_size=50000)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#2-sequence-length-optimization","title":"2. Sequence Length Optimization","text":"<pre><code># Analyze your text lengths first\ntext_lengths = [len(text.split()) for text in your_texts]\nmax_length = int(np.percentile(text_lengths, 95))  # Use 95th percentile\n\npreprocessor = TextPreprocessingLayer(max_length=max_length)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#3-tokenizer-selection","title":"3. Tokenizer Selection","text":"<pre><code># Word tokenizer - better for most cases\nword_tokenizer = TextPreprocessingLayer(tokenizer='word')\n\n# Character tokenizer - for morphologically rich languages\nchar_tokenizer = TextPreprocessingLayer(tokenizer='char')\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":""},{"location":"layers/text-preprocessing-layer/#1-vocabulary-size-too-small","title":"1. Vocabulary Size Too Small","text":"<pre><code># \u274c Too small - loses information\nsmall_vocab = TextPreprocessingLayer(vocab_size=100)\n\n# \u2705 Appropriate size\ngood_vocab = TextPreprocessingLayer(vocab_size=10000)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#2-sequence-length-too-short","title":"2. Sequence Length Too Short","text":"<pre><code># \u274c Too short - truncates important information\nshort_seq = TextPreprocessingLayer(max_length=10)\n\n# \u2705 Appropriate length\ngood_seq = TextPreprocessingLayer(max_length=100)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#3-inconsistent-preprocessing","title":"3. Inconsistent Preprocessing","text":"<pre><code># \u274c Different preprocessing for train/test\ntrain_preprocessor = TextPreprocessingLayer(vocab_size=5000)\ntest_preprocessor = TextPreprocessingLayer(vocab_size=10000)\n\n# \u2705 Consistent preprocessing\npreprocessor = TextPreprocessingLayer(vocab_size=10000)\n</code></pre>"},{"location":"layers/text-preprocessing-layer/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>AdvancedNumericalEmbedding - For numerical feature embedding</li> <li>VariableSelection - For feature selection</li> <li>GatedFeatureFusion - For combining text and numerical features</li> </ul>"},{"location":"layers/text-preprocessing-layer/#api-reference","title":"\ud83d\udcda API Reference","text":""},{"location":"layers/text-preprocessing-layer/#kmr.layers.TextPreprocessingLayer","title":"kmr.layers.TextPreprocessingLayer","text":""},{"location":"layers/transformer-block/","title":"\ud83d\udd04 TransformerBlock\ud83d\udd04 TransformerBlock","text":"\ud83d\udd34 Advanced \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/transformer-block/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>TransformerBlock</code> implements a standard transformer block with multi-head self-attention followed by a feed-forward network, with residual connections and layer normalization. This layer is particularly useful for capturing complex relationships in tabular data and sequence processing.</p> <p>This layer is particularly powerful for tabular data where feature interactions are complex, making it ideal for sophisticated feature processing and relationship modeling.</p>"},{"location":"layers/transformer-block/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The TransformerBlock processes data through a transformer architecture:</p> <ol> <li>Multi-Head Attention: Applies multi-head self-attention to capture relationships</li> <li>Residual Connection: Adds input to attention output for gradient flow</li> <li>Layer Normalization: Normalizes the attention output</li> <li>Feed-Forward Network: Applies two-layer feed-forward network</li> <li>Residual Connection: Adds attention output to feed-forward output</li> <li>Layer Normalization: Normalizes the final output</li> </ol> <pre><code>graph TD\n    A[Input Features] --&gt; B[Multi-Head Attention]\n    B --&gt; C[Add &amp; Norm]\n    A --&gt; C\n    C --&gt; D[Feed-Forward Network]\n    D --&gt; E[Add &amp; Norm]\n    C --&gt; E\n    E --&gt; F[Output Features]\n\n    G[Layer Normalization] --&gt; C\n    H[Layer Normalization] --&gt; E\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style F fill:#e8f5e9,stroke:#66bb6a\n    style B fill:#fff9e6,stroke:#ffb74d\n    style D fill:#f3e5f5,stroke:#9c27b0\n    style C fill:#e1f5fe,stroke:#03a9f4\n    style E fill:#e1f5fe,stroke:#03a9f4</code></pre>"},{"location":"layers/transformer-block/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach TransformerBlock's Solution Feature Interactions Limited interaction modeling \ud83c\udfaf Multi-head attention captures complex interactions Sequence Processing RNN-based processing \u26a1 Parallel processing with attention mechanisms Long Dependencies Limited by sequence length \ud83e\udde0 Self-attention captures long-range dependencies Tabular Data Simple feature processing \ud83d\udd17 Sophisticated processing for tabular data"},{"location":"layers/transformer-block/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Tabular Data Processing: Complex feature interaction modeling</li> <li>Sequence Processing: Time series and sequential data</li> <li>Feature Engineering: Sophisticated feature transformation</li> <li>Attention Mechanisms: Implementing attention-based processing</li> <li>Deep Learning: Building deep transformer architectures</li> </ul>"},{"location":"layers/transformer-block/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/transformer-block/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import TransformerBlock\n\n# Create sample input data\nbatch_size, seq_len, dim_model = 32, 10, 64\nx = keras.random.normal((batch_size, seq_len, dim_model))\n\n# Apply transformer block\ntransformer = TransformerBlock(\n    dim_model=64,\n    num_heads=4,\n    ff_units=128,\n    dropout_rate=0.1\n)\noutput = transformer(x)\n\nprint(f\"Input shape: {x.shape}\")           # (32, 10, 64)\nprint(f\"Output shape: {output.shape}\")     # (32, 10, 64)\n</code></pre>"},{"location":"layers/transformer-block/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import TransformerBlock\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu'),\n    TransformerBlock(dim_model=64, num_heads=4, ff_units=128, dropout_rate=0.1),\n    keras.layers.Dense(32, activation='relu'),\n    TransformerBlock(dim_model=32, num_heads=2, ff_units=64, dropout_rate=0.1),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"layers/transformer-block/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import TransformerBlock\n\n# Define inputs\ninputs = keras.Input(shape=(20, 32))  # 20 time steps, 32 features\n\n# Apply transformer block\nx = TransformerBlock(\n    dim_model=32,\n    num_heads=4,\n    ff_units=64,\n    dropout_rate=0.1\n)(inputs)\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(x)\nx = TransformerBlock(\n    dim_model=64,\n    num_heads=4,\n    ff_units=128,\n    dropout_rate=0.1\n)(x)\nx = keras.layers.Dense(32, activation='relu')(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"layers/transformer-block/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with multiple transformer blocks\ndef create_transformer_network():\n    inputs = keras.Input(shape=(15, 48))  # 15 time steps, 48 features\n\n    # Multiple transformer blocks\n    x = TransformerBlock(\n        dim_model=48,\n        num_heads=6,\n        ff_units=96,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = TransformerBlock(\n        dim_model=48,\n        num_heads=6,\n        ff_units=96,\n        dropout_rate=0.1\n    )(x)\n\n    x = TransformerBlock(\n        dim_model=48,\n        num_heads=6,\n        ff_units=96,\n        dropout_rate=0.1\n    )(x)\n\n    # Global pooling and final processing\n    x = keras.layers.GlobalAveragePooling1D()(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Multi-task output\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(x)\n    regression = keras.layers.Dense(1, name='regression')(x)\n\n    return keras.Model(inputs, [classification, regression])\n\nmodel = create_transformer_network()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/transformer-block/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/transformer-block/#kmr.layers.TransformerBlock","title":"kmr.layers.TransformerBlock","text":"<p>This module implements a TransformerBlock layer that applies transformer-style self-attention and feed-forward processing to input tensors. It's particularly useful for capturing complex relationships in tabular data.</p>"},{"location":"layers/transformer-block/#kmr.layers.TransformerBlock-classes","title":"Classes","text":""},{"location":"layers/transformer-block/#kmr.layers.TransformerBlock.TransformerBlock","title":"TransformerBlock","text":"<pre><code>TransformerBlock(\n    dim_model: int = 32,\n    num_heads: int = 3,\n    ff_units: int = 16,\n    dropout_rate: float = 0.2,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Transformer block with multi-head attention and feed-forward layers.</p> <p>This layer implements a standard transformer block with multi-head self-attention followed by a feed-forward network, with residual connections and layer normalization.</p> <p>Parameters:</p> Name Type Description Default <code>dim_model</code> <code>int</code> <p>Dimensionality of the model.</p> <code>32</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>3</code> <code>ff_units</code> <code>int</code> <p>Number of units in the feed-forward network.</p> <code>16</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization.</p> <code>0.2</code> <code>name</code> <code>str</code> <p>Name for the layer.</p> <code>None</code> Input shape <p>Tensor with shape: <code>(batch_size, sequence_length, dim_model)</code> or <code>(batch_size, dim_model)</code> which will be automatically reshaped.</p> Output shape <p>Tensor with shape: <code>(batch_size, sequence_length, dim_model)</code> or <code>(batch_size, dim_model)</code> matching the input shape.</p> Example <pre><code>import keras\nfrom kmr.layers import TransformerBlock\n\n# Create sample input data\nx = keras.random.normal((32, 10, 64))  # 32 samples, 10 time steps, 64 features\n\n# Apply transformer block\ntransformer = TransformerBlock(dim_model=64, num_heads=4, ff_units=128, dropout_rate=0.1)\ny = transformer(x)\nprint(\"Output shape:\", y.shape)  # (32, 10, 64)\n</code></pre> <p>Initialize the TransformerBlock layer.</p> <p>Parameters:</p> Name Type Description Default <code>dim_model</code> <code>int</code> <p>Model dimension.</p> <code>32</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> <code>3</code> <code>ff_units</code> <code>int</code> <p>Feed-forward units.</p> <code>16</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.2</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/transformer-block/#kmr.layers.TransformerBlock.TransformerBlock-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...]\n) -&gt; tuple[int, ...]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input tensor.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Shape of the output tensor.</p>"},{"location":"layers/transformer-block/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/transformer-block/#dim_model-int","title":"<code>dim_model</code> (int)","text":"<ul> <li>Purpose: Dimensionality of the model</li> <li>Range: 8 to 512+ (typically 32-128)</li> <li>Impact: Determines the size of the feature space</li> <li>Recommendation: Start with 32-64, scale based on data complexity</li> </ul>"},{"location":"layers/transformer-block/#num_heads-int","title":"<code>num_heads</code> (int)","text":"<ul> <li>Purpose: Number of attention heads</li> <li>Range: 1 to 16+ (typically 2-8)</li> <li>Impact: More heads = more attention patterns</li> <li>Recommendation: Start with 4-6, adjust based on data complexity</li> </ul>"},{"location":"layers/transformer-block/#ff_units-int","title":"<code>ff_units</code> (int)","text":"<ul> <li>Purpose: Number of units in the feed-forward network</li> <li>Range: 16 to 512+ (typically 64-256)</li> <li>Impact: Larger values = more complex transformations</li> <li>Recommendation: Start with 2x dim_model, scale as needed</li> </ul>"},{"location":"layers/transformer-block/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Dropout rate for regularization</li> <li>Range: 0.0 to 0.5 (typically 0.1-0.2)</li> <li>Impact: Higher values = more regularization</li> <li>Recommendation: Start with 0.1, adjust based on overfitting</li> </ul>"},{"location":"layers/transformer-block/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium models, scales with attention heads</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to attention mechanisms</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for complex relationship modeling</li> <li>Best For: Tabular data with complex feature interactions</li> </ul>"},{"location":"layers/transformer-block/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/transformer-block/#example-1-tabular-data-processing","title":"Example 1: Tabular Data Processing","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import TransformerBlock\n\n# Create a transformer for tabular data\ndef create_tabular_transformer():\n    inputs = keras.Input(shape=(25, 32))  # 25 features, 32 dimensions\n\n    # Transformer processing\n    x = TransformerBlock(\n        dim_model=32,\n        num_heads=4,\n        ff_units=64,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = TransformerBlock(\n        dim_model=32,\n        num_heads=4,\n        ff_units=64,\n        dropout_rate=0.1\n    )(x)\n\n    # Global pooling and final processing\n    x = keras.layers.GlobalAveragePooling1D()(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n\n    # Output\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    return keras.Model(inputs, outputs)\n\nmodel = create_tabular_transformer()\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Test with sample data\nsample_data = keras.random.normal((100, 25, 32))\npredictions = model(sample_data)\nprint(f\"Tabular transformer predictions shape: {predictions.shape}\")\n</code></pre>"},{"location":"layers/transformer-block/#example-2-time-series-processing","title":"Example 2: Time Series Processing","text":"<pre><code># Create a transformer for time series data\ndef create_time_series_transformer():\n    inputs = keras.Input(shape=(30, 16))  # 30 time steps, 16 features\n\n    # Multiple transformer blocks\n    x = TransformerBlock(\n        dim_model=16,\n        num_heads=4,\n        ff_units=32,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = TransformerBlock(\n        dim_model=16,\n        num_heads=4,\n        ff_units=32,\n        dropout_rate=0.1\n    )(x)\n\n    # Global pooling and final processing\n    x = keras.layers.GlobalAveragePooling1D()(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dropout(0.2)(x)\n\n    # Multi-task output\n    trend = keras.layers.Dense(1, name='trend')(x)\n    seasonality = keras.layers.Dense(1, name='seasonality')(x)\n    anomaly = keras.layers.Dense(1, activation='sigmoid', name='anomaly')(x)\n\n    return keras.Model(inputs, [trend, seasonality, anomaly])\n\nmodel = create_time_series_transformer()\nmodel.compile(\n    optimizer='adam',\n    loss={'trend': 'mse', 'seasonality': 'mse', 'anomaly': 'binary_crossentropy'},\n    loss_weights={'trend': 1.0, 'seasonality': 0.5, 'anomaly': 0.3}\n)\n</code></pre>"},{"location":"layers/transformer-block/#example-3-attention-analysis","title":"Example 3: Attention Analysis","text":"<pre><code># Analyze attention patterns in transformer\ndef analyze_attention_patterns():\n    # Create model with transformer\n    inputs = keras.Input(shape=(10, 32))\n    x = TransformerBlock(\n        dim_model=32,\n        num_heads=4,\n        ff_units=64,\n        dropout_rate=0.1\n    )(inputs)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs, outputs)\n\n    # Test with sample data\n    sample_data = keras.random.normal((5, 10, 32))\n    predictions = model(sample_data)\n\n    print(\"Attention Analysis:\")\n    print(\"=\" * 40)\n    print(f\"Input shape: {sample_data.shape}\")\n    print(f\"Output shape: {predictions.shape}\")\n    print(f\"Model parameters: {model.count_params()}\")\n\n    return model\n\n# Analyze attention patterns\n# model = analyze_attention_patterns()\n</code></pre>"},{"location":"layers/transformer-block/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Model Dimension: Start with 32-64, scale based on data complexity</li> <li>Attention Heads: Use 4-6 heads for most applications</li> <li>Feed-Forward Units: Use 2x model dimension as starting point</li> <li>Dropout Rate: Use 0.1-0.2 for regularization</li> <li>Residual Connections: Built-in residual connections for gradient flow</li> <li>Layer Normalization: Built-in layer normalization for stable training</li> </ul>"},{"location":"layers/transformer-block/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Model Dimension: Must match input feature dimension</li> <li>Attention Heads: Must divide model dimension evenly</li> <li>Memory Usage: Scales with attention heads and sequence length</li> <li>Overfitting: Monitor for overfitting with complex models</li> <li>Gradient Flow: Residual connections help but monitor training</li> </ul>"},{"location":"layers/transformer-block/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>TabularAttention - Tabular attention mechanisms</li> <li>MultiResolutionTabularAttention - Multi-resolution attention</li> <li>GatedResidualNetwork - Gated residual networks</li> <li>TabularMoELayer - Mixture of experts</li> </ul>"},{"location":"layers/transformer-block/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Attention Is All You Need - Original transformer paper</li> <li>Multi-Head Attention - Multi-head attention mechanism</li> <li>Transformer Architecture - Transformer concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Feature Engineering Tutorial - Complete guide to feature engineering</li> </ul>"},{"location":"layers/variable-selection/","title":"\ud83c\udfaf VariableSelection\ud83c\udfaf VariableSelection","text":"\ud83d\udfe1 Intermediate \u2705 Stable \ud83d\udd25 Popular"},{"location":"layers/variable-selection/#overview","title":"\ud83c\udfaf Overview","text":"<p>The <code>VariableSelection</code> layer implements dynamic feature selection using gated residual networks (GRNs). Unlike traditional feature selection methods that make static decisions, this layer learns to dynamically select and weight features based on the input context, making it particularly powerful for time series and tabular data where feature importance can vary.</p> <p>This layer applies a gated residual network to each feature independently and learns feature weights through a softmax layer, optionally using a context vector to condition the feature selection process.</p>"},{"location":"layers/variable-selection/#how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>The VariableSelection layer processes features through a sophisticated selection mechanism:</p> <ol> <li>Feature Processing: Each feature is processed independently through a gated residual network</li> <li>Weight Learning: A selection network learns weights for each feature</li> <li>Context Integration: Optionally uses a context vector to condition the selection</li> <li>Softmax Weighting: Applies softmax to normalize feature weights</li> <li>Feature Aggregation: Combines features based on learned weights</li> </ol> <pre><code>graph TD\n    A[Input Features: batch_size, nr_features, feature_dim] --&gt; B[Feature GRNs]\n    C[Context Vector: batch_size, context_dim] --&gt; D[Context Processing]\n\n    B --&gt; E[Feature Representations]\n    D --&gt; F[Context Representation]\n\n    E --&gt; G[Selection Network]\n    F --&gt; G\n\n    G --&gt; H[Feature Weights]\n    H --&gt; I[Softmax Normalization]\n    I --&gt; J[Weighted Feature Selection]\n\n    E --&gt; K[Feature Aggregation]\n    J --&gt; K\n    K --&gt; L[Selected Features + Weights]\n\n    style A fill:#e6f3ff,stroke:#4a86e8\n    style C fill:#fff9e6,stroke:#ffb74d\n    style L fill:#e8f5e9,stroke:#66bb6a\n    style G fill:#f3e5f5,stroke:#9c27b0</code></pre>"},{"location":"layers/variable-selection/#why-use-this-layer","title":"\ud83d\udca1 Why Use This Layer?","text":"Challenge Traditional Approach VariableSelection's Solution Feature Selection Static selection or manual feature engineering \ud83c\udfaf Dynamic selection that adapts to input context Feature Importance Fixed importance or post-hoc analysis \u26a1 Learned importance during training Context Awareness Ignore contextual information \ud83e\udde0 Context-conditioned selection using context vectors Feature Interactions Treat features independently \ud83d\udd17 Gated processing that considers feature relationships"},{"location":"layers/variable-selection/#use-cases","title":"\ud83d\udcca Use Cases","text":"<ul> <li>Time Series Forecasting: Selecting relevant features for different time periods</li> <li>Dynamic Feature Engineering: Adapting feature selection based on data patterns</li> <li>Context-Aware Modeling: Using external context to guide feature selection</li> <li>High-Dimensional Data: Intelligently reducing feature space</li> <li>Multi-Task Learning: Different feature selections for different tasks</li> </ul>"},{"location":"layers/variable-selection/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"layers/variable-selection/#basic-usage","title":"Basic Usage","text":"<pre><code>import keras\nfrom kmr.layers import VariableSelection\n\n# Create sample input data\nbatch_size, nr_features, feature_dim = 32, 10, 16\nx = keras.random.normal((batch_size, nr_features, feature_dim))\n\n# Apply variable selection\nvs = VariableSelection(nr_features=nr_features, units=32, dropout_rate=0.1)\nselected_features, feature_weights = vs(x)\n\nprint(f\"Selected features shape: {selected_features.shape}\")  # (32, 16)\nprint(f\"Feature weights shape: {feature_weights.shape}\")      # (32, 10)\n</code></pre>"},{"location":"layers/variable-selection/#with-context-vector","title":"With Context Vector","text":"<pre><code># Create data with context\nfeatures = keras.random.normal((32, 10, 16))\ncontext = keras.random.normal((32, 64))  # 64-dimensional context\n\n# Apply variable selection with context\nvs_context = VariableSelection(\n    nr_features=10, \n    units=32, \n    dropout_rate=0.1, \n    use_context=True\n)\nselected, weights = vs_context([features, context])\n\nprint(f\"Selected features shape: {selected.shape}\")  # (32, 16)\nprint(f\"Feature weights shape: {weights.shape}\")     # (32, 10)\n</code></pre>"},{"location":"layers/variable-selection/#in-a-sequential-model","title":"In a Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import VariableSelection\n\n# Create a model with variable selection\nmodel = keras.Sequential([\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Reshape((1, 32)),  # Reshape for variable selection\n    VariableSelection(nr_features=1, units=16, dropout_rate=0.1),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"layers/variable-selection/#in-a-functional-model","title":"In a Functional Model","text":"<pre><code>import keras\nfrom kmr.layers import VariableSelection\n\n# Define inputs\nfeatures_input = keras.Input(shape=(10, 16), name='features')\ncontext_input = keras.Input(shape=(64,), name='context')\n\n# Apply variable selection with context\nselected_features, weights = VariableSelection(\n    nr_features=10, \n    units=32, \n    dropout_rate=0.1, \n    use_context=True\n)([features_input, context_input])\n\n# Continue processing\nx = keras.layers.Dense(64, activation='relu')(selected_features)\nx = keras.layers.Dropout(0.2)(x)\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model([features_input, context_input], outputs)\n</code></pre>"},{"location":"layers/variable-selection/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Advanced configuration with custom parameters\nvs = VariableSelection(\n    nr_features=20,\n    units=64,           # Larger hidden units for complex selection\n    dropout_rate=0.2,   # Higher dropout for regularization\n    use_context=True,   # Enable context conditioning\n    name=\"advanced_variable_selection\"\n)\n\n# Use in a complex model\nfeatures = keras.Input(shape=(20, 32), name='features')\ncontext = keras.Input(shape=(128,), name='context')\n\nselected, weights = vs([features, context])\n\n# Multi-task processing\ntask1 = keras.layers.Dense(32, activation='relu')(selected)\ntask1 = keras.layers.Dense(5, activation='softmax', name='classification')(task1)\n\ntask2 = keras.layers.Dense(16, activation='relu')(selected)\ntask2 = keras.layers.Dense(1, name='regression')(task2)\n\nmodel = keras.Model([features, context], [task1, task2])\n</code></pre>"},{"location":"layers/variable-selection/#api-reference","title":"\ud83d\udcd6 API Reference","text":""},{"location":"layers/variable-selection/#kmr.layers.VariableSelection","title":"kmr.layers.VariableSelection","text":"<p>This module implements a VariableSelection layer that applies a gated residual network to each feature independently and learns feature weights through a softmax layer. It's particularly useful for dynamic feature selection in time series and tabular models.</p>"},{"location":"layers/variable-selection/#kmr.layers.VariableSelection-classes","title":"Classes","text":""},{"location":"layers/variable-selection/#kmr.layers.VariableSelection.VariableSelection","title":"VariableSelection","text":"<pre><code>VariableSelection(\n    nr_features: int,\n    units: int,\n    dropout_rate: float = 0.1,\n    use_context: bool = False,\n    name: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>Layer for dynamic feature selection using gated residual networks.</p> <p>This layer applies a gated residual network to each feature independently and learns feature weights through a softmax layer. It can optionally use a context vector to condition the feature selection.</p> <p>Parameters:</p> Name Type Description Default <code>nr_features</code> <code>int</code> <p>Number of input features</p> required <code>units</code> <code>int</code> <p>Number of hidden units in the gated residual network</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>use_context</code> <code>bool</code> <p>Whether to use a context vector for conditioning</p> <code>False</code> <code>name</code> <code>str</code> <p>Name for the layer</p> <code>None</code> Input shape <p>If use_context is False:     - Single tensor with shape: <code>(batch_size, nr_features, feature_dim)</code> If use_context is True:     - List of two tensors:         - Features tensor with shape: <code>(batch_size, nr_features, feature_dim)</code>         - Context tensor with shape: <code>(batch_size, context_dim)</code></p> Output shape <p>Tuple of two tensors: - Selected features: <code>(batch_size, feature_dim)</code> - Feature weights: <code>(batch_size, nr_features)</code></p> Example <pre><code>import keras\nfrom kmr.layers import VariableSelection\n\n# Create sample input data\nx = keras.random.normal((32, 10, 16))  # 32 batches, 10 features, 16 dims per feature\n\n# Without context\nvs = VariableSelection(nr_features=10, units=32, dropout_rate=0.1)\nselected, weights = vs(x)\nprint(\"Selected features shape:\", selected.shape)  # (32, 16)\nprint(\"Feature weights shape:\", weights.shape)  # (32, 10)\n\n# With context\ncontext = keras.random.normal((32, 64))  # 32 batches, 64-dim context\nvs_context = VariableSelection(nr_features=10, units=32, dropout_rate=0.1, use_context=True)\nselected, weights = vs_context([x, context])\n</code></pre> <p>Initialize the VariableSelection layer.</p> <p>Parameters:</p> Name Type Description Default <code>nr_features</code> <code>int</code> <p>Number of input features.</p> required <code>units</code> <code>int</code> <p>Number of units in the selection network.</p> required <code>dropout_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>use_context</code> <code>bool</code> <p>Whether to use context for selection.</p> <code>False</code> <code>name</code> <code>str | None</code> <p>Name of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"layers/variable-selection/#kmr.layers.VariableSelection.VariableSelection-functions","title":"Functions","text":"compute_output_shape <pre><code>compute_output_shape(\n    input_shape: tuple[int, ...] | list[tuple[int, ...]]\n) -&gt; list[tuple[int, ...]]\n</code></pre> <p>Compute the output shape of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>tuple[int, ...] | list[tuple[int, ...]]</code> <p>Shape of the input tensor or list of shapes if using context.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, ...]]</code> <p>List of shapes for the output tensors.</p>"},{"location":"layers/variable-selection/#parameters-deep-dive","title":"\ud83d\udd27 Parameters Deep Dive","text":""},{"location":"layers/variable-selection/#nr_features-int","title":"<code>nr_features</code> (int)","text":"<ul> <li>Purpose: Number of input features to select from</li> <li>Range: 1 to 1000+ (typically 5-50)</li> <li>Impact: Must match the number of features in your input</li> <li>Recommendation: Set to the actual number of features you want to select from</li> </ul>"},{"location":"layers/variable-selection/#units-int","title":"<code>units</code> (int)","text":"<ul> <li>Purpose: Number of hidden units in the selection network</li> <li>Range: 8 to 512+ (typically 16-128)</li> <li>Impact: Larger values = more complex selection patterns but more parameters</li> <li>Recommendation: Start with 32, scale based on feature complexity</li> </ul>"},{"location":"layers/variable-selection/#dropout_rate-float","title":"<code>dropout_rate</code> (float)","text":"<ul> <li>Purpose: Regularization to prevent overfitting</li> <li>Range: 0.0 to 0.9</li> <li>Impact: Higher values = more regularization but potentially less learning</li> <li>Recommendation: Start with 0.1, increase if overfitting occurs</li> </ul>"},{"location":"layers/variable-selection/#use_context-bool","title":"<code>use_context</code> (bool)","text":"<ul> <li>Purpose: Whether to use a context vector for conditioning</li> <li>Default: False</li> <li>Impact: Enables context-aware feature selection</li> <li>Recommendation: Use True when you have contextual information</li> </ul>"},{"location":"layers/variable-selection/#performance-characteristics","title":"\ud83d\udcc8 Performance Characteristics","text":"<ul> <li>Speed: \u26a1\u26a1\u26a1 Fast for small to medium feature counts, scales with nr_features</li> <li>Memory: \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe Moderate memory usage due to per-feature processing</li> <li>Accuracy: \ud83c\udfaf\ud83c\udfaf\ud83c\udfaf\ud83c\udfaf Excellent for dynamic feature selection tasks</li> <li>Best For: Time series and tabular data with varying feature importance</li> </ul>"},{"location":"layers/variable-selection/#examples","title":"\ud83c\udfa8 Examples","text":""},{"location":"layers/variable-selection/#example-1-time-series-feature-selection","title":"Example 1: Time Series Feature Selection","text":"<pre><code>import keras\nimport numpy as np\nfrom kmr.layers import VariableSelection\n\n# Simulate time series data with multiple features\nbatch_size, time_steps, features = 32, 24, 8  # 24 hours, 8 features per hour\ntime_series_data = keras.random.normal((batch_size, time_steps, features))\n\n# Context: time of day, day of week, etc.\ncontext_data = keras.random.normal((batch_size, 16))  # 16-dim context\n\n# Build time series model with variable selection\nfeatures_input = keras.Input(shape=(time_steps, features), name='time_series')\ncontext_input = keras.Input(shape=(16,), name='context')\n\n# Apply variable selection to each time step\nselected_features, weights = VariableSelection(\n    nr_features=time_steps,\n    units=32,\n    dropout_rate=0.1,\n    use_context=True\n)([time_series_data, context_data])\n\n# Process selected features\nx = keras.layers.Dense(64, activation='relu')(selected_features)\nx = keras.layers.Dropout(0.2)(x)\nforecast = keras.layers.Dense(1)(x)  # Predict next value\n\nmodel = keras.Model([features_input, context_input], forecast)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Analyze feature weights over time\nprint(\"Feature weights shape:\", weights.shape)  # (32, 24)\nprint(\"Average weights per time step:\", np.mean(weights, axis=0))\n</code></pre>"},{"location":"layers/variable-selection/#example-2-multi-task-feature-selection","title":"Example 2: Multi-Task Feature Selection","text":"<pre><code># Different tasks may need different feature selections\ndef create_multi_task_model():\n    features = keras.Input(shape=(15, 20), name='features')  # 15 features, 20 dims each\n    context = keras.Input(shape=(32,), name='context')\n\n    # Shared variable selection\n    selected, weights = VariableSelection(\n        nr_features=15,\n        units=48,\n        dropout_rate=0.15,\n        use_context=True\n    )([features, context])\n\n    # Task-specific processing\n    # Classification task\n    cls_features = keras.layers.Dense(64, activation='relu')(selected)\n    cls_features = keras.layers.Dropout(0.3)(cls_features)\n    classification = keras.layers.Dense(3, activation='softmax', name='classification')(cls_features)\n\n    # Regression task\n    reg_features = keras.layers.Dense(32, activation='relu')(selected)\n    reg_features = keras.layers.Dropout(0.2)(reg_features)\n    regression = keras.layers.Dense(1, name='regression')(reg_features)\n\n    return keras.Model([features, context], [classification, regression])\n\nmodel = create_multi_task_model()\nmodel.compile(\n    optimizer='adam',\n    loss={'classification': 'categorical_crossentropy', 'regression': 'mse'},\n    loss_weights={'classification': 1.0, 'regression': 0.5}\n)\n</code></pre>"},{"location":"layers/variable-selection/#example-3-feature-importance-analysis","title":"Example 3: Feature Importance Analysis","text":"<pre><code># Analyze which features are being selected\ndef analyze_feature_selection(model, test_data, feature_names=None):\n    \"\"\"Analyze feature selection patterns.\"\"\"\n    # Get the variable selection layer\n    vs_layer = None\n    for layer in model.layers:\n        if isinstance(layer, VariableSelection):\n            vs_layer = layer\n            break\n\n    if vs_layer is None:\n        print(\"No VariableSelection layer found\")\n        return\n\n    # Get feature weights\n    features, context = test_data\n    _, weights = vs_layer([features, context])\n\n    # Analyze weights\n    avg_weights = np.mean(weights, axis=0)\n    print(\"Average feature weights:\")\n    for i, weight in enumerate(avg_weights):\n        feature_name = feature_names[i] if feature_names else f\"Feature_{i}\"\n        print(f\"{feature_name}: {weight:.4f}\")\n\n    # Find most important features\n    top_features = np.argsort(avg_weights)[-5:]  # Top 5 features\n    print(f\"\\nTop 5 most important features: {top_features}\")\n\n    return weights\n\n# Use with your model\n# weights = analyze_feature_selection(model, [test_features, test_context], feature_names)\n</code></pre>"},{"location":"layers/variable-selection/#tips-best-practices","title":"\ud83d\udca1 Tips &amp; Best Practices","text":"<ul> <li>Feature Dimension: Ensure feature_dim is consistent across all features</li> <li>Context Usage: Use context vectors when you have relevant contextual information</li> <li>Units Sizing: Start with units = nr_features * 2, adjust based on complexity</li> <li>Regularization: Use appropriate dropout to prevent overfitting</li> <li>Weight Analysis: Monitor feature weights to understand selection patterns</li> <li>Batch Size: Works best with larger batch sizes for stable weight learning</li> </ul>"},{"location":"layers/variable-selection/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":"<ul> <li>Input Shape: Must be 3D tensor (batch_size, nr_features, feature_dim)</li> <li>Context Mismatch: Context vector must be 2D (batch_size, context_dim)</li> <li>Feature Count: nr_features must match actual number of input features</li> <li>Memory Usage: Scales with nr_features - be careful with large feature counts</li> <li>Weight Interpretation: Weights are relative, not absolute importance</li> </ul>"},{"location":"layers/variable-selection/#related-layers","title":"\ud83d\udd17 Related Layers","text":"<ul> <li>GatedFeatureSelection - Gated feature selection mechanism</li> <li>GatedResidualNetwork - Core GRN used in variable selection</li> <li>TabularAttention - Attention-based feature processing</li> <li>DistributionAwareEncoder - Distribution-aware feature encoding</li> </ul>"},{"location":"layers/variable-selection/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li>Temporal Fusion Transformers - Original paper on variable selection</li> <li>Gated Residual Networks - GRN architecture details</li> <li>Feature Selection in Deep Learning - Feature selection concepts</li> <li>KMR Layer Explorer - Browse all available layers</li> <li>Time Series Tutorial - Complete guide to time series modeling</li> </ul>"},{"location":"tutorials/basic-workflows/","title":"\ud83d\udd04 Basic Workflows","text":"<p>Learn the fundamental workflows for building tabular models with KMR layers. This tutorial covers the most common patterns and best practices.</p>"},{"location":"tutorials/basic-workflows/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Data Preparation</li> <li>Model Building</li> <li>Training and Evaluation</li> <li>Common Patterns</li> <li>Troubleshooting</li> </ol>"},{"location":"tutorials/basic-workflows/#data-preparation","title":"\ud83d\udcca Data Preparation","text":""},{"location":"tutorials/basic-workflows/#loading-and-preprocessing","title":"Loading and Preprocessing","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom kmr.layers import DifferentiableTabularPreprocessor\n\n# Load your dataset\ndf = pd.read_csv('your_dataset.csv')\n\n# Separate features and target\nX = df.drop('target', axis=1)\ny = df['target']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Convert to numpy arrays\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\nprint(f\"Training shape: {X_train.shape}\")\nprint(f\"Test shape: {X_test.shape}\")\n</code></pre>"},{"location":"tutorials/basic-workflows/#handling-missing-values","title":"Handling Missing Values","text":"<pre><code>from kmr.layers import DifferentiableTabularPreprocessor\n\n# Create preprocessing layer\npreprocessor = DifferentiableTabularPreprocessor(\n    imputation_strategy='learnable',\n    normalization='learnable'\n)\n\n# Fit on training data\npreprocessor.adapt(X_train)\n\n# Transform data\nX_train_processed = preprocessor(X_train)\nX_test_processed = preprocessor(X_test)\n</code></pre>"},{"location":"tutorials/basic-workflows/#model-building","title":"\ud83c\udfd7\ufe0f Model Building","text":""},{"location":"tutorials/basic-workflows/#basic-sequential-model","title":"Basic Sequential Model","text":"<pre><code>import keras\nfrom kmr.layers import (\n    VariableSelection,\n    TabularAttention,\n    GatedFeatureFusion\n)\n\ndef create_basic_model(input_dim, num_classes):\n    \"\"\"Create a basic tabular model with KMR layers.\"\"\"\n\n    # Input layer\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Feature selection\n    x = VariableSelection(hidden_dim=64, dropout=0.1)(x)\n\n    # Attention mechanism\n    x = TabularAttention(num_heads=8, key_dim=64, dropout=0.1)(x)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128, dropout=0.1)(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Create model\nmodel = create_basic_model(input_dim=X_train.shape[1], num_classes=3)\nmodel.summary()\n</code></pre>"},{"location":"tutorials/basic-workflows/#advanced-model-with-residual-connections","title":"Advanced Model with Residual Connections","text":"<pre><code>from kmr.layers import GatedResidualNetwork, TransformerBlock\n\ndef create_advanced_model(input_dim, num_classes):\n    \"\"\"Create an advanced model with residual connections.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Residual blocks\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n\n    # Transformer block\n    x = TransformerBlock(dim_model=64, num_heads=4, ff_units=128)(x)\n\n    # Feature selection\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Create advanced model\nadvanced_model = create_advanced_model(input_dim=X_train.shape[1], num_classes=3)\n</code></pre>"},{"location":"tutorials/basic-workflows/#training-and-evaluation","title":"\ud83c\udfaf Training and Evaluation","text":""},{"location":"tutorials/basic-workflows/#model-compilation","title":"Model Compilation","text":"<pre><code># Compile model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# For regression tasks\n# model.compile(\n#     optimizer=keras.optimizers.Adam(learning_rate=0.001),\n#     loss='mse',\n#     metrics=['mae']\n# )\n</code></pre>"},{"location":"tutorials/basic-workflows/#training-with-callbacks","title":"Training with Callbacks","text":"<pre><code>from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Define callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=10,\n        restore_best_weights=True\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7\n    )\n]\n\n# Train model\nhistory = model.fit(\n    X_train_processed,\n    y_train,\n    validation_split=0.2,\n    epochs=100,\n    batch_size=32,\n    callbacks=callbacks,\n    verbose=1\n)\n</code></pre>"},{"location":"tutorials/basic-workflows/#evaluation","title":"Evaluation","text":"<pre><code># Evaluate model\ntest_loss, test_accuracy = model.evaluate(X_test_processed, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Make predictions\npredictions = model.predict(X_test_processed)\npredicted_classes = np.argmax(predictions, axis=1)\n</code></pre>"},{"location":"tutorials/basic-workflows/#common-patterns","title":"\ud83d\udd04 Common Patterns","text":""},{"location":"tutorials/basic-workflows/#1-feature-engineering-pipeline","title":"1. Feature Engineering Pipeline","text":"<pre><code>from kmr.layers import (\n    AdvancedNumericalEmbedding,\n    DistributionAwareEncoder,\n    SparseAttentionWeighting\n)\n\ndef feature_engineering_pipeline(inputs):\n    \"\"\"Advanced feature engineering pipeline.\"\"\"\n\n    # Numerical embedding\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(inputs)\n\n    # Distribution-aware encoding\n    x = DistributionAwareEncoder(encoding_dim=64)(x)\n\n    # Sparse attention weighting\n    x = SparseAttentionWeighting(temperature=1.0)(x)\n\n    return x\n</code></pre>"},{"location":"tutorials/basic-workflows/#2-multi-head-processing","title":"2. Multi-Head Processing","text":"<pre><code>from kmr.layers import MultiResolutionTabularAttention\n\ndef multi_head_model(inputs):\n    \"\"\"Model with multi-resolution attention.\"\"\"\n\n    # Multi-resolution attention\n    x = MultiResolutionTabularAttention(\n        num_heads=8,\n        numerical_heads=4,\n        categorical_heads=4\n    )(inputs)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    return x\n</code></pre>"},{"location":"tutorials/basic-workflows/#3-ensemble-approach","title":"3. Ensemble Approach","text":"<pre><code>from kmr.layers import BoostingEnsembleLayer\n\ndef ensemble_model(inputs):\n    \"\"\"Model with boosting ensemble.\"\"\"\n\n    # Boosting ensemble\n    x = BoostingEnsembleLayer(\n        num_learners=3,\n        learner_units=64\n    )(inputs)\n\n    # Final processing\n    x = GatedResidualNetwork(units=64)(x)\n\n    return x\n</code></pre>"},{"location":"tutorials/basic-workflows/#4-anomaly-detection","title":"4. Anomaly Detection","text":"<pre><code>from kmr.layers import NumericalAnomalyDetection\n\ndef anomaly_detection_model(inputs):\n    \"\"\"Model with anomaly detection.\"\"\"\n\n    # Anomaly detection\n    anomaly_output = NumericalAnomalyDetection()(inputs)\n\n    # Main processing\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = TabularAttention(num_heads=8)(x)\n\n    return x, anomaly_output\n</code></pre>"},{"location":"tutorials/basic-workflows/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/basic-workflows/#common-issues","title":"Common Issues","text":""},{"location":"tutorials/basic-workflows/#memory-issues","title":"Memory Issues","text":"<pre><code># Reduce model size\nlayer = TabularAttention(\n    num_heads=4,      # Reduce from 8\n    key_dim=32,       # Reduce from 64\n    dropout=0.1\n)\n\n# Use smaller batch size\nmodel.fit(X_train, y_train, batch_size=16)  # Instead of 32\n</code></pre>"},{"location":"tutorials/basic-workflows/#training-instability","title":"Training Instability","text":"<pre><code># Add gradient clipping\nmodel.compile(\n    optimizer=keras.optimizers.Adam(clipnorm=1.0),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Use learning rate scheduling\ndef lr_schedule(epoch):\n    return 0.001 * (0.1 ** (epoch // 20))\n\ncallbacks.append(keras.callbacks.LearningRateScheduler(lr_schedule))\n</code></pre>"},{"location":"tutorials/basic-workflows/#overfitting","title":"Overfitting","text":"<pre><code># Increase regularization\nlayer = VariableSelection(\n    hidden_dim=64,\n    dropout=0.3  # Increase dropout\n)\n\n# Add early stopping\ncallbacks.append(\n    EarlyStopping(monitor='val_loss', patience=5)\n)\n</code></pre>"},{"location":"tutorials/basic-workflows/#performance-optimization","title":"Performance Optimization","text":""},{"location":"tutorials/basic-workflows/#speed-optimization","title":"Speed Optimization","text":"<pre><code># Use fewer attention heads\nlayer = TabularAttention(num_heads=4, key_dim=32)\n\n# Reduce hidden dimensions\nlayer = VariableSelection(hidden_dim=32)\n\n# Use mixed precision\nkeras.mixed_precision.set_global_policy('mixed_float16')\n</code></pre>"},{"location":"tutorials/basic-workflows/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Use gradient checkpointing\nmodel.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss='categorical_crossentropy',\n    run_eagerly=False  # Use graph mode\n)\n\n# Process data in smaller chunks\ndef process_in_chunks(data, chunk_size=1000):\n    results = []\n    for i in range(0, len(data), chunk_size):\n        chunk = data[i:i+chunk_size]\n        result = model.predict(chunk)\n        results.append(result)\n    return np.concatenate(results)\n</code></pre>"},{"location":"tutorials/basic-workflows/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Feature Engineering: Learn advanced feature engineering techniques</li> <li>Model Building: Explore specialized architectures</li> <li>Examples: See real-world applications</li> <li>API Reference: Deep dive into layer parameters</li> </ol> <p>Ready for more advanced topics? Check out Feature Engineering next!</p>"},{"location":"tutorials/feature-engineering/","title":"\ud83d\udd27 Feature Engineering Tutorial","text":"<p>Master the art of feature engineering with KMR layers. Learn how to transform, select, and create powerful features for your tabular models.</p>"},{"location":"tutorials/feature-engineering/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Understanding Feature Engineering</li> <li>Numerical Feature Processing</li> <li>Categorical Feature Handling</li> <li>Feature Selection Techniques</li> <li>Advanced Feature Creation</li> <li>Best Practices</li> </ol>"},{"location":"tutorials/feature-engineering/#understanding-feature-engineering","title":"\ud83c\udfaf Understanding Feature Engineering","text":"<p>Feature engineering is the process of creating, transforming, and selecting features to improve model performance. KMR provides specialized layers for this purpose.</p>"},{"location":"tutorials/feature-engineering/#why-feature-engineering-matters","title":"Why Feature Engineering Matters","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom kmr.layers import AdvancedNumericalEmbedding, DistributionAwareEncoder\n\n# Example: Raw features vs Engineered features\nraw_features = np.random.normal(0, 1, (1000, 10))\n\n# Raw features - limited representation\nprint(\"Raw features shape:\", raw_features.shape)\n\n# Engineered features - richer representation\nembedding_layer = AdvancedNumericalEmbedding(embedding_dim=64)\nengineered_features = embedding_layer(raw_features)\nprint(\"Engineered features shape:\", engineered_features.shape)\n</code></pre>"},{"location":"tutorials/feature-engineering/#numerical-feature-processing","title":"\ud83d\udd22 Numerical Feature Processing","text":""},{"location":"tutorials/feature-engineering/#1-advanced-numerical-embedding","title":"1. Advanced Numerical Embedding","text":"<p>Transform numerical features into rich embeddings:</p> <pre><code>from kmr.layers import AdvancedNumericalEmbedding\n\ndef create_numerical_embedding(input_dim, embedding_dim=64):\n    \"\"\"Create numerical feature embeddings.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Advanced numerical embedding\n    x = AdvancedNumericalEmbedding(\n        embedding_dim=embedding_dim,\n        num_bins=10,\n        hidden_dim=128\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\nembedding_model = create_numerical_embedding(input_dim=20, embedding_dim=64)\n</code></pre>"},{"location":"tutorials/feature-engineering/#2-distribution-aware-encoding","title":"2. Distribution-Aware Encoding","text":"<p>Automatically detect and encode feature distributions:</p> <pre><code>from kmr.layers import DistributionAwareEncoder\n\ndef create_distribution_aware_encoding(input_dim):\n    \"\"\"Create distribution-aware feature encoding.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Distribution-aware encoding\n    x = DistributionAwareEncoder(\n        encoding_dim=64,\n        detection_method='auto'\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\ndistribution_model = create_distribution_aware_encoding(input_dim=20)\n</code></pre>"},{"location":"tutorials/feature-engineering/#3-distribution-transformation","title":"3. Distribution Transformation","text":"<p>Transform features to follow specific distributions:</p> <pre><code>from kmr.layers import DistributionTransformLayer\n\ndef create_distribution_transform(input_dim):\n    \"\"\"Transform features to normal distribution.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Distribution transformation\n    x = DistributionTransformLayer(\n        transform_type='auto',\n        method='box-cox'\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\ntransform_model = create_distribution_transform(input_dim=20)\n</code></pre>"},{"location":"tutorials/feature-engineering/#categorical-feature-handling","title":"\ud83c\udff7\ufe0f Categorical Feature Handling","text":""},{"location":"tutorials/feature-engineering/#1-date-and-time-features","title":"1. Date and Time Features","text":"<p>Process temporal features effectively:</p> <pre><code>from kmr.layers import DateParsingLayer, DateEncodingLayer, SeasonLayer\n\ndef create_temporal_features():\n    \"\"\"Create comprehensive temporal feature processing.\"\"\"\n\n    # Date parsing\n    date_parser = DateParsingLayer()\n\n    # Date encoding\n    date_encoder = DateEncodingLayer(min_year=1900, max_year=2100)\n\n    # Season extraction\n    season_layer = SeasonLayer()\n\n    return date_parser, date_encoder, season_layer\n\n# Usage\ndate_parser, date_encoder, season_layer = create_temporal_features()\n\n# Process date strings\ndate_strings = ['2023-01-15', '2023-06-20', '2023-12-25']\nparsed_dates = date_parser(date_strings)\nencoded_dates = date_encoder(parsed_dates)\nseasonal_features = season_layer(parsed_dates)\n</code></pre>"},{"location":"tutorials/feature-engineering/#2-text-preprocessing","title":"2. Text Preprocessing","text":"<p>Handle text features in tabular data:</p> <pre><code>from kmr.layers import TextPreprocessingLayer\n\ndef create_text_preprocessing():\n    \"\"\"Create text preprocessing pipeline.\"\"\"\n\n    text_preprocessor = TextPreprocessingLayer(\n        max_length=100,\n        vocab_size=10000,\n        tokenizer='word'\n    )\n\n    return text_preprocessor\n\n# Usage\ntext_preprocessor = create_text_preprocessing()\n</code></pre>"},{"location":"tutorials/feature-engineering/#feature-selection-techniques","title":"\ud83c\udfaf Feature Selection Techniques","text":""},{"location":"tutorials/feature-engineering/#1-variable-selection","title":"1. Variable Selection","text":"<p>Intelligently select important features:</p> <pre><code>from kmr.layers import VariableSelection\n\ndef create_variable_selection(input_dim, hidden_dim=64):\n    \"\"\"Create intelligent variable selection.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Variable selection\n    x = VariableSelection(\n        hidden_dim=hidden_dim,\n        dropout=0.1,\n        use_context=False\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\nselection_model = create_variable_selection(input_dim=20, hidden_dim=64)\n</code></pre>"},{"location":"tutorials/feature-engineering/#2-gated-feature-selection","title":"2. Gated Feature Selection","text":"<p>Learnable feature selection with gating:</p> <pre><code>from kmr.layers import GatedFeatureSelection\n\ndef create_gated_selection(input_dim, hidden_dim=64):\n    \"\"\"Create gated feature selection.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Gated feature selection\n    x = GatedFeatureSelection(\n        hidden_dim=hidden_dim,\n        dropout=0.1,\n        activation='relu'\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\ngated_model = create_gated_selection(input_dim=20, hidden_dim=64)\n</code></pre>"},{"location":"tutorials/feature-engineering/#3-sparse-attention-weighting","title":"3. Sparse Attention Weighting","text":"<p>Sparse feature weighting for efficiency:</p> <pre><code>from kmr.layers import SparseAttentionWeighting\n\ndef create_sparse_weighting(input_dim):\n    \"\"\"Create sparse attention weighting.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Sparse attention weighting\n    x = SparseAttentionWeighting(\n        temperature=1.0,\n        dropout=0.1,\n        sparsity_threshold=0.1\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\nsparse_model = create_sparse_weighting(input_dim=20)\n</code></pre>"},{"location":"tutorials/feature-engineering/#advanced-feature-creation","title":"\ud83d\ude80 Advanced Feature Creation","text":""},{"location":"tutorials/feature-engineering/#1-feature-fusion","title":"1. Feature Fusion","text":"<p>Combine multiple feature representations:</p> <pre><code>from kmr.layers import GatedFeatureFusion\n\ndef create_feature_fusion(input_dim1, input_dim2, hidden_dim=128):\n    \"\"\"Create feature fusion mechanism.\"\"\"\n\n    inputs1 = keras.Input(shape=(input_dim1,))\n    inputs2 = keras.Input(shape=(input_dim2,))\n\n    # Feature fusion\n    x = GatedFeatureFusion(\n        hidden_dim=hidden_dim,\n        dropout=0.1,\n        activation='relu'\n    )([inputs1, inputs2])\n\n    return keras.Model([inputs1, inputs2], x)\n\n# Usage\nfusion_model = create_feature_fusion(input_dim1=10, input_dim2=10, hidden_dim=128)\n</code></pre>"},{"location":"tutorials/feature-engineering/#2-feature-cutout","title":"2. Feature Cutout","text":"<p>Data augmentation for features:</p> <pre><code>from kmr.layers import FeatureCutout\n\ndef create_feature_augmentation(input_dim):\n    \"\"\"Create feature augmentation pipeline.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Feature cutout for augmentation\n    x = FeatureCutout(\n        cutout_prob=0.1,\n        noise_value=0.0,\n        training_only=True\n    )(inputs)\n\n    return keras.Model(inputs, x)\n\n# Usage\naugmentation_model = create_feature_augmentation(input_dim=20)\n</code></pre>"},{"location":"tutorials/feature-engineering/#3-graph-based-features","title":"3. Graph-Based Features","text":"<p>Process features as a graph:</p> <pre><code>from kmr.layers import AdvancedGraphFeature, GraphFeatureAggregation\n\ndef create_graph_features(input_dim, hidden_dim=64):\n    \"\"\"Create graph-based feature processing.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Graph feature processing\n    x = AdvancedGraphFeature(\n        hidden_dim=hidden_dim,\n        num_heads=4,\n        dropout=0.1\n    )(inputs)\n\n    # Graph aggregation\n    x = GraphFeatureAggregation(\n        aggregation_method='mean',\n        hidden_dim=hidden_dim\n    )(x)\n\n    return keras.Model(inputs, x)\n\n# Usage\ngraph_model = create_graph_features(input_dim=20, hidden_dim=64)\n</code></pre>"},{"location":"tutorials/feature-engineering/#complete-feature-engineering-pipeline","title":"\ud83c\udf9b\ufe0f Complete Feature Engineering Pipeline","text":""},{"location":"tutorials/feature-engineering/#end-to-end-pipeline","title":"End-to-End Pipeline","text":"<pre><code>def create_complete_feature_pipeline(input_dim, num_classes):\n    \"\"\"Create a complete feature engineering pipeline.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # 1. Preprocessing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # 2. Numerical embedding\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n\n    # 3. Distribution-aware encoding\n    x = DistributionAwareEncoder(encoding_dim=64)(x)\n\n    # 4. Variable selection\n    x = VariableSelection(hidden_dim=64)(x)\n\n    # 5. Sparse attention weighting\n    x = SparseAttentionWeighting(temperature=1.0)(x)\n\n    # 6. Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # 7. Output layer\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\npipeline_model = create_complete_feature_pipeline(input_dim=20, num_classes=3)\npipeline_model.summary()\n</code></pre>"},{"location":"tutorials/feature-engineering/#multi-branch-pipeline","title":"Multi-Branch Pipeline","text":"<pre><code>def create_multi_branch_pipeline(input_dim, num_classes):\n    \"\"\"Create a multi-branch feature engineering pipeline.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Branch 1: Numerical processing\n    branch1 = AdvancedNumericalEmbedding(embedding_dim=64)(inputs)\n    branch1 = DistributionAwareEncoder(encoding_dim=64)(branch1)\n\n    # Branch 2: Selection processing\n    branch2 = VariableSelection(hidden_dim=64)(inputs)\n    branch2 = GatedFeatureSelection(hidden_dim=64)(branch2)\n\n    # Branch 3: Graph processing\n    branch3 = AdvancedGraphFeature(hidden_dim=64)(inputs)\n    branch3 = GraphFeatureAggregation(hidden_dim=64)(branch3)\n\n    # Fusion\n    x = GatedFeatureFusion(hidden_dim=128)([branch1, branch2, branch3])\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmulti_branch_model = create_multi_branch_pipeline(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/feature-engineering/#best-practices","title":"\ud83d\udcca Best Practices","text":""},{"location":"tutorials/feature-engineering/#1-start-simple-add-complexity","title":"1. Start Simple, Add Complexity","text":"<pre><code># Start with basic preprocessing\ndef basic_pipeline(inputs):\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = VariableSelection(hidden_dim=32)(x)\n    return x\n\n# Gradually add complexity\ndef advanced_pipeline(inputs):\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = AdvancedNumericalEmbedding(embedding_dim=64)(x)\n    x = VariableSelection(hidden_dim=64)(x)\n    x = SparseAttentionWeighting()(x)\n    return x\n</code></pre>"},{"location":"tutorials/feature-engineering/#2-monitor-feature-importance","title":"2. Monitor Feature Importance","text":"<pre><code># Use attention weights to understand feature importance\ndef create_interpretable_model(input_dim):\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Attention layer with weights\n    x, attention_weights = TabularAttention(\n        num_heads=8,\n        key_dim=64,\n        use_attention_weights=True\n    )(inputs)\n\n    return keras.Model(inputs, [x, attention_weights])\n\n# Get attention weights\nmodel = create_interpretable_model(input_dim=20)\noutputs, attention_weights = model.predict(X_test)\nprint(\"Attention weights shape:\", attention_weights.shape)\n</code></pre>"},{"location":"tutorials/feature-engineering/#3-feature-engineering-validation","title":"3. Feature Engineering Validation","text":"<pre><code># Validate feature engineering impact\ndef compare_models(X_train, y_train, X_test, y_test):\n    \"\"\"Compare models with and without feature engineering.\"\"\"\n\n    # Model 1: Raw features\n    inputs1 = keras.Input(shape=(X_train.shape[1],))\n    x1 = keras.layers.Dense(64, activation='relu')(inputs1)\n    x1 = keras.layers.Dense(32, activation='relu')(x1)\n    outputs1 = keras.layers.Dense(3, activation='softmax')(x1)\n    model1 = keras.Model(inputs1, outputs1)\n\n    # Model 2: With feature engineering\n    inputs2 = keras.Input(shape=(X_train.shape[1],))\n    x2 = DifferentiableTabularPreprocessor()(inputs2)\n    x2 = AdvancedNumericalEmbedding(embedding_dim=64)(x2)\n    x2 = VariableSelection(hidden_dim=64)(x2)\n    x2 = TabularAttention(num_heads=8, key_dim=64)(x2)\n    outputs2 = keras.layers.Dense(3, activation='softmax')(x2)\n    model2 = keras.Model(inputs2, outputs2)\n\n    # Compile and train both models\n    for model in [model1, model2]:\n        model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        model.fit(X_train, y_train, epochs=10, verbose=0)\n\n    # Compare performance\n    score1 = model1.evaluate(X_test, y_test, verbose=0)\n    score2 = model2.evaluate(X_test, y_test, verbose=0)\n\n    print(f\"Raw features accuracy: {score1[1]:.4f}\")\n    print(f\"Engineered features accuracy: {score2[1]:.4f}\")\n\n    return model1, model2\n\n# Usage\nmodel1, model2 = compare_models(X_train, y_train, X_test, y_test)\n</code></pre>"},{"location":"tutorials/feature-engineering/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Model Building: Learn advanced model architectures</li> <li>Examples: See real-world feature engineering applications</li> <li>API Reference: Deep dive into layer parameters</li> <li>Performance: Optimize your feature engineering pipeline</li> </ol> <p>Ready to build models? Check out Model Building next!</p>"},{"location":"tutorials/model-building/","title":"\ud83c\udfd7\ufe0f Model Building Tutorial","text":"<p>Learn how to build sophisticated tabular models using KMR layers. This tutorial covers advanced architectures, design patterns, and optimization techniques.</p>"},{"location":"tutorials/model-building/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Architecture Patterns</li> <li>Attention-Based Models</li> <li>Residual and Gated Networks</li> <li>Ensemble Methods</li> <li>Specialized Architectures</li> <li>Performance Optimization</li> </ol>"},{"location":"tutorials/model-building/#architecture-patterns","title":"\ud83c\udfdb\ufe0f Architecture Patterns","text":""},{"location":"tutorials/model-building/#1-sequential-architecture","title":"1. Sequential Architecture","text":"<p>The most straightforward approach - layers applied in sequence:</p> <pre><code>import keras\nfrom kmr.layers import (\n    DifferentiableTabularPreprocessor,\n    VariableSelection,\n    TabularAttention,\n    GatedFeatureFusion\n)\n\ndef create_sequential_model(input_dim, num_classes):\n    \"\"\"Create a sequential tabular model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Sequential processing\n    x = DifferentiableTabularPreprocessor()(inputs)\n    x = VariableSelection(hidden_dim=64, dropout=0.1)(x)\n    x = TabularAttention(num_heads=8, key_dim=64, dropout=0.1)(x)\n    x = GatedFeatureFusion(hidden_dim=128, dropout=0.1)(x)\n\n    # Output layer\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmodel = create_sequential_model(input_dim=20, num_classes=3)\nmodel.summary()\n</code></pre>"},{"location":"tutorials/model-building/#2-parallel-architecture","title":"2. Parallel Architecture","text":"<p>Multiple processing branches that are later combined:</p> <pre><code>def create_parallel_model(input_dim, num_classes):\n    \"\"\"Create a parallel processing model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Branch 1: Attention-based processing\n    branch1 = TabularAttention(num_heads=8, key_dim=64)(inputs)\n    branch1 = GatedFeatureFusion(hidden_dim=64)(branch1)\n\n    # Branch 2: Selection-based processing\n    branch2 = VariableSelection(hidden_dim=64)(inputs)\n    branch2 = GatedFeatureFusion(hidden_dim=64)(branch2)\n\n    # Branch 3: Direct processing\n    branch3 = keras.layers.Dense(64, activation='relu')(inputs)\n    branch3 = keras.layers.Dense(64, activation='relu')(branch3)\n\n    # Combine branches\n    combined = keras.layers.Concatenate()([branch1, branch2, branch3])\n    x = keras.layers.Dense(128, activation='relu')(combined)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nparallel_model = create_parallel_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#3-residual-architecture","title":"3. Residual Architecture","text":"<p>Skip connections for improved gradient flow:</p> <pre><code>from kmr.layers import GatedResidualNetwork\n\ndef create_residual_model(input_dim, num_classes):\n    \"\"\"Create a residual model with skip connections.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Initial processing\n    x = DifferentiableTabularPreprocessor()(inputs)\n\n    # Residual blocks\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n\n    # Skip connection\n    x = keras.layers.Add()([inputs, x])\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nresidual_model = create_residual_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#attention-based-models","title":"\ud83e\udde0 Attention-Based Models","text":""},{"location":"tutorials/model-building/#1-multi-head-attention-model","title":"1. Multi-Head Attention Model","text":"<pre><code>from kmr.layers import (\n    TabularAttention,\n    MultiResolutionTabularAttention,\n    InterpretableMultiHeadAttention\n)\n\ndef create_attention_model(input_dim, num_classes):\n    \"\"\"Create a multi-head attention model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Multi-resolution attention\n    x = MultiResolutionTabularAttention(\n        num_heads=8,\n        numerical_heads=4,\n        categorical_heads=4,\n        dropout=0.1\n    )(inputs)\n\n    # Interpretable attention\n    x = InterpretableMultiHeadAttention(\n        num_heads=8,\n        key_dim=64,\n        dropout=0.1\n    )(x)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nattention_model = create_attention_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#2-column-and-row-attention-model","title":"2. Column and Row Attention Model","text":"<pre><code>from kmr.layers import ColumnAttention, RowAttention\n\ndef create_column_row_attention_model(input_dim, num_classes):\n    \"\"\"Create a model with column and row attention.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Column attention (feature-level)\n    x = ColumnAttention(hidden_dim=64, dropout=0.1)(inputs)\n\n    # Row attention (sample-level)\n    x = RowAttention(hidden_dim=64, dropout=0.1)(x)\n\n    # Feature fusion\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\ncolumn_row_model = create_column_row_attention_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#residual-and-gated-networks","title":"\ud83d\udd04 Residual and Gated Networks","text":""},{"location":"tutorials/model-building/#1-gated-residual-network","title":"1. Gated Residual Network","text":"<pre><code>from kmr.layers import GatedResidualNetwork, GatedLinearUnit\n\ndef create_gated_residual_model(input_dim, num_classes):\n    \"\"\"Create a gated residual network model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Gated residual blocks\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(inputs)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n    x = GatedResidualNetwork(units=64, dropout_rate=0.1)(x)\n\n    # Gated linear unit\n    x = GatedLinearUnit(units=64)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\ngated_residual_model = create_gated_residual_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#2-transformer-block-model","title":"2. Transformer Block Model","text":"<pre><code>from kmr.layers import TransformerBlock\n\ndef create_transformer_model(input_dim, num_classes):\n    \"\"\"Create a transformer-based model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Transformer blocks\n    x = TransformerBlock(\n        dim_model=64,\n        num_heads=4,\n        ff_units=128,\n        dropout_rate=0.1\n    )(inputs)\n\n    x = TransformerBlock(\n        dim_model=64,\n        num_heads=4,\n        ff_units=128,\n        dropout_rate=0.1\n    )(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\ntransformer_model = create_transformer_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#ensemble-methods","title":"\ud83c\udfaf Ensemble Methods","text":""},{"location":"tutorials/model-building/#1-mixture-of-experts","title":"1. Mixture of Experts","text":"<pre><code>from kmr.layers import TabularMoELayer\n\ndef create_moe_model(input_dim, num_classes):\n    \"\"\"Create a mixture of experts model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Mixture of experts\n    x = TabularMoELayer(\n        num_experts=4,\n        expert_units=16\n    )(inputs)\n\n    # Additional processing\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmoe_model = create_moe_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#2-boosting-ensemble","title":"2. Boosting Ensemble","text":"<pre><code>from kmr.layers import BoostingEnsembleLayer\n\ndef create_boosting_model(input_dim, num_classes):\n    \"\"\"Create a boosting ensemble model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Boosting ensemble\n    x = BoostingEnsembleLayer(\n        num_learners=3,\n        learner_units=64,\n        hidden_activation='relu'\n    )(inputs)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nboosting_model = create_boosting_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#specialized-architectures","title":"\ud83d\ude80 Specialized Architectures","text":""},{"location":"tutorials/model-building/#1-graph-based-model","title":"1. Graph-Based Model","text":"<pre><code>from kmr.layers import (\n    AdvancedGraphFeature,\n    GraphFeatureAggregation,\n    MultiHeadGraphFeaturePreprocessor\n)\n\ndef create_graph_model(input_dim, num_classes):\n    \"\"\"Create a graph-based model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Graph feature processing\n    x = AdvancedGraphFeature(\n        hidden_dim=64,\n        num_heads=4,\n        dropout=0.1\n    )(inputs)\n\n    # Graph aggregation\n    x = GraphFeatureAggregation(\n        aggregation_method='mean',\n        hidden_dim=64\n    )(x)\n\n    # Multi-head graph preprocessing\n    x = MultiHeadGraphFeaturePreprocessor(\n        num_heads=4,\n        hidden_dim=64\n    )(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\ngraph_model = create_graph_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#2-anomaly-detection-model","title":"2. Anomaly Detection Model","text":"<pre><code>from kmr.layers import (\n    NumericalAnomalyDetection,\n    CategoricalAnomalyDetectionLayer\n)\n\ndef create_anomaly_detection_model(input_dim, num_classes):\n    \"\"\"Create a model with anomaly detection.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Anomaly detection\n    numerical_anomalies = NumericalAnomalyDetection()(inputs)\n    categorical_anomalies = CategoricalAnomalyDetectionLayer()(inputs)\n\n    # Main processing\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = TabularAttention(num_heads=8)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, [outputs, numerical_anomalies, categorical_anomalies])\n\n# Usage\nanomaly_model = create_anomaly_detection_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#3-business-rules-integration","title":"3. Business Rules Integration","text":"<pre><code>from kmr.layers import BusinessRulesLayer\n\ndef create_business_rules_model(input_dim, num_classes, rules):\n    \"\"\"Create a model with business rules integration.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Business rules layer\n    x = BusinessRulesLayer(\n        rules=rules,\n        feature_type='numerical',\n        trainable_weights=True\n    )(inputs)\n\n    # Additional processing\n    x = VariableSelection(hidden_dim=64)(x)\n    x = TabularAttention(num_heads=8)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nrules = [\n    {'feature': 'age', 'operator': '&gt;', 'value': 18, 'weight': 1.0},\n    {'feature': 'income', 'operator': '&gt;', 'value': 50000, 'weight': 0.8}\n]\nbusiness_model = create_business_rules_model(input_dim=20, num_classes=3, rules=rules)\n</code></pre>"},{"location":"tutorials/model-building/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"tutorials/model-building/#1-memory-efficient-model","title":"1. Memory-Efficient Model","text":"<pre><code>def create_memory_efficient_model(input_dim, num_classes):\n    \"\"\"Create a memory-efficient model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use smaller dimensions\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = TabularAttention(num_heads=4, key_dim=32)(x)\n    x = GatedFeatureFusion(hidden_dim=64)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmemory_efficient_model = create_memory_efficient_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#2-speed-optimized-model","title":"2. Speed-Optimized Model","text":"<pre><code>def create_speed_optimized_model(input_dim, num_classes):\n    \"\"\"Create a speed-optimized model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use fewer layers and smaller dimensions\n    x = VariableSelection(hidden_dim=32)(inputs)\n    x = TabularAttention(num_heads=4, key_dim=32)(x)\n\n    # Output\n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nspeed_optimized_model = create_speed_optimized_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#3-mixed-precision-training","title":"3. Mixed Precision Training","text":"<pre><code># Enable mixed precision\nkeras.mixed_precision.set_global_policy('mixed_float16')\n\ndef create_mixed_precision_model(input_dim, num_classes):\n    \"\"\"Create a mixed precision model.\"\"\"\n\n    inputs = keras.Input(shape=(input_dim,))\n\n    # Use mixed precision layers\n    x = VariableSelection(hidden_dim=64)(inputs)\n    x = TabularAttention(num_heads=8, key_dim=64)(x)\n    x = GatedFeatureFusion(hidden_dim=128)(x)\n\n    # Output (use float32 for final layer)\n    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Usage\nmixed_precision_model = create_mixed_precision_model(input_dim=20, num_classes=3)\n</code></pre>"},{"location":"tutorials/model-building/#model-compilation-and-training","title":"\ud83d\udd27 Model Compilation and Training","text":""},{"location":"tutorials/model-building/#1-advanced-compilation","title":"1. Advanced Compilation","text":"<pre><code>def compile_model(model, learning_rate=0.001):\n    \"\"\"Compile model with advanced settings.\"\"\"\n\n    # Learning rate scheduling\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=learning_rate,\n        decay_steps=1000,\n        decay_rate=0.9\n    )\n\n    # Compile with advanced optimizer\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            learning_rate=lr_schedule,\n            clipnorm=1.0\n        ),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n\n# Usage\nmodel = create_sequential_model(input_dim=20, num_classes=3)\nmodel = compile_model(model, learning_rate=0.001)\n</code></pre>"},{"location":"tutorials/model-building/#2-advanced-training","title":"2. Advanced Training","text":"<pre><code>def train_model(model, X_train, y_train, X_val, y_val):\n    \"\"\"Train model with advanced callbacks.\"\"\"\n\n    # Callbacks\n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=5,\n            min_lr=1e-7\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.h5',\n            monitor='val_loss',\n            save_best_only=True\n        )\n    ]\n\n    # Train\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=100,\n        batch_size=32,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    return history\n\n# Usage\nhistory = train_model(model, X_train, y_train, X_val, y_val)\n</code></pre>"},{"location":"tutorials/model-building/#model-evaluation-and-analysis","title":"\ud83d\udcca Model Evaluation and Analysis","text":""},{"location":"tutorials/model-building/#1-comprehensive-evaluation","title":"1. Comprehensive Evaluation","text":"<pre><code>def evaluate_model(model, X_test, y_test):\n    \"\"\"Comprehensive model evaluation.\"\"\"\n\n    # Basic evaluation\n    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n\n    # Predictions\n    predictions = model.predict(X_test)\n    predicted_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_test, axis=1)\n\n    # Additional metrics\n    from sklearn.metrics import classification_report, confusion_matrix\n\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_classes, predicted_classes))\n\n    return test_accuracy, test_loss\n\n# Usage\naccuracy, loss = evaluate_model(model, X_test, y_test)\n</code></pre>"},{"location":"tutorials/model-building/#2-model-interpretation","title":"2. Model Interpretation","text":"<pre><code>def interpret_model(model, X_test, layer_name='tabular_attention'):\n    \"\"\"Interpret model using attention weights.\"\"\"\n\n    # Get attention weights\n    attention_model = keras.Model(\n        inputs=model.input,\n        outputs=model.get_layer(layer_name).output\n    )\n\n    attention_weights = attention_model.predict(X_test)\n\n    # Analyze attention patterns\n    mean_attention = np.mean(attention_weights, axis=0)\n    print(\"Mean attention weights:\", mean_attention)\n\n    return attention_weights\n\n# Usage\nattention_weights = interpret_model(model, X_test)\n</code></pre>"},{"location":"tutorials/model-building/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ol> <li>Examples: See real-world model building applications</li> <li>API Reference: Deep dive into layer parameters</li> <li>Performance: Optimize your models for production</li> <li>Advanced Topics: Explore cutting-edge techniques</li> </ol> <p>Ready to see real examples? Check out the Examples section!</p>"}]}